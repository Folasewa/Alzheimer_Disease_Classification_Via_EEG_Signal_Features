<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <title id="head-title">report.html</title>
      <link href="assets/style.css" rel="stylesheet" type="text/css"/>
  </head>
  <body>
    <h1 id="title">report.html</h1>
    <p>Report generated on 14-Feb-2025 at 02:57:01 by <a href="https://pypi.python.org/pypi/pytest-html">pytest-html</a>
        v4.1.1</p>
    <div id="environment-header">
      <h2>Environment</h2>
    </div>
    <table id="environment"></table>
    <!-- TEMPLATES -->
      <template id="template_environment_row">
      <tr>
        <td></td>
        <td></td>
      </tr>
    </template>
    <template id="template_results-table__body--empty">
      <tbody class="results-table-row">
        <tr id="not-found-message">
          <td colspan="4">No results found. Check the filters.</th>
        </tr>
    </template>
    <template id="template_results-table__tbody">
      <tbody class="results-table-row">
        <tr class="collapsible">
        </tr>
        <tr class="extras-row">
          <td class="extra" colspan="4">
            <div class="extraHTML"></div>
            <div class="media">
              <div class="media-container">
                  <div class="media-container__nav--left"><</div>
                  <div class="media-container__viewport">
                    <img src="" />
                    <video controls>
                      <source src="" type="video/mp4">
                    </video>
                  </div>
                  <div class="media-container__nav--right">></div>
                </div>
                <div class="media__name"></div>
                <div class="media__counter"></div>
            </div>
            <div class="logwrapper">
              <div class="logexpander"></div>
              <div class="log"></div>
            </div>
          </td>
        </tr>
      </tbody>
    </template>
    <!-- END TEMPLATES -->
    <div class="summary">
      <div class="summary__data">
        <h2>Summary</h2>
        <div class="additional-summary prefix">
        </div>
        <p class="run-count">42 tests took 15 ms.</p>
        <p class="filter">(Un)check the boxes to filter the results.</p>
        <div class="summary__reload">
          <div class="summary__reload__button hidden" onclick="location.reload()">
            <div>There are still tests running. <br />Reload this page to get the latest results!</div>
          </div>
        </div>
        <div class="summary__spacer"></div>
        <div class="controls">
          <div class="filters">
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="failed" />
            <span class="failed">39 Failed,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="passed" />
            <span class="passed">3 Passed,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="skipped" disabled/>
            <span class="skipped">0 Skipped,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="xfailed" disabled/>
            <span class="xfailed">0 Expected failures,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="xpassed" disabled/>
            <span class="xpassed">0 Unexpected passes,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="error" disabled/>
            <span class="error">0 Errors,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="rerun" disabled/>
            <span class="rerun">0 Reruns</span>
          </div>
          <div class="collapse">
            <button id="show_all_details">Show all details</button>&nbsp;/&nbsp;<button id="hide_all_details">Hide all details</button>
          </div>
        </div>
      </div>
      <div class="additional-summary summary">
      </div>
      <div class="additional-summary postfix">
      </div>
    </div>
    <table id="results-table">
      <thead id="results-table-head">
        <tr>
          <th class="sortable" data-column-type="result">Result</th>
          <th class="sortable" data-column-type="testId">Test</th>
          <th class="sortable" data-column-type="duration">Duration</th>
          <th>Links</th>
        </tr>
      </thead>
    </table>
  </body>
  <footer>
    <div id="data-container" data-jsonblob="{&#34;environment&#34;: {&#34;Python&#34;: &#34;3.12.7&#34;, &#34;Platform&#34;: &#34;macOS-15.1.1-arm64-arm-64bit&#34;, &#34;Packages&#34;: {&#34;pytest&#34;: &#34;8.3.4&#34;, &#34;pluggy&#34;: &#34;1.5.0&#34;}, &#34;Plugins&#34;: {&#34;cov&#34;: &#34;5.0.0&#34;, &#34;html&#34;: &#34;4.1.1&#34;, &#34;metadata&#34;: &#34;3.1.1&#34;}}, &#34;tests&#34;: {&#34;tests/test_classification_model.py::test_apply_label_pos&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_classification_model.py::test_apply_label_pos&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_classification_model.py::test_apply_label_pos&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_apply_label_pos():\n&amp;gt;       raw_data = pd.read_csv(&amp;quot;test_data/dataset_project_ds_nolabel.csv&amp;quot;)\n\ntests/test_classification_model.py:8: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026: in read_csv\n    return _read(filepath_or_buffer, kwds)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620: in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620: in __init__\n    self._engine = self._make_engine(f, self.engine)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880: in _make_engine\n    self.handles = get_handle(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\npath_or_buf = &amp;#x27;test_data/dataset_project_ds_nolabel.csv&amp;#x27;, mode = &amp;#x27;r&amp;#x27;\n\n    @doc(compression_options=_shared_docs[&amp;quot;compression_options&amp;quot;] % &amp;quot;path_or_buf&amp;quot;)\n    def get_handle(\n        path_or_buf: FilePath | BaseBuffer,\n        mode: str,\n        *,\n        encoding: str | None = None,\n        compression: CompressionOptions | None = None,\n        memory_map: bool = False,\n        is_text: bool = True,\n        errors: str | None = None,\n        storage_options: StorageOptions | None = None,\n    ) -&amp;gt; IOHandles[str] | IOHandles[bytes]:\n        &amp;quot;&amp;quot;&amp;quot;\n        Get file handle for given path/buffer and mode.\n    \n        Parameters\n        ----------\n        path_or_buf : str or file handle\n            File path or object.\n        mode : str\n            Mode to open path_or_buf with.\n        encoding : str or None\n            Encoding to use.\n        {compression_options}\n    \n               May be a dict with key &amp;#x27;method&amp;#x27; as compression mode\n               and other keys as compression options if compression\n               mode is &amp;#x27;zip&amp;#x27;.\n    \n               Passing compression options as keys in dict is\n               supported for compression modes &amp;#x27;gzip&amp;#x27;, &amp;#x27;bz2&amp;#x27;, &amp;#x27;zstd&amp;#x27; and &amp;#x27;zip&amp;#x27;.\n    \n            .. versionchanged:: 1.4.0 Zstandard support.\n    \n        memory_map : bool, default False\n            See parsers._parser_params for more information. Only used by read_csv.\n        is_text : bool, default True\n            Whether the type of the content passed to the file/buffer is string or\n            bytes. This is not the same as `&amp;quot;b&amp;quot; not in mode`. If a string content is\n            passed to a binary file/buffer, a wrapper is inserted.\n        errors : str, default &amp;#x27;strict&amp;#x27;\n            Specifies how encoding and decoding errors are to be handled.\n            See the errors argument for :func:`open` for a full list\n            of options.\n        storage_options: StorageOptions = None\n            Passed to _get_filepath_or_buffer\n    \n        Returns the dataclass IOHandles\n        &amp;quot;&amp;quot;&amp;quot;\n        # Windows does not default to utf-8. Set to utf-8 for a consistent behavior\n        encoding = encoding or &amp;quot;utf-8&amp;quot;\n    \n        errors = errors or &amp;quot;strict&amp;quot;\n    \n        # read_csv does not know whether the buffer is opened in binary/text mode\n        if _is_binary_mode(path_or_buf, mode) and &amp;quot;b&amp;quot; not in mode:\n            mode += &amp;quot;b&amp;quot;\n    \n        # validate encoding and errors\n        codecs.lookup(encoding)\n        if isinstance(errors, str):\n            codecs.lookup_error(errors)\n    \n        # open URLs\n        ioargs = _get_filepath_or_buffer(\n            path_or_buf,\n            encoding=encoding,\n            compression=compression,\n            mode=mode,\n            storage_options=storage_options,\n        )\n    \n        handle = ioargs.filepath_or_buffer\n        handles: list[BaseBuffer]\n    \n        # memory mapping needs to be the first step\n        # only used for read_csv\n        handle, memory_map, handles = _maybe_memory_map(handle, memory_map)\n    \n        is_path = isinstance(handle, str)\n        compression_args = dict(ioargs.compression)\n        compression = compression_args.pop(&amp;quot;method&amp;quot;)\n    \n        # Only for write methods\n        if &amp;quot;r&amp;quot; not in mode and is_path:\n            check_parent_directory(str(handle))\n    \n        if compression:\n            if compression != &amp;quot;zstd&amp;quot;:\n                # compression libraries do not like an explicit text-mode\n                ioargs.mode = ioargs.mode.replace(&amp;quot;t&amp;quot;, &amp;quot;&amp;quot;)\n            elif compression == &amp;quot;zstd&amp;quot; and &amp;quot;b&amp;quot; not in ioargs.mode:\n                # python-zstandard defaults to text mode, but we always expect\n                # compression libraries to use binary mode.\n                ioargs.mode += &amp;quot;b&amp;quot;\n    \n            # GZ Compression\n            if compression == &amp;quot;gzip&amp;quot;:\n                if isinstance(handle, str):\n                    # error: Incompatible types in assignment (expression has type\n                    # &amp;quot;GzipFile&amp;quot;, variable has type &amp;quot;Union[str, BaseBuffer]&amp;quot;)\n                    handle = gzip.GzipFile(  # type: ignore[assignment]\n                        filename=handle,\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n                else:\n                    handle = gzip.GzipFile(\n                        # No overload variant of &amp;quot;GzipFile&amp;quot; matches argument types\n                        # &amp;quot;Union[str, BaseBuffer]&amp;quot;, &amp;quot;str&amp;quot;, &amp;quot;Dict[str, Any]&amp;quot;\n                        fileobj=handle,  # type: ignore[call-overload]\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n    \n            # BZ Compression\n            elif compression == &amp;quot;bz2&amp;quot;:\n                # Overload of &amp;quot;BZ2File&amp;quot; to handle pickle protocol 5\n                # &amp;quot;Union[str, BaseBuffer]&amp;quot;, &amp;quot;str&amp;quot;, &amp;quot;Dict[str, Any]&amp;quot;\n                handle = get_bz2_file()(  # type: ignore[call-overload]\n                    handle,\n                    mode=ioargs.mode,\n                    **compression_args,\n                )\n    \n            # ZIP Compression\n            elif compression == &amp;quot;zip&amp;quot;:\n                # error: Argument 1 to &amp;quot;_BytesZipFile&amp;quot; has incompatible type\n                # &amp;quot;Union[str, BaseBuffer]&amp;quot;; expected &amp;quot;Union[Union[str, PathLike[str]],\n                # ReadBuffer[bytes], WriteBuffer[bytes]]&amp;quot;\n                handle = _BytesZipFile(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n                if handle.buffer.mode == &amp;quot;r&amp;quot;:\n                    handles.append(handle)\n                    zip_names = handle.buffer.namelist()\n                    if len(zip_names) == 1:\n                        handle = handle.buffer.open(zip_names.pop())\n                    elif not zip_names:\n                        raise ValueError(f&amp;quot;Zero files found in ZIP file {path_or_buf}&amp;quot;)\n                    else:\n                        raise ValueError(\n                            &amp;quot;Multiple files found in ZIP file. &amp;quot;\n                            f&amp;quot;Only one file per ZIP: {zip_names}&amp;quot;\n                        )\n    \n            # TAR Encoding\n            elif compression == &amp;quot;tar&amp;quot;:\n                compression_args.setdefault(&amp;quot;mode&amp;quot;, ioargs.mode)\n                if isinstance(handle, str):\n                    handle = _BytesTarFile(name=handle, **compression_args)\n                else:\n                    # error: Argument &amp;quot;fileobj&amp;quot; to &amp;quot;_BytesTarFile&amp;quot; has incompatible\n                    # type &amp;quot;BaseBuffer&amp;quot;; expected &amp;quot;Union[ReadBuffer[bytes],\n                    # WriteBuffer[bytes], None]&amp;quot;\n                    handle = _BytesTarFile(\n                        fileobj=handle, **compression_args  # type: ignore[arg-type]\n                    )\n                assert isinstance(handle, _BytesTarFile)\n                if &amp;quot;r&amp;quot; in handle.buffer.mode:\n                    handles.append(handle)\n                    files = handle.buffer.getnames()\n                    if len(files) == 1:\n                        file = handle.buffer.extractfile(files[0])\n                        assert file is not None\n                        handle = file\n                    elif not files:\n                        raise ValueError(f&amp;quot;Zero files found in TAR archive {path_or_buf}&amp;quot;)\n                    else:\n                        raise ValueError(\n                            &amp;quot;Multiple files found in TAR archive. &amp;quot;\n                            f&amp;quot;Only one file per TAR archive: {files}&amp;quot;\n                        )\n    \n            # XZ Compression\n            elif compression == &amp;quot;xz&amp;quot;:\n                # error: Argument 1 to &amp;quot;LZMAFile&amp;quot; has incompatible type &amp;quot;Union[str,\n                # BaseBuffer]&amp;quot;; expected &amp;quot;Optional[Union[Union[str, bytes, PathLike[str],\n                # PathLike[bytes]], IO[bytes]], None]&amp;quot;\n                handle = get_lzma_file()(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n    \n            # Zstd Compression\n            elif compression == &amp;quot;zstd&amp;quot;:\n                zstd = import_optional_dependency(&amp;quot;zstandard&amp;quot;)\n                if &amp;quot;r&amp;quot; in ioargs.mode:\n                    open_args = {&amp;quot;dctx&amp;quot;: zstd.ZstdDecompressor(**compression_args)}\n                else:\n                    open_args = {&amp;quot;cctx&amp;quot;: zstd.ZstdCompressor(**compression_args)}\n                handle = zstd.open(\n                    handle,\n                    mode=ioargs.mode,\n                    **open_args,\n                )\n    \n            # Unrecognized Compression\n            else:\n                msg = f&amp;quot;Unrecognized compression type: {compression}&amp;quot;\n                raise ValueError(msg)\n    \n            assert not isinstance(handle, str)\n            handles.append(handle)\n    \n        elif isinstance(handle, str):\n            # Check whether the filename is to be opened in binary mode.\n            # Binary mode does not support &amp;#x27;encoding&amp;#x27; and &amp;#x27;newline&amp;#x27;.\n            if ioargs.encoding and &amp;quot;b&amp;quot; not in ioargs.mode:\n                # Encoding\n&amp;gt;               handle = open(\n                    handle,\n                    ioargs.mode,\n                    encoding=ioargs.encoding,\n                    errors=errors,\n                    newline=&amp;quot;&amp;quot;,\n                )\nE               FileNotFoundError: [Errno 2] No such file or directory: &amp;#x27;test_data/dataset_project_ds_nolabel.csv&amp;#x27;\n\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/common.py:873: FileNotFoundError\n&#34;}], &#34;tests/test_classification_model.py::test_apply_label_neg&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_classification_model.py::test_apply_label_neg&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_classification_model.py::test_apply_label_neg&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_apply_label_neg():\n&amp;gt;       raw_data = pd.read_csv(&amp;quot;test_data/dataset_project_ds_nodata.csv&amp;quot;)\n\ntests/test_classification_model.py:13: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026: in read_csv\n    return _read(filepath_or_buffer, kwds)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620: in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620: in __init__\n    self._engine = self._make_engine(f, self.engine)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880: in _make_engine\n    self.handles = get_handle(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\npath_or_buf = &amp;#x27;test_data/dataset_project_ds_nodata.csv&amp;#x27;, mode = &amp;#x27;r&amp;#x27;\n\n    @doc(compression_options=_shared_docs[&amp;quot;compression_options&amp;quot;] % &amp;quot;path_or_buf&amp;quot;)\n    def get_handle(\n        path_or_buf: FilePath | BaseBuffer,\n        mode: str,\n        *,\n        encoding: str | None = None,\n        compression: CompressionOptions | None = None,\n        memory_map: bool = False,\n        is_text: bool = True,\n        errors: str | None = None,\n        storage_options: StorageOptions | None = None,\n    ) -&amp;gt; IOHandles[str] | IOHandles[bytes]:\n        &amp;quot;&amp;quot;&amp;quot;\n        Get file handle for given path/buffer and mode.\n    \n        Parameters\n        ----------\n        path_or_buf : str or file handle\n            File path or object.\n        mode : str\n            Mode to open path_or_buf with.\n        encoding : str or None\n            Encoding to use.\n        {compression_options}\n    \n               May be a dict with key &amp;#x27;method&amp;#x27; as compression mode\n               and other keys as compression options if compression\n               mode is &amp;#x27;zip&amp;#x27;.\n    \n               Passing compression options as keys in dict is\n               supported for compression modes &amp;#x27;gzip&amp;#x27;, &amp;#x27;bz2&amp;#x27;, &amp;#x27;zstd&amp;#x27; and &amp;#x27;zip&amp;#x27;.\n    \n            .. versionchanged:: 1.4.0 Zstandard support.\n    \n        memory_map : bool, default False\n            See parsers._parser_params for more information. Only used by read_csv.\n        is_text : bool, default True\n            Whether the type of the content passed to the file/buffer is string or\n            bytes. This is not the same as `&amp;quot;b&amp;quot; not in mode`. If a string content is\n            passed to a binary file/buffer, a wrapper is inserted.\n        errors : str, default &amp;#x27;strict&amp;#x27;\n            Specifies how encoding and decoding errors are to be handled.\n            See the errors argument for :func:`open` for a full list\n            of options.\n        storage_options: StorageOptions = None\n            Passed to _get_filepath_or_buffer\n    \n        Returns the dataclass IOHandles\n        &amp;quot;&amp;quot;&amp;quot;\n        # Windows does not default to utf-8. Set to utf-8 for a consistent behavior\n        encoding = encoding or &amp;quot;utf-8&amp;quot;\n    \n        errors = errors or &amp;quot;strict&amp;quot;\n    \n        # read_csv does not know whether the buffer is opened in binary/text mode\n        if _is_binary_mode(path_or_buf, mode) and &amp;quot;b&amp;quot; not in mode:\n            mode += &amp;quot;b&amp;quot;\n    \n        # validate encoding and errors\n        codecs.lookup(encoding)\n        if isinstance(errors, str):\n            codecs.lookup_error(errors)\n    \n        # open URLs\n        ioargs = _get_filepath_or_buffer(\n            path_or_buf,\n            encoding=encoding,\n            compression=compression,\n            mode=mode,\n            storage_options=storage_options,\n        )\n    \n        handle = ioargs.filepath_or_buffer\n        handles: list[BaseBuffer]\n    \n        # memory mapping needs to be the first step\n        # only used for read_csv\n        handle, memory_map, handles = _maybe_memory_map(handle, memory_map)\n    \n        is_path = isinstance(handle, str)\n        compression_args = dict(ioargs.compression)\n        compression = compression_args.pop(&amp;quot;method&amp;quot;)\n    \n        # Only for write methods\n        if &amp;quot;r&amp;quot; not in mode and is_path:\n            check_parent_directory(str(handle))\n    \n        if compression:\n            if compression != &amp;quot;zstd&amp;quot;:\n                # compression libraries do not like an explicit text-mode\n                ioargs.mode = ioargs.mode.replace(&amp;quot;t&amp;quot;, &amp;quot;&amp;quot;)\n            elif compression == &amp;quot;zstd&amp;quot; and &amp;quot;b&amp;quot; not in ioargs.mode:\n                # python-zstandard defaults to text mode, but we always expect\n                # compression libraries to use binary mode.\n                ioargs.mode += &amp;quot;b&amp;quot;\n    \n            # GZ Compression\n            if compression == &amp;quot;gzip&amp;quot;:\n                if isinstance(handle, str):\n                    # error: Incompatible types in assignment (expression has type\n                    # &amp;quot;GzipFile&amp;quot;, variable has type &amp;quot;Union[str, BaseBuffer]&amp;quot;)\n                    handle = gzip.GzipFile(  # type: ignore[assignment]\n                        filename=handle,\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n                else:\n                    handle = gzip.GzipFile(\n                        # No overload variant of &amp;quot;GzipFile&amp;quot; matches argument types\n                        # &amp;quot;Union[str, BaseBuffer]&amp;quot;, &amp;quot;str&amp;quot;, &amp;quot;Dict[str, Any]&amp;quot;\n                        fileobj=handle,  # type: ignore[call-overload]\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n    \n            # BZ Compression\n            elif compression == &amp;quot;bz2&amp;quot;:\n                # Overload of &amp;quot;BZ2File&amp;quot; to handle pickle protocol 5\n                # &amp;quot;Union[str, BaseBuffer]&amp;quot;, &amp;quot;str&amp;quot;, &amp;quot;Dict[str, Any]&amp;quot;\n                handle = get_bz2_file()(  # type: ignore[call-overload]\n                    handle,\n                    mode=ioargs.mode,\n                    **compression_args,\n                )\n    \n            # ZIP Compression\n            elif compression == &amp;quot;zip&amp;quot;:\n                # error: Argument 1 to &amp;quot;_BytesZipFile&amp;quot; has incompatible type\n                # &amp;quot;Union[str, BaseBuffer]&amp;quot;; expected &amp;quot;Union[Union[str, PathLike[str]],\n                # ReadBuffer[bytes], WriteBuffer[bytes]]&amp;quot;\n                handle = _BytesZipFile(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n                if handle.buffer.mode == &amp;quot;r&amp;quot;:\n                    handles.append(handle)\n                    zip_names = handle.buffer.namelist()\n                    if len(zip_names) == 1:\n                        handle = handle.buffer.open(zip_names.pop())\n                    elif not zip_names:\n                        raise ValueError(f&amp;quot;Zero files found in ZIP file {path_or_buf}&amp;quot;)\n                    else:\n                        raise ValueError(\n                            &amp;quot;Multiple files found in ZIP file. &amp;quot;\n                            f&amp;quot;Only one file per ZIP: {zip_names}&amp;quot;\n                        )\n    \n            # TAR Encoding\n            elif compression == &amp;quot;tar&amp;quot;:\n                compression_args.setdefault(&amp;quot;mode&amp;quot;, ioargs.mode)\n                if isinstance(handle, str):\n                    handle = _BytesTarFile(name=handle, **compression_args)\n                else:\n                    # error: Argument &amp;quot;fileobj&amp;quot; to &amp;quot;_BytesTarFile&amp;quot; has incompatible\n                    # type &amp;quot;BaseBuffer&amp;quot;; expected &amp;quot;Union[ReadBuffer[bytes],\n                    # WriteBuffer[bytes], None]&amp;quot;\n                    handle = _BytesTarFile(\n                        fileobj=handle, **compression_args  # type: ignore[arg-type]\n                    )\n                assert isinstance(handle, _BytesTarFile)\n                if &amp;quot;r&amp;quot; in handle.buffer.mode:\n                    handles.append(handle)\n                    files = handle.buffer.getnames()\n                    if len(files) == 1:\n                        file = handle.buffer.extractfile(files[0])\n                        assert file is not None\n                        handle = file\n                    elif not files:\n                        raise ValueError(f&amp;quot;Zero files found in TAR archive {path_or_buf}&amp;quot;)\n                    else:\n                        raise ValueError(\n                            &amp;quot;Multiple files found in TAR archive. &amp;quot;\n                            f&amp;quot;Only one file per TAR archive: {files}&amp;quot;\n                        )\n    \n            # XZ Compression\n            elif compression == &amp;quot;xz&amp;quot;:\n                # error: Argument 1 to &amp;quot;LZMAFile&amp;quot; has incompatible type &amp;quot;Union[str,\n                # BaseBuffer]&amp;quot;; expected &amp;quot;Optional[Union[Union[str, bytes, PathLike[str],\n                # PathLike[bytes]], IO[bytes]], None]&amp;quot;\n                handle = get_lzma_file()(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n    \n            # Zstd Compression\n            elif compression == &amp;quot;zstd&amp;quot;:\n                zstd = import_optional_dependency(&amp;quot;zstandard&amp;quot;)\n                if &amp;quot;r&amp;quot; in ioargs.mode:\n                    open_args = {&amp;quot;dctx&amp;quot;: zstd.ZstdDecompressor(**compression_args)}\n                else:\n                    open_args = {&amp;quot;cctx&amp;quot;: zstd.ZstdCompressor(**compression_args)}\n                handle = zstd.open(\n                    handle,\n                    mode=ioargs.mode,\n                    **open_args,\n                )\n    \n            # Unrecognized Compression\n            else:\n                msg = f&amp;quot;Unrecognized compression type: {compression}&amp;quot;\n                raise ValueError(msg)\n    \n            assert not isinstance(handle, str)\n            handles.append(handle)\n    \n        elif isinstance(handle, str):\n            # Check whether the filename is to be opened in binary mode.\n            # Binary mode does not support &amp;#x27;encoding&amp;#x27; and &amp;#x27;newline&amp;#x27;.\n            if ioargs.encoding and &amp;quot;b&amp;quot; not in ioargs.mode:\n                # Encoding\n&amp;gt;               handle = open(\n                    handle,\n                    ioargs.mode,\n                    encoding=ioargs.encoding,\n                    errors=errors,\n                    newline=&amp;quot;&amp;quot;,\n                )\nE               FileNotFoundError: [Errno 2] No such file or directory: &amp;#x27;test_data/dataset_project_ds_nodata.csv&amp;#x27;\n\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/common.py:873: FileNotFoundError\n&#34;}], &#34;tests/test_classification_model.py::test_data_cleaning_pos&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_classification_model.py::test_data_cleaning_pos&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_classification_model.py::test_data_cleaning_pos&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_data_cleaning_pos():\n&amp;gt;       raw_data = pd.read_csv(&amp;quot;test_data/dataset_project_ds_nolabel.csv&amp;quot;)\n\ntests/test_classification_model.py:18: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026: in read_csv\n    return _read(filepath_or_buffer, kwds)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620: in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620: in __init__\n    self._engine = self._make_engine(f, self.engine)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880: in _make_engine\n    self.handles = get_handle(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\npath_or_buf = &amp;#x27;test_data/dataset_project_ds_nolabel.csv&amp;#x27;, mode = &amp;#x27;r&amp;#x27;\n\n    @doc(compression_options=_shared_docs[&amp;quot;compression_options&amp;quot;] % &amp;quot;path_or_buf&amp;quot;)\n    def get_handle(\n        path_or_buf: FilePath | BaseBuffer,\n        mode: str,\n        *,\n        encoding: str | None = None,\n        compression: CompressionOptions | None = None,\n        memory_map: bool = False,\n        is_text: bool = True,\n        errors: str | None = None,\n        storage_options: StorageOptions | None = None,\n    ) -&amp;gt; IOHandles[str] | IOHandles[bytes]:\n        &amp;quot;&amp;quot;&amp;quot;\n        Get file handle for given path/buffer and mode.\n    \n        Parameters\n        ----------\n        path_or_buf : str or file handle\n            File path or object.\n        mode : str\n            Mode to open path_or_buf with.\n        encoding : str or None\n            Encoding to use.\n        {compression_options}\n    \n               May be a dict with key &amp;#x27;method&amp;#x27; as compression mode\n               and other keys as compression options if compression\n               mode is &amp;#x27;zip&amp;#x27;.\n    \n               Passing compression options as keys in dict is\n               supported for compression modes &amp;#x27;gzip&amp;#x27;, &amp;#x27;bz2&amp;#x27;, &amp;#x27;zstd&amp;#x27; and &amp;#x27;zip&amp;#x27;.\n    \n            .. versionchanged:: 1.4.0 Zstandard support.\n    \n        memory_map : bool, default False\n            See parsers._parser_params for more information. Only used by read_csv.\n        is_text : bool, default True\n            Whether the type of the content passed to the file/buffer is string or\n            bytes. This is not the same as `&amp;quot;b&amp;quot; not in mode`. If a string content is\n            passed to a binary file/buffer, a wrapper is inserted.\n        errors : str, default &amp;#x27;strict&amp;#x27;\n            Specifies how encoding and decoding errors are to be handled.\n            See the errors argument for :func:`open` for a full list\n            of options.\n        storage_options: StorageOptions = None\n            Passed to _get_filepath_or_buffer\n    \n        Returns the dataclass IOHandles\n        &amp;quot;&amp;quot;&amp;quot;\n        # Windows does not default to utf-8. Set to utf-8 for a consistent behavior\n        encoding = encoding or &amp;quot;utf-8&amp;quot;\n    \n        errors = errors or &amp;quot;strict&amp;quot;\n    \n        # read_csv does not know whether the buffer is opened in binary/text mode\n        if _is_binary_mode(path_or_buf, mode) and &amp;quot;b&amp;quot; not in mode:\n            mode += &amp;quot;b&amp;quot;\n    \n        # validate encoding and errors\n        codecs.lookup(encoding)\n        if isinstance(errors, str):\n            codecs.lookup_error(errors)\n    \n        # open URLs\n        ioargs = _get_filepath_or_buffer(\n            path_or_buf,\n            encoding=encoding,\n            compression=compression,\n            mode=mode,\n            storage_options=storage_options,\n        )\n    \n        handle = ioargs.filepath_or_buffer\n        handles: list[BaseBuffer]\n    \n        # memory mapping needs to be the first step\n        # only used for read_csv\n        handle, memory_map, handles = _maybe_memory_map(handle, memory_map)\n    \n        is_path = isinstance(handle, str)\n        compression_args = dict(ioargs.compression)\n        compression = compression_args.pop(&amp;quot;method&amp;quot;)\n    \n        # Only for write methods\n        if &amp;quot;r&amp;quot; not in mode and is_path:\n            check_parent_directory(str(handle))\n    \n        if compression:\n            if compression != &amp;quot;zstd&amp;quot;:\n                # compression libraries do not like an explicit text-mode\n                ioargs.mode = ioargs.mode.replace(&amp;quot;t&amp;quot;, &amp;quot;&amp;quot;)\n            elif compression == &amp;quot;zstd&amp;quot; and &amp;quot;b&amp;quot; not in ioargs.mode:\n                # python-zstandard defaults to text mode, but we always expect\n                # compression libraries to use binary mode.\n                ioargs.mode += &amp;quot;b&amp;quot;\n    \n            # GZ Compression\n            if compression == &amp;quot;gzip&amp;quot;:\n                if isinstance(handle, str):\n                    # error: Incompatible types in assignment (expression has type\n                    # &amp;quot;GzipFile&amp;quot;, variable has type &amp;quot;Union[str, BaseBuffer]&amp;quot;)\n                    handle = gzip.GzipFile(  # type: ignore[assignment]\n                        filename=handle,\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n                else:\n                    handle = gzip.GzipFile(\n                        # No overload variant of &amp;quot;GzipFile&amp;quot; matches argument types\n                        # &amp;quot;Union[str, BaseBuffer]&amp;quot;, &amp;quot;str&amp;quot;, &amp;quot;Dict[str, Any]&amp;quot;\n                        fileobj=handle,  # type: ignore[call-overload]\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n    \n            # BZ Compression\n            elif compression == &amp;quot;bz2&amp;quot;:\n                # Overload of &amp;quot;BZ2File&amp;quot; to handle pickle protocol 5\n                # &amp;quot;Union[str, BaseBuffer]&amp;quot;, &amp;quot;str&amp;quot;, &amp;quot;Dict[str, Any]&amp;quot;\n                handle = get_bz2_file()(  # type: ignore[call-overload]\n                    handle,\n                    mode=ioargs.mode,\n                    **compression_args,\n                )\n    \n            # ZIP Compression\n            elif compression == &amp;quot;zip&amp;quot;:\n                # error: Argument 1 to &amp;quot;_BytesZipFile&amp;quot; has incompatible type\n                # &amp;quot;Union[str, BaseBuffer]&amp;quot;; expected &amp;quot;Union[Union[str, PathLike[str]],\n                # ReadBuffer[bytes], WriteBuffer[bytes]]&amp;quot;\n                handle = _BytesZipFile(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n                if handle.buffer.mode == &amp;quot;r&amp;quot;:\n                    handles.append(handle)\n                    zip_names = handle.buffer.namelist()\n                    if len(zip_names) == 1:\n                        handle = handle.buffer.open(zip_names.pop())\n                    elif not zip_names:\n                        raise ValueError(f&amp;quot;Zero files found in ZIP file {path_or_buf}&amp;quot;)\n                    else:\n                        raise ValueError(\n                            &amp;quot;Multiple files found in ZIP file. &amp;quot;\n                            f&amp;quot;Only one file per ZIP: {zip_names}&amp;quot;\n                        )\n    \n            # TAR Encoding\n            elif compression == &amp;quot;tar&amp;quot;:\n                compression_args.setdefault(&amp;quot;mode&amp;quot;, ioargs.mode)\n                if isinstance(handle, str):\n                    handle = _BytesTarFile(name=handle, **compression_args)\n                else:\n                    # error: Argument &amp;quot;fileobj&amp;quot; to &amp;quot;_BytesTarFile&amp;quot; has incompatible\n                    # type &amp;quot;BaseBuffer&amp;quot;; expected &amp;quot;Union[ReadBuffer[bytes],\n                    # WriteBuffer[bytes], None]&amp;quot;\n                    handle = _BytesTarFile(\n                        fileobj=handle, **compression_args  # type: ignore[arg-type]\n                    )\n                assert isinstance(handle, _BytesTarFile)\n                if &amp;quot;r&amp;quot; in handle.buffer.mode:\n                    handles.append(handle)\n                    files = handle.buffer.getnames()\n                    if len(files) == 1:\n                        file = handle.buffer.extractfile(files[0])\n                        assert file is not None\n                        handle = file\n                    elif not files:\n                        raise ValueError(f&amp;quot;Zero files found in TAR archive {path_or_buf}&amp;quot;)\n                    else:\n                        raise ValueError(\n                            &amp;quot;Multiple files found in TAR archive. &amp;quot;\n                            f&amp;quot;Only one file per TAR archive: {files}&amp;quot;\n                        )\n    \n            # XZ Compression\n            elif compression == &amp;quot;xz&amp;quot;:\n                # error: Argument 1 to &amp;quot;LZMAFile&amp;quot; has incompatible type &amp;quot;Union[str,\n                # BaseBuffer]&amp;quot;; expected &amp;quot;Optional[Union[Union[str, bytes, PathLike[str],\n                # PathLike[bytes]], IO[bytes]], None]&amp;quot;\n                handle = get_lzma_file()(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n    \n            # Zstd Compression\n            elif compression == &amp;quot;zstd&amp;quot;:\n                zstd = import_optional_dependency(&amp;quot;zstandard&amp;quot;)\n                if &amp;quot;r&amp;quot; in ioargs.mode:\n                    open_args = {&amp;quot;dctx&amp;quot;: zstd.ZstdDecompressor(**compression_args)}\n                else:\n                    open_args = {&amp;quot;cctx&amp;quot;: zstd.ZstdCompressor(**compression_args)}\n                handle = zstd.open(\n                    handle,\n                    mode=ioargs.mode,\n                    **open_args,\n                )\n    \n            # Unrecognized Compression\n            else:\n                msg = f&amp;quot;Unrecognized compression type: {compression}&amp;quot;\n                raise ValueError(msg)\n    \n            assert not isinstance(handle, str)\n            handles.append(handle)\n    \n        elif isinstance(handle, str):\n            # Check whether the filename is to be opened in binary mode.\n            # Binary mode does not support &amp;#x27;encoding&amp;#x27; and &amp;#x27;newline&amp;#x27;.\n            if ioargs.encoding and &amp;quot;b&amp;quot; not in ioargs.mode:\n                # Encoding\n&amp;gt;               handle = open(\n                    handle,\n                    ioargs.mode,\n                    encoding=ioargs.encoding,\n                    errors=errors,\n                    newline=&amp;quot;&amp;quot;,\n                )\nE               FileNotFoundError: [Errno 2] No such file or directory: &amp;#x27;test_data/dataset_project_ds_nolabel.csv&amp;#x27;\n\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/common.py:873: FileNotFoundError\n&#34;}], &#34;tests/test_classification_model.py::test_data_cleaning_neg&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_classification_model.py::test_data_cleaning_neg&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_classification_model.py::test_data_cleaning_neg&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_data_cleaning_neg():\n&amp;gt;       raw_data = pd.read_csv(&amp;quot;test_data/dataset_project_ds_nodata.csv&amp;quot;)\n\ntests/test_classification_model.py:28: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026: in read_csv\n    return _read(filepath_or_buffer, kwds)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620: in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620: in __init__\n    self._engine = self._make_engine(f, self.engine)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880: in _make_engine\n    self.handles = get_handle(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\npath_or_buf = &amp;#x27;test_data/dataset_project_ds_nodata.csv&amp;#x27;, mode = &amp;#x27;r&amp;#x27;\n\n    @doc(compression_options=_shared_docs[&amp;quot;compression_options&amp;quot;] % &amp;quot;path_or_buf&amp;quot;)\n    def get_handle(\n        path_or_buf: FilePath | BaseBuffer,\n        mode: str,\n        *,\n        encoding: str | None = None,\n        compression: CompressionOptions | None = None,\n        memory_map: bool = False,\n        is_text: bool = True,\n        errors: str | None = None,\n        storage_options: StorageOptions | None = None,\n    ) -&amp;gt; IOHandles[str] | IOHandles[bytes]:\n        &amp;quot;&amp;quot;&amp;quot;\n        Get file handle for given path/buffer and mode.\n    \n        Parameters\n        ----------\n        path_or_buf : str or file handle\n            File path or object.\n        mode : str\n            Mode to open path_or_buf with.\n        encoding : str or None\n            Encoding to use.\n        {compression_options}\n    \n               May be a dict with key &amp;#x27;method&amp;#x27; as compression mode\n               and other keys as compression options if compression\n               mode is &amp;#x27;zip&amp;#x27;.\n    \n               Passing compression options as keys in dict is\n               supported for compression modes &amp;#x27;gzip&amp;#x27;, &amp;#x27;bz2&amp;#x27;, &amp;#x27;zstd&amp;#x27; and &amp;#x27;zip&amp;#x27;.\n    \n            .. versionchanged:: 1.4.0 Zstandard support.\n    \n        memory_map : bool, default False\n            See parsers._parser_params for more information. Only used by read_csv.\n        is_text : bool, default True\n            Whether the type of the content passed to the file/buffer is string or\n            bytes. This is not the same as `&amp;quot;b&amp;quot; not in mode`. If a string content is\n            passed to a binary file/buffer, a wrapper is inserted.\n        errors : str, default &amp;#x27;strict&amp;#x27;\n            Specifies how encoding and decoding errors are to be handled.\n            See the errors argument for :func:`open` for a full list\n            of options.\n        storage_options: StorageOptions = None\n            Passed to _get_filepath_or_buffer\n    \n        Returns the dataclass IOHandles\n        &amp;quot;&amp;quot;&amp;quot;\n        # Windows does not default to utf-8. Set to utf-8 for a consistent behavior\n        encoding = encoding or &amp;quot;utf-8&amp;quot;\n    \n        errors = errors or &amp;quot;strict&amp;quot;\n    \n        # read_csv does not know whether the buffer is opened in binary/text mode\n        if _is_binary_mode(path_or_buf, mode) and &amp;quot;b&amp;quot; not in mode:\n            mode += &amp;quot;b&amp;quot;\n    \n        # validate encoding and errors\n        codecs.lookup(encoding)\n        if isinstance(errors, str):\n            codecs.lookup_error(errors)\n    \n        # open URLs\n        ioargs = _get_filepath_or_buffer(\n            path_or_buf,\n            encoding=encoding,\n            compression=compression,\n            mode=mode,\n            storage_options=storage_options,\n        )\n    \n        handle = ioargs.filepath_or_buffer\n        handles: list[BaseBuffer]\n    \n        # memory mapping needs to be the first step\n        # only used for read_csv\n        handle, memory_map, handles = _maybe_memory_map(handle, memory_map)\n    \n        is_path = isinstance(handle, str)\n        compression_args = dict(ioargs.compression)\n        compression = compression_args.pop(&amp;quot;method&amp;quot;)\n    \n        # Only for write methods\n        if &amp;quot;r&amp;quot; not in mode and is_path:\n            check_parent_directory(str(handle))\n    \n        if compression:\n            if compression != &amp;quot;zstd&amp;quot;:\n                # compression libraries do not like an explicit text-mode\n                ioargs.mode = ioargs.mode.replace(&amp;quot;t&amp;quot;, &amp;quot;&amp;quot;)\n            elif compression == &amp;quot;zstd&amp;quot; and &amp;quot;b&amp;quot; not in ioargs.mode:\n                # python-zstandard defaults to text mode, but we always expect\n                # compression libraries to use binary mode.\n                ioargs.mode += &amp;quot;b&amp;quot;\n    \n            # GZ Compression\n            if compression == &amp;quot;gzip&amp;quot;:\n                if isinstance(handle, str):\n                    # error: Incompatible types in assignment (expression has type\n                    # &amp;quot;GzipFile&amp;quot;, variable has type &amp;quot;Union[str, BaseBuffer]&amp;quot;)\n                    handle = gzip.GzipFile(  # type: ignore[assignment]\n                        filename=handle,\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n                else:\n                    handle = gzip.GzipFile(\n                        # No overload variant of &amp;quot;GzipFile&amp;quot; matches argument types\n                        # &amp;quot;Union[str, BaseBuffer]&amp;quot;, &amp;quot;str&amp;quot;, &amp;quot;Dict[str, Any]&amp;quot;\n                        fileobj=handle,  # type: ignore[call-overload]\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n    \n            # BZ Compression\n            elif compression == &amp;quot;bz2&amp;quot;:\n                # Overload of &amp;quot;BZ2File&amp;quot; to handle pickle protocol 5\n                # &amp;quot;Union[str, BaseBuffer]&amp;quot;, &amp;quot;str&amp;quot;, &amp;quot;Dict[str, Any]&amp;quot;\n                handle = get_bz2_file()(  # type: ignore[call-overload]\n                    handle,\n                    mode=ioargs.mode,\n                    **compression_args,\n                )\n    \n            # ZIP Compression\n            elif compression == &amp;quot;zip&amp;quot;:\n                # error: Argument 1 to &amp;quot;_BytesZipFile&amp;quot; has incompatible type\n                # &amp;quot;Union[str, BaseBuffer]&amp;quot;; expected &amp;quot;Union[Union[str, PathLike[str]],\n                # ReadBuffer[bytes], WriteBuffer[bytes]]&amp;quot;\n                handle = _BytesZipFile(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n                if handle.buffer.mode == &amp;quot;r&amp;quot;:\n                    handles.append(handle)\n                    zip_names = handle.buffer.namelist()\n                    if len(zip_names) == 1:\n                        handle = handle.buffer.open(zip_names.pop())\n                    elif not zip_names:\n                        raise ValueError(f&amp;quot;Zero files found in ZIP file {path_or_buf}&amp;quot;)\n                    else:\n                        raise ValueError(\n                            &amp;quot;Multiple files found in ZIP file. &amp;quot;\n                            f&amp;quot;Only one file per ZIP: {zip_names}&amp;quot;\n                        )\n    \n            # TAR Encoding\n            elif compression == &amp;quot;tar&amp;quot;:\n                compression_args.setdefault(&amp;quot;mode&amp;quot;, ioargs.mode)\n                if isinstance(handle, str):\n                    handle = _BytesTarFile(name=handle, **compression_args)\n                else:\n                    # error: Argument &amp;quot;fileobj&amp;quot; to &amp;quot;_BytesTarFile&amp;quot; has incompatible\n                    # type &amp;quot;BaseBuffer&amp;quot;; expected &amp;quot;Union[ReadBuffer[bytes],\n                    # WriteBuffer[bytes], None]&amp;quot;\n                    handle = _BytesTarFile(\n                        fileobj=handle, **compression_args  # type: ignore[arg-type]\n                    )\n                assert isinstance(handle, _BytesTarFile)\n                if &amp;quot;r&amp;quot; in handle.buffer.mode:\n                    handles.append(handle)\n                    files = handle.buffer.getnames()\n                    if len(files) == 1:\n                        file = handle.buffer.extractfile(files[0])\n                        assert file is not None\n                        handle = file\n                    elif not files:\n                        raise ValueError(f&amp;quot;Zero files found in TAR archive {path_or_buf}&amp;quot;)\n                    else:\n                        raise ValueError(\n                            &amp;quot;Multiple files found in TAR archive. &amp;quot;\n                            f&amp;quot;Only one file per TAR archive: {files}&amp;quot;\n                        )\n    \n            # XZ Compression\n            elif compression == &amp;quot;xz&amp;quot;:\n                # error: Argument 1 to &amp;quot;LZMAFile&amp;quot; has incompatible type &amp;quot;Union[str,\n                # BaseBuffer]&amp;quot;; expected &amp;quot;Optional[Union[Union[str, bytes, PathLike[str],\n                # PathLike[bytes]], IO[bytes]], None]&amp;quot;\n                handle = get_lzma_file()(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n    \n            # Zstd Compression\n            elif compression == &amp;quot;zstd&amp;quot;:\n                zstd = import_optional_dependency(&amp;quot;zstandard&amp;quot;)\n                if &amp;quot;r&amp;quot; in ioargs.mode:\n                    open_args = {&amp;quot;dctx&amp;quot;: zstd.ZstdDecompressor(**compression_args)}\n                else:\n                    open_args = {&amp;quot;cctx&amp;quot;: zstd.ZstdCompressor(**compression_args)}\n                handle = zstd.open(\n                    handle,\n                    mode=ioargs.mode,\n                    **open_args,\n                )\n    \n            # Unrecognized Compression\n            else:\n                msg = f&amp;quot;Unrecognized compression type: {compression}&amp;quot;\n                raise ValueError(msg)\n    \n            assert not isinstance(handle, str)\n            handles.append(handle)\n    \n        elif isinstance(handle, str):\n            # Check whether the filename is to be opened in binary mode.\n            # Binary mode does not support &amp;#x27;encoding&amp;#x27; and &amp;#x27;newline&amp;#x27;.\n            if ioargs.encoding and &amp;quot;b&amp;quot; not in ioargs.mode:\n                # Encoding\n&amp;gt;               handle = open(\n                    handle,\n                    ioargs.mode,\n                    encoding=ioargs.encoding,\n                    errors=errors,\n                    newline=&amp;quot;&amp;quot;,\n                )\nE               FileNotFoundError: [Errno 2] No such file or directory: &amp;#x27;test_data/dataset_project_ds_nodata.csv&amp;#x27;\n\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/common.py:873: FileNotFoundError\n&#34;}], &#34;tests/test_classification_model.py::test_data_splitting_pos&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_classification_model.py::test_data_splitting_pos&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_classification_model.py::test_data_splitting_pos&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_data_splitting_pos():\n&amp;gt;       raw_data = pd.read_csv(&amp;quot;test_data/dataset_project_ds_nolabel.csv&amp;quot;)\n\ntests/test_classification_model.py:35: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026: in read_csv\n    return _read(filepath_or_buffer, kwds)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620: in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620: in __init__\n    self._engine = self._make_engine(f, self.engine)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880: in _make_engine\n    self.handles = get_handle(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\npath_or_buf = &amp;#x27;test_data/dataset_project_ds_nolabel.csv&amp;#x27;, mode = &amp;#x27;r&amp;#x27;\n\n    @doc(compression_options=_shared_docs[&amp;quot;compression_options&amp;quot;] % &amp;quot;path_or_buf&amp;quot;)\n    def get_handle(\n        path_or_buf: FilePath | BaseBuffer,\n        mode: str,\n        *,\n        encoding: str | None = None,\n        compression: CompressionOptions | None = None,\n        memory_map: bool = False,\n        is_text: bool = True,\n        errors: str | None = None,\n        storage_options: StorageOptions | None = None,\n    ) -&amp;gt; IOHandles[str] | IOHandles[bytes]:\n        &amp;quot;&amp;quot;&amp;quot;\n        Get file handle for given path/buffer and mode.\n    \n        Parameters\n        ----------\n        path_or_buf : str or file handle\n            File path or object.\n        mode : str\n            Mode to open path_or_buf with.\n        encoding : str or None\n            Encoding to use.\n        {compression_options}\n    \n               May be a dict with key &amp;#x27;method&amp;#x27; as compression mode\n               and other keys as compression options if compression\n               mode is &amp;#x27;zip&amp;#x27;.\n    \n               Passing compression options as keys in dict is\n               supported for compression modes &amp;#x27;gzip&amp;#x27;, &amp;#x27;bz2&amp;#x27;, &amp;#x27;zstd&amp;#x27; and &amp;#x27;zip&amp;#x27;.\n    \n            .. versionchanged:: 1.4.0 Zstandard support.\n    \n        memory_map : bool, default False\n            See parsers._parser_params for more information. Only used by read_csv.\n        is_text : bool, default True\n            Whether the type of the content passed to the file/buffer is string or\n            bytes. This is not the same as `&amp;quot;b&amp;quot; not in mode`. If a string content is\n            passed to a binary file/buffer, a wrapper is inserted.\n        errors : str, default &amp;#x27;strict&amp;#x27;\n            Specifies how encoding and decoding errors are to be handled.\n            See the errors argument for :func:`open` for a full list\n            of options.\n        storage_options: StorageOptions = None\n            Passed to _get_filepath_or_buffer\n    \n        Returns the dataclass IOHandles\n        &amp;quot;&amp;quot;&amp;quot;\n        # Windows does not default to utf-8. Set to utf-8 for a consistent behavior\n        encoding = encoding or &amp;quot;utf-8&amp;quot;\n    \n        errors = errors or &amp;quot;strict&amp;quot;\n    \n        # read_csv does not know whether the buffer is opened in binary/text mode\n        if _is_binary_mode(path_or_buf, mode) and &amp;quot;b&amp;quot; not in mode:\n            mode += &amp;quot;b&amp;quot;\n    \n        # validate encoding and errors\n        codecs.lookup(encoding)\n        if isinstance(errors, str):\n            codecs.lookup_error(errors)\n    \n        # open URLs\n        ioargs = _get_filepath_or_buffer(\n            path_or_buf,\n            encoding=encoding,\n            compression=compression,\n            mode=mode,\n            storage_options=storage_options,\n        )\n    \n        handle = ioargs.filepath_or_buffer\n        handles: list[BaseBuffer]\n    \n        # memory mapping needs to be the first step\n        # only used for read_csv\n        handle, memory_map, handles = _maybe_memory_map(handle, memory_map)\n    \n        is_path = isinstance(handle, str)\n        compression_args = dict(ioargs.compression)\n        compression = compression_args.pop(&amp;quot;method&amp;quot;)\n    \n        # Only for write methods\n        if &amp;quot;r&amp;quot; not in mode and is_path:\n            check_parent_directory(str(handle))\n    \n        if compression:\n            if compression != &amp;quot;zstd&amp;quot;:\n                # compression libraries do not like an explicit text-mode\n                ioargs.mode = ioargs.mode.replace(&amp;quot;t&amp;quot;, &amp;quot;&amp;quot;)\n            elif compression == &amp;quot;zstd&amp;quot; and &amp;quot;b&amp;quot; not in ioargs.mode:\n                # python-zstandard defaults to text mode, but we always expect\n                # compression libraries to use binary mode.\n                ioargs.mode += &amp;quot;b&amp;quot;\n    \n            # GZ Compression\n            if compression == &amp;quot;gzip&amp;quot;:\n                if isinstance(handle, str):\n                    # error: Incompatible types in assignment (expression has type\n                    # &amp;quot;GzipFile&amp;quot;, variable has type &amp;quot;Union[str, BaseBuffer]&amp;quot;)\n                    handle = gzip.GzipFile(  # type: ignore[assignment]\n                        filename=handle,\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n                else:\n                    handle = gzip.GzipFile(\n                        # No overload variant of &amp;quot;GzipFile&amp;quot; matches argument types\n                        # &amp;quot;Union[str, BaseBuffer]&amp;quot;, &amp;quot;str&amp;quot;, &amp;quot;Dict[str, Any]&amp;quot;\n                        fileobj=handle,  # type: ignore[call-overload]\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n    \n            # BZ Compression\n            elif compression == &amp;quot;bz2&amp;quot;:\n                # Overload of &amp;quot;BZ2File&amp;quot; to handle pickle protocol 5\n                # &amp;quot;Union[str, BaseBuffer]&amp;quot;, &amp;quot;str&amp;quot;, &amp;quot;Dict[str, Any]&amp;quot;\n                handle = get_bz2_file()(  # type: ignore[call-overload]\n                    handle,\n                    mode=ioargs.mode,\n                    **compression_args,\n                )\n    \n            # ZIP Compression\n            elif compression == &amp;quot;zip&amp;quot;:\n                # error: Argument 1 to &amp;quot;_BytesZipFile&amp;quot; has incompatible type\n                # &amp;quot;Union[str, BaseBuffer]&amp;quot;; expected &amp;quot;Union[Union[str, PathLike[str]],\n                # ReadBuffer[bytes], WriteBuffer[bytes]]&amp;quot;\n                handle = _BytesZipFile(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n                if handle.buffer.mode == &amp;quot;r&amp;quot;:\n                    handles.append(handle)\n                    zip_names = handle.buffer.namelist()\n                    if len(zip_names) == 1:\n                        handle = handle.buffer.open(zip_names.pop())\n                    elif not zip_names:\n                        raise ValueError(f&amp;quot;Zero files found in ZIP file {path_or_buf}&amp;quot;)\n                    else:\n                        raise ValueError(\n                            &amp;quot;Multiple files found in ZIP file. &amp;quot;\n                            f&amp;quot;Only one file per ZIP: {zip_names}&amp;quot;\n                        )\n    \n            # TAR Encoding\n            elif compression == &amp;quot;tar&amp;quot;:\n                compression_args.setdefault(&amp;quot;mode&amp;quot;, ioargs.mode)\n                if isinstance(handle, str):\n                    handle = _BytesTarFile(name=handle, **compression_args)\n                else:\n                    # error: Argument &amp;quot;fileobj&amp;quot; to &amp;quot;_BytesTarFile&amp;quot; has incompatible\n                    # type &amp;quot;BaseBuffer&amp;quot;; expected &amp;quot;Union[ReadBuffer[bytes],\n                    # WriteBuffer[bytes], None]&amp;quot;\n                    handle = _BytesTarFile(\n                        fileobj=handle, **compression_args  # type: ignore[arg-type]\n                    )\n                assert isinstance(handle, _BytesTarFile)\n                if &amp;quot;r&amp;quot; in handle.buffer.mode:\n                    handles.append(handle)\n                    files = handle.buffer.getnames()\n                    if len(files) == 1:\n                        file = handle.buffer.extractfile(files[0])\n                        assert file is not None\n                        handle = file\n                    elif not files:\n                        raise ValueError(f&amp;quot;Zero files found in TAR archive {path_or_buf}&amp;quot;)\n                    else:\n                        raise ValueError(\n                            &amp;quot;Multiple files found in TAR archive. &amp;quot;\n                            f&amp;quot;Only one file per TAR archive: {files}&amp;quot;\n                        )\n    \n            # XZ Compression\n            elif compression == &amp;quot;xz&amp;quot;:\n                # error: Argument 1 to &amp;quot;LZMAFile&amp;quot; has incompatible type &amp;quot;Union[str,\n                # BaseBuffer]&amp;quot;; expected &amp;quot;Optional[Union[Union[str, bytes, PathLike[str],\n                # PathLike[bytes]], IO[bytes]], None]&amp;quot;\n                handle = get_lzma_file()(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n    \n            # Zstd Compression\n            elif compression == &amp;quot;zstd&amp;quot;:\n                zstd = import_optional_dependency(&amp;quot;zstandard&amp;quot;)\n                if &amp;quot;r&amp;quot; in ioargs.mode:\n                    open_args = {&amp;quot;dctx&amp;quot;: zstd.ZstdDecompressor(**compression_args)}\n                else:\n                    open_args = {&amp;quot;cctx&amp;quot;: zstd.ZstdCompressor(**compression_args)}\n                handle = zstd.open(\n                    handle,\n                    mode=ioargs.mode,\n                    **open_args,\n                )\n    \n            # Unrecognized Compression\n            else:\n                msg = f&amp;quot;Unrecognized compression type: {compression}&amp;quot;\n                raise ValueError(msg)\n    \n            assert not isinstance(handle, str)\n            handles.append(handle)\n    \n        elif isinstance(handle, str):\n            # Check whether the filename is to be opened in binary mode.\n            # Binary mode does not support &amp;#x27;encoding&amp;#x27; and &amp;#x27;newline&amp;#x27;.\n            if ioargs.encoding and &amp;quot;b&amp;quot; not in ioargs.mode:\n                # Encoding\n&amp;gt;               handle = open(\n                    handle,\n                    ioargs.mode,\n                    encoding=ioargs.encoding,\n                    errors=errors,\n                    newline=&amp;quot;&amp;quot;,\n                )\nE               FileNotFoundError: [Errno 2] No such file or directory: &amp;#x27;test_data/dataset_project_ds_nolabel.csv&amp;#x27;\n\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/common.py:873: FileNotFoundError\n&#34;}], &#34;tests/test_classification_model.py::test_data_splitting_neg&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_classification_model.py::test_data_splitting_neg&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_classification_model.py::test_data_splitting_neg&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_data_splitting_neg():\n&amp;gt;       raw_data = pd.read_csv(&amp;quot;test_data/dataset_project_ds_nodata.csv&amp;quot;)\n\ntests/test_classification_model.py:43: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026: in read_csv\n    return _read(filepath_or_buffer, kwds)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620: in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620: in __init__\n    self._engine = self._make_engine(f, self.engine)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880: in _make_engine\n    self.handles = get_handle(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\npath_or_buf = &amp;#x27;test_data/dataset_project_ds_nodata.csv&amp;#x27;, mode = &amp;#x27;r&amp;#x27;\n\n    @doc(compression_options=_shared_docs[&amp;quot;compression_options&amp;quot;] % &amp;quot;path_or_buf&amp;quot;)\n    def get_handle(\n        path_or_buf: FilePath | BaseBuffer,\n        mode: str,\n        *,\n        encoding: str | None = None,\n        compression: CompressionOptions | None = None,\n        memory_map: bool = False,\n        is_text: bool = True,\n        errors: str | None = None,\n        storage_options: StorageOptions | None = None,\n    ) -&amp;gt; IOHandles[str] | IOHandles[bytes]:\n        &amp;quot;&amp;quot;&amp;quot;\n        Get file handle for given path/buffer and mode.\n    \n        Parameters\n        ----------\n        path_or_buf : str or file handle\n            File path or object.\n        mode : str\n            Mode to open path_or_buf with.\n        encoding : str or None\n            Encoding to use.\n        {compression_options}\n    \n               May be a dict with key &amp;#x27;method&amp;#x27; as compression mode\n               and other keys as compression options if compression\n               mode is &amp;#x27;zip&amp;#x27;.\n    \n               Passing compression options as keys in dict is\n               supported for compression modes &amp;#x27;gzip&amp;#x27;, &amp;#x27;bz2&amp;#x27;, &amp;#x27;zstd&amp;#x27; and &amp;#x27;zip&amp;#x27;.\n    \n            .. versionchanged:: 1.4.0 Zstandard support.\n    \n        memory_map : bool, default False\n            See parsers._parser_params for more information. Only used by read_csv.\n        is_text : bool, default True\n            Whether the type of the content passed to the file/buffer is string or\n            bytes. This is not the same as `&amp;quot;b&amp;quot; not in mode`. If a string content is\n            passed to a binary file/buffer, a wrapper is inserted.\n        errors : str, default &amp;#x27;strict&amp;#x27;\n            Specifies how encoding and decoding errors are to be handled.\n            See the errors argument for :func:`open` for a full list\n            of options.\n        storage_options: StorageOptions = None\n            Passed to _get_filepath_or_buffer\n    \n        Returns the dataclass IOHandles\n        &amp;quot;&amp;quot;&amp;quot;\n        # Windows does not default to utf-8. Set to utf-8 for a consistent behavior\n        encoding = encoding or &amp;quot;utf-8&amp;quot;\n    \n        errors = errors or &amp;quot;strict&amp;quot;\n    \n        # read_csv does not know whether the buffer is opened in binary/text mode\n        if _is_binary_mode(path_or_buf, mode) and &amp;quot;b&amp;quot; not in mode:\n            mode += &amp;quot;b&amp;quot;\n    \n        # validate encoding and errors\n        codecs.lookup(encoding)\n        if isinstance(errors, str):\n            codecs.lookup_error(errors)\n    \n        # open URLs\n        ioargs = _get_filepath_or_buffer(\n            path_or_buf,\n            encoding=encoding,\n            compression=compression,\n            mode=mode,\n            storage_options=storage_options,\n        )\n    \n        handle = ioargs.filepath_or_buffer\n        handles: list[BaseBuffer]\n    \n        # memory mapping needs to be the first step\n        # only used for read_csv\n        handle, memory_map, handles = _maybe_memory_map(handle, memory_map)\n    \n        is_path = isinstance(handle, str)\n        compression_args = dict(ioargs.compression)\n        compression = compression_args.pop(&amp;quot;method&amp;quot;)\n    \n        # Only for write methods\n        if &amp;quot;r&amp;quot; not in mode and is_path:\n            check_parent_directory(str(handle))\n    \n        if compression:\n            if compression != &amp;quot;zstd&amp;quot;:\n                # compression libraries do not like an explicit text-mode\n                ioargs.mode = ioargs.mode.replace(&amp;quot;t&amp;quot;, &amp;quot;&amp;quot;)\n            elif compression == &amp;quot;zstd&amp;quot; and &amp;quot;b&amp;quot; not in ioargs.mode:\n                # python-zstandard defaults to text mode, but we always expect\n                # compression libraries to use binary mode.\n                ioargs.mode += &amp;quot;b&amp;quot;\n    \n            # GZ Compression\n            if compression == &amp;quot;gzip&amp;quot;:\n                if isinstance(handle, str):\n                    # error: Incompatible types in assignment (expression has type\n                    # &amp;quot;GzipFile&amp;quot;, variable has type &amp;quot;Union[str, BaseBuffer]&amp;quot;)\n                    handle = gzip.GzipFile(  # type: ignore[assignment]\n                        filename=handle,\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n                else:\n                    handle = gzip.GzipFile(\n                        # No overload variant of &amp;quot;GzipFile&amp;quot; matches argument types\n                        # &amp;quot;Union[str, BaseBuffer]&amp;quot;, &amp;quot;str&amp;quot;, &amp;quot;Dict[str, Any]&amp;quot;\n                        fileobj=handle,  # type: ignore[call-overload]\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n    \n            # BZ Compression\n            elif compression == &amp;quot;bz2&amp;quot;:\n                # Overload of &amp;quot;BZ2File&amp;quot; to handle pickle protocol 5\n                # &amp;quot;Union[str, BaseBuffer]&amp;quot;, &amp;quot;str&amp;quot;, &amp;quot;Dict[str, Any]&amp;quot;\n                handle = get_bz2_file()(  # type: ignore[call-overload]\n                    handle,\n                    mode=ioargs.mode,\n                    **compression_args,\n                )\n    \n            # ZIP Compression\n            elif compression == &amp;quot;zip&amp;quot;:\n                # error: Argument 1 to &amp;quot;_BytesZipFile&amp;quot; has incompatible type\n                # &amp;quot;Union[str, BaseBuffer]&amp;quot;; expected &amp;quot;Union[Union[str, PathLike[str]],\n                # ReadBuffer[bytes], WriteBuffer[bytes]]&amp;quot;\n                handle = _BytesZipFile(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n                if handle.buffer.mode == &amp;quot;r&amp;quot;:\n                    handles.append(handle)\n                    zip_names = handle.buffer.namelist()\n                    if len(zip_names) == 1:\n                        handle = handle.buffer.open(zip_names.pop())\n                    elif not zip_names:\n                        raise ValueError(f&amp;quot;Zero files found in ZIP file {path_or_buf}&amp;quot;)\n                    else:\n                        raise ValueError(\n                            &amp;quot;Multiple files found in ZIP file. &amp;quot;\n                            f&amp;quot;Only one file per ZIP: {zip_names}&amp;quot;\n                        )\n    \n            # TAR Encoding\n            elif compression == &amp;quot;tar&amp;quot;:\n                compression_args.setdefault(&amp;quot;mode&amp;quot;, ioargs.mode)\n                if isinstance(handle, str):\n                    handle = _BytesTarFile(name=handle, **compression_args)\n                else:\n                    # error: Argument &amp;quot;fileobj&amp;quot; to &amp;quot;_BytesTarFile&amp;quot; has incompatible\n                    # type &amp;quot;BaseBuffer&amp;quot;; expected &amp;quot;Union[ReadBuffer[bytes],\n                    # WriteBuffer[bytes], None]&amp;quot;\n                    handle = _BytesTarFile(\n                        fileobj=handle, **compression_args  # type: ignore[arg-type]\n                    )\n                assert isinstance(handle, _BytesTarFile)\n                if &amp;quot;r&amp;quot; in handle.buffer.mode:\n                    handles.append(handle)\n                    files = handle.buffer.getnames()\n                    if len(files) == 1:\n                        file = handle.buffer.extractfile(files[0])\n                        assert file is not None\n                        handle = file\n                    elif not files:\n                        raise ValueError(f&amp;quot;Zero files found in TAR archive {path_or_buf}&amp;quot;)\n                    else:\n                        raise ValueError(\n                            &amp;quot;Multiple files found in TAR archive. &amp;quot;\n                            f&amp;quot;Only one file per TAR archive: {files}&amp;quot;\n                        )\n    \n            # XZ Compression\n            elif compression == &amp;quot;xz&amp;quot;:\n                # error: Argument 1 to &amp;quot;LZMAFile&amp;quot; has incompatible type &amp;quot;Union[str,\n                # BaseBuffer]&amp;quot;; expected &amp;quot;Optional[Union[Union[str, bytes, PathLike[str],\n                # PathLike[bytes]], IO[bytes]], None]&amp;quot;\n                handle = get_lzma_file()(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n    \n            # Zstd Compression\n            elif compression == &amp;quot;zstd&amp;quot;:\n                zstd = import_optional_dependency(&amp;quot;zstandard&amp;quot;)\n                if &amp;quot;r&amp;quot; in ioargs.mode:\n                    open_args = {&amp;quot;dctx&amp;quot;: zstd.ZstdDecompressor(**compression_args)}\n                else:\n                    open_args = {&amp;quot;cctx&amp;quot;: zstd.ZstdCompressor(**compression_args)}\n                handle = zstd.open(\n                    handle,\n                    mode=ioargs.mode,\n                    **open_args,\n                )\n    \n            # Unrecognized Compression\n            else:\n                msg = f&amp;quot;Unrecognized compression type: {compression}&amp;quot;\n                raise ValueError(msg)\n    \n            assert not isinstance(handle, str)\n            handles.append(handle)\n    \n        elif isinstance(handle, str):\n            # Check whether the filename is to be opened in binary mode.\n            # Binary mode does not support &amp;#x27;encoding&amp;#x27; and &amp;#x27;newline&amp;#x27;.\n            if ioargs.encoding and &amp;quot;b&amp;quot; not in ioargs.mode:\n                # Encoding\n&amp;gt;               handle = open(\n                    handle,\n                    ioargs.mode,\n                    encoding=ioargs.encoding,\n                    errors=errors,\n                    newline=&amp;quot;&amp;quot;,\n                )\nE               FileNotFoundError: [Errno 2] No such file or directory: &amp;#x27;test_data/dataset_project_ds_nodata.csv&amp;#x27;\n\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/common.py:873: FileNotFoundError\n&#34;}], &#34;tests/test_classification_model.py::test_evaluate_model_pos&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_classification_model.py::test_evaluate_model_pos&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_classification_model.py::test_evaluate_model_pos&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_evaluate_model_pos():\n&amp;gt;       raw_data = pd.read_csv(&amp;quot;test_data/dataset_project_ds_nolabel.csv&amp;quot;)\n\ntests/test_classification_model.py:53: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026: in read_csv\n    return _read(filepath_or_buffer, kwds)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620: in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620: in __init__\n    self._engine = self._make_engine(f, self.engine)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880: in _make_engine\n    self.handles = get_handle(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\npath_or_buf = &amp;#x27;test_data/dataset_project_ds_nolabel.csv&amp;#x27;, mode = &amp;#x27;r&amp;#x27;\n\n    @doc(compression_options=_shared_docs[&amp;quot;compression_options&amp;quot;] % &amp;quot;path_or_buf&amp;quot;)\n    def get_handle(\n        path_or_buf: FilePath | BaseBuffer,\n        mode: str,\n        *,\n        encoding: str | None = None,\n        compression: CompressionOptions | None = None,\n        memory_map: bool = False,\n        is_text: bool = True,\n        errors: str | None = None,\n        storage_options: StorageOptions | None = None,\n    ) -&amp;gt; IOHandles[str] | IOHandles[bytes]:\n        &amp;quot;&amp;quot;&amp;quot;\n        Get file handle for given path/buffer and mode.\n    \n        Parameters\n        ----------\n        path_or_buf : str or file handle\n            File path or object.\n        mode : str\n            Mode to open path_or_buf with.\n        encoding : str or None\n            Encoding to use.\n        {compression_options}\n    \n               May be a dict with key &amp;#x27;method&amp;#x27; as compression mode\n               and other keys as compression options if compression\n               mode is &amp;#x27;zip&amp;#x27;.\n    \n               Passing compression options as keys in dict is\n               supported for compression modes &amp;#x27;gzip&amp;#x27;, &amp;#x27;bz2&amp;#x27;, &amp;#x27;zstd&amp;#x27; and &amp;#x27;zip&amp;#x27;.\n    \n            .. versionchanged:: 1.4.0 Zstandard support.\n    \n        memory_map : bool, default False\n            See parsers._parser_params for more information. Only used by read_csv.\n        is_text : bool, default True\n            Whether the type of the content passed to the file/buffer is string or\n            bytes. This is not the same as `&amp;quot;b&amp;quot; not in mode`. If a string content is\n            passed to a binary file/buffer, a wrapper is inserted.\n        errors : str, default &amp;#x27;strict&amp;#x27;\n            Specifies how encoding and decoding errors are to be handled.\n            See the errors argument for :func:`open` for a full list\n            of options.\n        storage_options: StorageOptions = None\n            Passed to _get_filepath_or_buffer\n    \n        Returns the dataclass IOHandles\n        &amp;quot;&amp;quot;&amp;quot;\n        # Windows does not default to utf-8. Set to utf-8 for a consistent behavior\n        encoding = encoding or &amp;quot;utf-8&amp;quot;\n    \n        errors = errors or &amp;quot;strict&amp;quot;\n    \n        # read_csv does not know whether the buffer is opened in binary/text mode\n        if _is_binary_mode(path_or_buf, mode) and &amp;quot;b&amp;quot; not in mode:\n            mode += &amp;quot;b&amp;quot;\n    \n        # validate encoding and errors\n        codecs.lookup(encoding)\n        if isinstance(errors, str):\n            codecs.lookup_error(errors)\n    \n        # open URLs\n        ioargs = _get_filepath_or_buffer(\n            path_or_buf,\n            encoding=encoding,\n            compression=compression,\n            mode=mode,\n            storage_options=storage_options,\n        )\n    \n        handle = ioargs.filepath_or_buffer\n        handles: list[BaseBuffer]\n    \n        # memory mapping needs to be the first step\n        # only used for read_csv\n        handle, memory_map, handles = _maybe_memory_map(handle, memory_map)\n    \n        is_path = isinstance(handle, str)\n        compression_args = dict(ioargs.compression)\n        compression = compression_args.pop(&amp;quot;method&amp;quot;)\n    \n        # Only for write methods\n        if &amp;quot;r&amp;quot; not in mode and is_path:\n            check_parent_directory(str(handle))\n    \n        if compression:\n            if compression != &amp;quot;zstd&amp;quot;:\n                # compression libraries do not like an explicit text-mode\n                ioargs.mode = ioargs.mode.replace(&amp;quot;t&amp;quot;, &amp;quot;&amp;quot;)\n            elif compression == &amp;quot;zstd&amp;quot; and &amp;quot;b&amp;quot; not in ioargs.mode:\n                # python-zstandard defaults to text mode, but we always expect\n                # compression libraries to use binary mode.\n                ioargs.mode += &amp;quot;b&amp;quot;\n    \n            # GZ Compression\n            if compression == &amp;quot;gzip&amp;quot;:\n                if isinstance(handle, str):\n                    # error: Incompatible types in assignment (expression has type\n                    # &amp;quot;GzipFile&amp;quot;, variable has type &amp;quot;Union[str, BaseBuffer]&amp;quot;)\n                    handle = gzip.GzipFile(  # type: ignore[assignment]\n                        filename=handle,\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n                else:\n                    handle = gzip.GzipFile(\n                        # No overload variant of &amp;quot;GzipFile&amp;quot; matches argument types\n                        # &amp;quot;Union[str, BaseBuffer]&amp;quot;, &amp;quot;str&amp;quot;, &amp;quot;Dict[str, Any]&amp;quot;\n                        fileobj=handle,  # type: ignore[call-overload]\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n    \n            # BZ Compression\n            elif compression == &amp;quot;bz2&amp;quot;:\n                # Overload of &amp;quot;BZ2File&amp;quot; to handle pickle protocol 5\n                # &amp;quot;Union[str, BaseBuffer]&amp;quot;, &amp;quot;str&amp;quot;, &amp;quot;Dict[str, Any]&amp;quot;\n                handle = get_bz2_file()(  # type: ignore[call-overload]\n                    handle,\n                    mode=ioargs.mode,\n                    **compression_args,\n                )\n    \n            # ZIP Compression\n            elif compression == &amp;quot;zip&amp;quot;:\n                # error: Argument 1 to &amp;quot;_BytesZipFile&amp;quot; has incompatible type\n                # &amp;quot;Union[str, BaseBuffer]&amp;quot;; expected &amp;quot;Union[Union[str, PathLike[str]],\n                # ReadBuffer[bytes], WriteBuffer[bytes]]&amp;quot;\n                handle = _BytesZipFile(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n                if handle.buffer.mode == &amp;quot;r&amp;quot;:\n                    handles.append(handle)\n                    zip_names = handle.buffer.namelist()\n                    if len(zip_names) == 1:\n                        handle = handle.buffer.open(zip_names.pop())\n                    elif not zip_names:\n                        raise ValueError(f&amp;quot;Zero files found in ZIP file {path_or_buf}&amp;quot;)\n                    else:\n                        raise ValueError(\n                            &amp;quot;Multiple files found in ZIP file. &amp;quot;\n                            f&amp;quot;Only one file per ZIP: {zip_names}&amp;quot;\n                        )\n    \n            # TAR Encoding\n            elif compression == &amp;quot;tar&amp;quot;:\n                compression_args.setdefault(&amp;quot;mode&amp;quot;, ioargs.mode)\n                if isinstance(handle, str):\n                    handle = _BytesTarFile(name=handle, **compression_args)\n                else:\n                    # error: Argument &amp;quot;fileobj&amp;quot; to &amp;quot;_BytesTarFile&amp;quot; has incompatible\n                    # type &amp;quot;BaseBuffer&amp;quot;; expected &amp;quot;Union[ReadBuffer[bytes],\n                    # WriteBuffer[bytes], None]&amp;quot;\n                    handle = _BytesTarFile(\n                        fileobj=handle, **compression_args  # type: ignore[arg-type]\n                    )\n                assert isinstance(handle, _BytesTarFile)\n                if &amp;quot;r&amp;quot; in handle.buffer.mode:\n                    handles.append(handle)\n                    files = handle.buffer.getnames()\n                    if len(files) == 1:\n                        file = handle.buffer.extractfile(files[0])\n                        assert file is not None\n                        handle = file\n                    elif not files:\n                        raise ValueError(f&amp;quot;Zero files found in TAR archive {path_or_buf}&amp;quot;)\n                    else:\n                        raise ValueError(\n                            &amp;quot;Multiple files found in TAR archive. &amp;quot;\n                            f&amp;quot;Only one file per TAR archive: {files}&amp;quot;\n                        )\n    \n            # XZ Compression\n            elif compression == &amp;quot;xz&amp;quot;:\n                # error: Argument 1 to &amp;quot;LZMAFile&amp;quot; has incompatible type &amp;quot;Union[str,\n                # BaseBuffer]&amp;quot;; expected &amp;quot;Optional[Union[Union[str, bytes, PathLike[str],\n                # PathLike[bytes]], IO[bytes]], None]&amp;quot;\n                handle = get_lzma_file()(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n    \n            # Zstd Compression\n            elif compression == &amp;quot;zstd&amp;quot;:\n                zstd = import_optional_dependency(&amp;quot;zstandard&amp;quot;)\n                if &amp;quot;r&amp;quot; in ioargs.mode:\n                    open_args = {&amp;quot;dctx&amp;quot;: zstd.ZstdDecompressor(**compression_args)}\n                else:\n                    open_args = {&amp;quot;cctx&amp;quot;: zstd.ZstdCompressor(**compression_args)}\n                handle = zstd.open(\n                    handle,\n                    mode=ioargs.mode,\n                    **open_args,\n                )\n    \n            # Unrecognized Compression\n            else:\n                msg = f&amp;quot;Unrecognized compression type: {compression}&amp;quot;\n                raise ValueError(msg)\n    \n            assert not isinstance(handle, str)\n            handles.append(handle)\n    \n        elif isinstance(handle, str):\n            # Check whether the filename is to be opened in binary mode.\n            # Binary mode does not support &amp;#x27;encoding&amp;#x27; and &amp;#x27;newline&amp;#x27;.\n            if ioargs.encoding and &amp;quot;b&amp;quot; not in ioargs.mode:\n                # Encoding\n&amp;gt;               handle = open(\n                    handle,\n                    ioargs.mode,\n                    encoding=ioargs.encoding,\n                    errors=errors,\n                    newline=&amp;quot;&amp;quot;,\n                )\nE               FileNotFoundError: [Errno 2] No such file or directory: &amp;#x27;test_data/dataset_project_ds_nolabel.csv&amp;#x27;\n\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/common.py:873: FileNotFoundError\n&#34;}], &#34;tests/test_classification_model.py::test_evaluate_model_neg&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_classification_model.py::test_evaluate_model_neg&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_classification_model.py::test_evaluate_model_neg&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_evaluate_model_neg():\n&amp;gt;       raw_data = pd.read_csv(&amp;quot;test_data/dataset_project_ds_nodata.csv&amp;quot;)\n\ntests/test_classification_model.py:63: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026: in read_csv\n    return _read(filepath_or_buffer, kwds)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620: in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620: in __init__\n    self._engine = self._make_engine(f, self.engine)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880: in _make_engine\n    self.handles = get_handle(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\npath_or_buf = &amp;#x27;test_data/dataset_project_ds_nodata.csv&amp;#x27;, mode = &amp;#x27;r&amp;#x27;\n\n    @doc(compression_options=_shared_docs[&amp;quot;compression_options&amp;quot;] % &amp;quot;path_or_buf&amp;quot;)\n    def get_handle(\n        path_or_buf: FilePath | BaseBuffer,\n        mode: str,\n        *,\n        encoding: str | None = None,\n        compression: CompressionOptions | None = None,\n        memory_map: bool = False,\n        is_text: bool = True,\n        errors: str | None = None,\n        storage_options: StorageOptions | None = None,\n    ) -&amp;gt; IOHandles[str] | IOHandles[bytes]:\n        &amp;quot;&amp;quot;&amp;quot;\n        Get file handle for given path/buffer and mode.\n    \n        Parameters\n        ----------\n        path_or_buf : str or file handle\n            File path or object.\n        mode : str\n            Mode to open path_or_buf with.\n        encoding : str or None\n            Encoding to use.\n        {compression_options}\n    \n               May be a dict with key &amp;#x27;method&amp;#x27; as compression mode\n               and other keys as compression options if compression\n               mode is &amp;#x27;zip&amp;#x27;.\n    \n               Passing compression options as keys in dict is\n               supported for compression modes &amp;#x27;gzip&amp;#x27;, &amp;#x27;bz2&amp;#x27;, &amp;#x27;zstd&amp;#x27; and &amp;#x27;zip&amp;#x27;.\n    \n            .. versionchanged:: 1.4.0 Zstandard support.\n    \n        memory_map : bool, default False\n            See parsers._parser_params for more information. Only used by read_csv.\n        is_text : bool, default True\n            Whether the type of the content passed to the file/buffer is string or\n            bytes. This is not the same as `&amp;quot;b&amp;quot; not in mode`. If a string content is\n            passed to a binary file/buffer, a wrapper is inserted.\n        errors : str, default &amp;#x27;strict&amp;#x27;\n            Specifies how encoding and decoding errors are to be handled.\n            See the errors argument for :func:`open` for a full list\n            of options.\n        storage_options: StorageOptions = None\n            Passed to _get_filepath_or_buffer\n    \n        Returns the dataclass IOHandles\n        &amp;quot;&amp;quot;&amp;quot;\n        # Windows does not default to utf-8. Set to utf-8 for a consistent behavior\n        encoding = encoding or &amp;quot;utf-8&amp;quot;\n    \n        errors = errors or &amp;quot;strict&amp;quot;\n    \n        # read_csv does not know whether the buffer is opened in binary/text mode\n        if _is_binary_mode(path_or_buf, mode) and &amp;quot;b&amp;quot; not in mode:\n            mode += &amp;quot;b&amp;quot;\n    \n        # validate encoding and errors\n        codecs.lookup(encoding)\n        if isinstance(errors, str):\n            codecs.lookup_error(errors)\n    \n        # open URLs\n        ioargs = _get_filepath_or_buffer(\n            path_or_buf,\n            encoding=encoding,\n            compression=compression,\n            mode=mode,\n            storage_options=storage_options,\n        )\n    \n        handle = ioargs.filepath_or_buffer\n        handles: list[BaseBuffer]\n    \n        # memory mapping needs to be the first step\n        # only used for read_csv\n        handle, memory_map, handles = _maybe_memory_map(handle, memory_map)\n    \n        is_path = isinstance(handle, str)\n        compression_args = dict(ioargs.compression)\n        compression = compression_args.pop(&amp;quot;method&amp;quot;)\n    \n        # Only for write methods\n        if &amp;quot;r&amp;quot; not in mode and is_path:\n            check_parent_directory(str(handle))\n    \n        if compression:\n            if compression != &amp;quot;zstd&amp;quot;:\n                # compression libraries do not like an explicit text-mode\n                ioargs.mode = ioargs.mode.replace(&amp;quot;t&amp;quot;, &amp;quot;&amp;quot;)\n            elif compression == &amp;quot;zstd&amp;quot; and &amp;quot;b&amp;quot; not in ioargs.mode:\n                # python-zstandard defaults to text mode, but we always expect\n                # compression libraries to use binary mode.\n                ioargs.mode += &amp;quot;b&amp;quot;\n    \n            # GZ Compression\n            if compression == &amp;quot;gzip&amp;quot;:\n                if isinstance(handle, str):\n                    # error: Incompatible types in assignment (expression has type\n                    # &amp;quot;GzipFile&amp;quot;, variable has type &amp;quot;Union[str, BaseBuffer]&amp;quot;)\n                    handle = gzip.GzipFile(  # type: ignore[assignment]\n                        filename=handle,\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n                else:\n                    handle = gzip.GzipFile(\n                        # No overload variant of &amp;quot;GzipFile&amp;quot; matches argument types\n                        # &amp;quot;Union[str, BaseBuffer]&amp;quot;, &amp;quot;str&amp;quot;, &amp;quot;Dict[str, Any]&amp;quot;\n                        fileobj=handle,  # type: ignore[call-overload]\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n    \n            # BZ Compression\n            elif compression == &amp;quot;bz2&amp;quot;:\n                # Overload of &amp;quot;BZ2File&amp;quot; to handle pickle protocol 5\n                # &amp;quot;Union[str, BaseBuffer]&amp;quot;, &amp;quot;str&amp;quot;, &amp;quot;Dict[str, Any]&amp;quot;\n                handle = get_bz2_file()(  # type: ignore[call-overload]\n                    handle,\n                    mode=ioargs.mode,\n                    **compression_args,\n                )\n    \n            # ZIP Compression\n            elif compression == &amp;quot;zip&amp;quot;:\n                # error: Argument 1 to &amp;quot;_BytesZipFile&amp;quot; has incompatible type\n                # &amp;quot;Union[str, BaseBuffer]&amp;quot;; expected &amp;quot;Union[Union[str, PathLike[str]],\n                # ReadBuffer[bytes], WriteBuffer[bytes]]&amp;quot;\n                handle = _BytesZipFile(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n                if handle.buffer.mode == &amp;quot;r&amp;quot;:\n                    handles.append(handle)\n                    zip_names = handle.buffer.namelist()\n                    if len(zip_names) == 1:\n                        handle = handle.buffer.open(zip_names.pop())\n                    elif not zip_names:\n                        raise ValueError(f&amp;quot;Zero files found in ZIP file {path_or_buf}&amp;quot;)\n                    else:\n                        raise ValueError(\n                            &amp;quot;Multiple files found in ZIP file. &amp;quot;\n                            f&amp;quot;Only one file per ZIP: {zip_names}&amp;quot;\n                        )\n    \n            # TAR Encoding\n            elif compression == &amp;quot;tar&amp;quot;:\n                compression_args.setdefault(&amp;quot;mode&amp;quot;, ioargs.mode)\n                if isinstance(handle, str):\n                    handle = _BytesTarFile(name=handle, **compression_args)\n                else:\n                    # error: Argument &amp;quot;fileobj&amp;quot; to &amp;quot;_BytesTarFile&amp;quot; has incompatible\n                    # type &amp;quot;BaseBuffer&amp;quot;; expected &amp;quot;Union[ReadBuffer[bytes],\n                    # WriteBuffer[bytes], None]&amp;quot;\n                    handle = _BytesTarFile(\n                        fileobj=handle, **compression_args  # type: ignore[arg-type]\n                    )\n                assert isinstance(handle, _BytesTarFile)\n                if &amp;quot;r&amp;quot; in handle.buffer.mode:\n                    handles.append(handle)\n                    files = handle.buffer.getnames()\n                    if len(files) == 1:\n                        file = handle.buffer.extractfile(files[0])\n                        assert file is not None\n                        handle = file\n                    elif not files:\n                        raise ValueError(f&amp;quot;Zero files found in TAR archive {path_or_buf}&amp;quot;)\n                    else:\n                        raise ValueError(\n                            &amp;quot;Multiple files found in TAR archive. &amp;quot;\n                            f&amp;quot;Only one file per TAR archive: {files}&amp;quot;\n                        )\n    \n            # XZ Compression\n            elif compression == &amp;quot;xz&amp;quot;:\n                # error: Argument 1 to &amp;quot;LZMAFile&amp;quot; has incompatible type &amp;quot;Union[str,\n                # BaseBuffer]&amp;quot;; expected &amp;quot;Optional[Union[Union[str, bytes, PathLike[str],\n                # PathLike[bytes]], IO[bytes]], None]&amp;quot;\n                handle = get_lzma_file()(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n    \n            # Zstd Compression\n            elif compression == &amp;quot;zstd&amp;quot;:\n                zstd = import_optional_dependency(&amp;quot;zstandard&amp;quot;)\n                if &amp;quot;r&amp;quot; in ioargs.mode:\n                    open_args = {&amp;quot;dctx&amp;quot;: zstd.ZstdDecompressor(**compression_args)}\n                else:\n                    open_args = {&amp;quot;cctx&amp;quot;: zstd.ZstdCompressor(**compression_args)}\n                handle = zstd.open(\n                    handle,\n                    mode=ioargs.mode,\n                    **open_args,\n                )\n    \n            # Unrecognized Compression\n            else:\n                msg = f&amp;quot;Unrecognized compression type: {compression}&amp;quot;\n                raise ValueError(msg)\n    \n            assert not isinstance(handle, str)\n            handles.append(handle)\n    \n        elif isinstance(handle, str):\n            # Check whether the filename is to be opened in binary mode.\n            # Binary mode does not support &amp;#x27;encoding&amp;#x27; and &amp;#x27;newline&amp;#x27;.\n            if ioargs.encoding and &amp;quot;b&amp;quot; not in ioargs.mode:\n                # Encoding\n&amp;gt;               handle = open(\n                    handle,\n                    ioargs.mode,\n                    encoding=ioargs.encoding,\n                    errors=errors,\n                    newline=&amp;quot;&amp;quot;,\n                )\nE               FileNotFoundError: [Errno 2] No such file or directory: &amp;#x27;test_data/dataset_project_ds_nodata.csv&amp;#x27;\n\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/common.py:873: FileNotFoundError\n&#34;}], &#34;tests/test_classification_model.py::test_train_evaluate_decision_tree_pos&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_classification_model.py::test_train_evaluate_decision_tree_pos&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_classification_model.py::test_train_evaluate_decision_tree_pos&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_train_evaluate_decision_tree_pos():\n&amp;gt;       raw_data = pd.read_csv(&amp;quot;test_data/dataset_project_ds_nolabel.csv&amp;quot;)\n\ntests/test_classification_model.py:74: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026: in read_csv\n    return _read(filepath_or_buffer, kwds)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620: in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620: in __init__\n    self._engine = self._make_engine(f, self.engine)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880: in _make_engine\n    self.handles = get_handle(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\npath_or_buf = &amp;#x27;test_data/dataset_project_ds_nolabel.csv&amp;#x27;, mode = &amp;#x27;r&amp;#x27;\n\n    @doc(compression_options=_shared_docs[&amp;quot;compression_options&amp;quot;] % &amp;quot;path_or_buf&amp;quot;)\n    def get_handle(\n        path_or_buf: FilePath | BaseBuffer,\n        mode: str,\n        *,\n        encoding: str | None = None,\n        compression: CompressionOptions | None = None,\n        memory_map: bool = False,\n        is_text: bool = True,\n        errors: str | None = None,\n        storage_options: StorageOptions | None = None,\n    ) -&amp;gt; IOHandles[str] | IOHandles[bytes]:\n        &amp;quot;&amp;quot;&amp;quot;\n        Get file handle for given path/buffer and mode.\n    \n        Parameters\n        ----------\n        path_or_buf : str or file handle\n            File path or object.\n        mode : str\n            Mode to open path_or_buf with.\n        encoding : str or None\n            Encoding to use.\n        {compression_options}\n    \n               May be a dict with key &amp;#x27;method&amp;#x27; as compression mode\n               and other keys as compression options if compression\n               mode is &amp;#x27;zip&amp;#x27;.\n    \n               Passing compression options as keys in dict is\n               supported for compression modes &amp;#x27;gzip&amp;#x27;, &amp;#x27;bz2&amp;#x27;, &amp;#x27;zstd&amp;#x27; and &amp;#x27;zip&amp;#x27;.\n    \n            .. versionchanged:: 1.4.0 Zstandard support.\n    \n        memory_map : bool, default False\n            See parsers._parser_params for more information. Only used by read_csv.\n        is_text : bool, default True\n            Whether the type of the content passed to the file/buffer is string or\n            bytes. This is not the same as `&amp;quot;b&amp;quot; not in mode`. If a string content is\n            passed to a binary file/buffer, a wrapper is inserted.\n        errors : str, default &amp;#x27;strict&amp;#x27;\n            Specifies how encoding and decoding errors are to be handled.\n            See the errors argument for :func:`open` for a full list\n            of options.\n        storage_options: StorageOptions = None\n            Passed to _get_filepath_or_buffer\n    \n        Returns the dataclass IOHandles\n        &amp;quot;&amp;quot;&amp;quot;\n        # Windows does not default to utf-8. Set to utf-8 for a consistent behavior\n        encoding = encoding or &amp;quot;utf-8&amp;quot;\n    \n        errors = errors or &amp;quot;strict&amp;quot;\n    \n        # read_csv does not know whether the buffer is opened in binary/text mode\n        if _is_binary_mode(path_or_buf, mode) and &amp;quot;b&amp;quot; not in mode:\n            mode += &amp;quot;b&amp;quot;\n    \n        # validate encoding and errors\n        codecs.lookup(encoding)\n        if isinstance(errors, str):\n            codecs.lookup_error(errors)\n    \n        # open URLs\n        ioargs = _get_filepath_or_buffer(\n            path_or_buf,\n            encoding=encoding,\n            compression=compression,\n            mode=mode,\n            storage_options=storage_options,\n        )\n    \n        handle = ioargs.filepath_or_buffer\n        handles: list[BaseBuffer]\n    \n        # memory mapping needs to be the first step\n        # only used for read_csv\n        handle, memory_map, handles = _maybe_memory_map(handle, memory_map)\n    \n        is_path = isinstance(handle, str)\n        compression_args = dict(ioargs.compression)\n        compression = compression_args.pop(&amp;quot;method&amp;quot;)\n    \n        # Only for write methods\n        if &amp;quot;r&amp;quot; not in mode and is_path:\n            check_parent_directory(str(handle))\n    \n        if compression:\n            if compression != &amp;quot;zstd&amp;quot;:\n                # compression libraries do not like an explicit text-mode\n                ioargs.mode = ioargs.mode.replace(&amp;quot;t&amp;quot;, &amp;quot;&amp;quot;)\n            elif compression == &amp;quot;zstd&amp;quot; and &amp;quot;b&amp;quot; not in ioargs.mode:\n                # python-zstandard defaults to text mode, but we always expect\n                # compression libraries to use binary mode.\n                ioargs.mode += &amp;quot;b&amp;quot;\n    \n            # GZ Compression\n            if compression == &amp;quot;gzip&amp;quot;:\n                if isinstance(handle, str):\n                    # error: Incompatible types in assignment (expression has type\n                    # &amp;quot;GzipFile&amp;quot;, variable has type &amp;quot;Union[str, BaseBuffer]&amp;quot;)\n                    handle = gzip.GzipFile(  # type: ignore[assignment]\n                        filename=handle,\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n                else:\n                    handle = gzip.GzipFile(\n                        # No overload variant of &amp;quot;GzipFile&amp;quot; matches argument types\n                        # &amp;quot;Union[str, BaseBuffer]&amp;quot;, &amp;quot;str&amp;quot;, &amp;quot;Dict[str, Any]&amp;quot;\n                        fileobj=handle,  # type: ignore[call-overload]\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n    \n            # BZ Compression\n            elif compression == &amp;quot;bz2&amp;quot;:\n                # Overload of &amp;quot;BZ2File&amp;quot; to handle pickle protocol 5\n                # &amp;quot;Union[str, BaseBuffer]&amp;quot;, &amp;quot;str&amp;quot;, &amp;quot;Dict[str, Any]&amp;quot;\n                handle = get_bz2_file()(  # type: ignore[call-overload]\n                    handle,\n                    mode=ioargs.mode,\n                    **compression_args,\n                )\n    \n            # ZIP Compression\n            elif compression == &amp;quot;zip&amp;quot;:\n                # error: Argument 1 to &amp;quot;_BytesZipFile&amp;quot; has incompatible type\n                # &amp;quot;Union[str, BaseBuffer]&amp;quot;; expected &amp;quot;Union[Union[str, PathLike[str]],\n                # ReadBuffer[bytes], WriteBuffer[bytes]]&amp;quot;\n                handle = _BytesZipFile(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n                if handle.buffer.mode == &amp;quot;r&amp;quot;:\n                    handles.append(handle)\n                    zip_names = handle.buffer.namelist()\n                    if len(zip_names) == 1:\n                        handle = handle.buffer.open(zip_names.pop())\n                    elif not zip_names:\n                        raise ValueError(f&amp;quot;Zero files found in ZIP file {path_or_buf}&amp;quot;)\n                    else:\n                        raise ValueError(\n                            &amp;quot;Multiple files found in ZIP file. &amp;quot;\n                            f&amp;quot;Only one file per ZIP: {zip_names}&amp;quot;\n                        )\n    \n            # TAR Encoding\n            elif compression == &amp;quot;tar&amp;quot;:\n                compression_args.setdefault(&amp;quot;mode&amp;quot;, ioargs.mode)\n                if isinstance(handle, str):\n                    handle = _BytesTarFile(name=handle, **compression_args)\n                else:\n                    # error: Argument &amp;quot;fileobj&amp;quot; to &amp;quot;_BytesTarFile&amp;quot; has incompatible\n                    # type &amp;quot;BaseBuffer&amp;quot;; expected &amp;quot;Union[ReadBuffer[bytes],\n                    # WriteBuffer[bytes], None]&amp;quot;\n                    handle = _BytesTarFile(\n                        fileobj=handle, **compression_args  # type: ignore[arg-type]\n                    )\n                assert isinstance(handle, _BytesTarFile)\n                if &amp;quot;r&amp;quot; in handle.buffer.mode:\n                    handles.append(handle)\n                    files = handle.buffer.getnames()\n                    if len(files) == 1:\n                        file = handle.buffer.extractfile(files[0])\n                        assert file is not None\n                        handle = file\n                    elif not files:\n                        raise ValueError(f&amp;quot;Zero files found in TAR archive {path_or_buf}&amp;quot;)\n                    else:\n                        raise ValueError(\n                            &amp;quot;Multiple files found in TAR archive. &amp;quot;\n                            f&amp;quot;Only one file per TAR archive: {files}&amp;quot;\n                        )\n    \n            # XZ Compression\n            elif compression == &amp;quot;xz&amp;quot;:\n                # error: Argument 1 to &amp;quot;LZMAFile&amp;quot; has incompatible type &amp;quot;Union[str,\n                # BaseBuffer]&amp;quot;; expected &amp;quot;Optional[Union[Union[str, bytes, PathLike[str],\n                # PathLike[bytes]], IO[bytes]], None]&amp;quot;\n                handle = get_lzma_file()(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n    \n            # Zstd Compression\n            elif compression == &amp;quot;zstd&amp;quot;:\n                zstd = import_optional_dependency(&amp;quot;zstandard&amp;quot;)\n                if &amp;quot;r&amp;quot; in ioargs.mode:\n                    open_args = {&amp;quot;dctx&amp;quot;: zstd.ZstdDecompressor(**compression_args)}\n                else:\n                    open_args = {&amp;quot;cctx&amp;quot;: zstd.ZstdCompressor(**compression_args)}\n                handle = zstd.open(\n                    handle,\n                    mode=ioargs.mode,\n                    **open_args,\n                )\n    \n            # Unrecognized Compression\n            else:\n                msg = f&amp;quot;Unrecognized compression type: {compression}&amp;quot;\n                raise ValueError(msg)\n    \n            assert not isinstance(handle, str)\n            handles.append(handle)\n    \n        elif isinstance(handle, str):\n            # Check whether the filename is to be opened in binary mode.\n            # Binary mode does not support &amp;#x27;encoding&amp;#x27; and &amp;#x27;newline&amp;#x27;.\n            if ioargs.encoding and &amp;quot;b&amp;quot; not in ioargs.mode:\n                # Encoding\n&amp;gt;               handle = open(\n                    handle,\n                    ioargs.mode,\n                    encoding=ioargs.encoding,\n                    errors=errors,\n                    newline=&amp;quot;&amp;quot;,\n                )\nE               FileNotFoundError: [Errno 2] No such file or directory: &amp;#x27;test_data/dataset_project_ds_nolabel.csv&amp;#x27;\n\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/common.py:873: FileNotFoundError\n&#34;}], &#34;tests/test_classification_model.py::test_train_evaluate_decision_tree_neg&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_classification_model.py::test_train_evaluate_decision_tree_neg&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_classification_model.py::test_train_evaluate_decision_tree_neg&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_train_evaluate_decision_tree_neg():\n&amp;gt;       raw_data = pd.read_csv(&amp;quot;test_data/dataset_project_ds_nodata.csv&amp;quot;)\n\ntests/test_classification_model.py:85: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026: in read_csv\n    return _read(filepath_or_buffer, kwds)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620: in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620: in __init__\n    self._engine = self._make_engine(f, self.engine)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880: in _make_engine\n    self.handles = get_handle(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\npath_or_buf = &amp;#x27;test_data/dataset_project_ds_nodata.csv&amp;#x27;, mode = &amp;#x27;r&amp;#x27;\n\n    @doc(compression_options=_shared_docs[&amp;quot;compression_options&amp;quot;] % &amp;quot;path_or_buf&amp;quot;)\n    def get_handle(\n        path_or_buf: FilePath | BaseBuffer,\n        mode: str,\n        *,\n        encoding: str | None = None,\n        compression: CompressionOptions | None = None,\n        memory_map: bool = False,\n        is_text: bool = True,\n        errors: str | None = None,\n        storage_options: StorageOptions | None = None,\n    ) -&amp;gt; IOHandles[str] | IOHandles[bytes]:\n        &amp;quot;&amp;quot;&amp;quot;\n        Get file handle for given path/buffer and mode.\n    \n        Parameters\n        ----------\n        path_or_buf : str or file handle\n            File path or object.\n        mode : str\n            Mode to open path_or_buf with.\n        encoding : str or None\n            Encoding to use.\n        {compression_options}\n    \n               May be a dict with key &amp;#x27;method&amp;#x27; as compression mode\n               and other keys as compression options if compression\n               mode is &amp;#x27;zip&amp;#x27;.\n    \n               Passing compression options as keys in dict is\n               supported for compression modes &amp;#x27;gzip&amp;#x27;, &amp;#x27;bz2&amp;#x27;, &amp;#x27;zstd&amp;#x27; and &amp;#x27;zip&amp;#x27;.\n    \n            .. versionchanged:: 1.4.0 Zstandard support.\n    \n        memory_map : bool, default False\n            See parsers._parser_params for more information. Only used by read_csv.\n        is_text : bool, default True\n            Whether the type of the content passed to the file/buffer is string or\n            bytes. This is not the same as `&amp;quot;b&amp;quot; not in mode`. If a string content is\n            passed to a binary file/buffer, a wrapper is inserted.\n        errors : str, default &amp;#x27;strict&amp;#x27;\n            Specifies how encoding and decoding errors are to be handled.\n            See the errors argument for :func:`open` for a full list\n            of options.\n        storage_options: StorageOptions = None\n            Passed to _get_filepath_or_buffer\n    \n        Returns the dataclass IOHandles\n        &amp;quot;&amp;quot;&amp;quot;\n        # Windows does not default to utf-8. Set to utf-8 for a consistent behavior\n        encoding = encoding or &amp;quot;utf-8&amp;quot;\n    \n        errors = errors or &amp;quot;strict&amp;quot;\n    \n        # read_csv does not know whether the buffer is opened in binary/text mode\n        if _is_binary_mode(path_or_buf, mode) and &amp;quot;b&amp;quot; not in mode:\n            mode += &amp;quot;b&amp;quot;\n    \n        # validate encoding and errors\n        codecs.lookup(encoding)\n        if isinstance(errors, str):\n            codecs.lookup_error(errors)\n    \n        # open URLs\n        ioargs = _get_filepath_or_buffer(\n            path_or_buf,\n            encoding=encoding,\n            compression=compression,\n            mode=mode,\n            storage_options=storage_options,\n        )\n    \n        handle = ioargs.filepath_or_buffer\n        handles: list[BaseBuffer]\n    \n        # memory mapping needs to be the first step\n        # only used for read_csv\n        handle, memory_map, handles = _maybe_memory_map(handle, memory_map)\n    \n        is_path = isinstance(handle, str)\n        compression_args = dict(ioargs.compression)\n        compression = compression_args.pop(&amp;quot;method&amp;quot;)\n    \n        # Only for write methods\n        if &amp;quot;r&amp;quot; not in mode and is_path:\n            check_parent_directory(str(handle))\n    \n        if compression:\n            if compression != &amp;quot;zstd&amp;quot;:\n                # compression libraries do not like an explicit text-mode\n                ioargs.mode = ioargs.mode.replace(&amp;quot;t&amp;quot;, &amp;quot;&amp;quot;)\n            elif compression == &amp;quot;zstd&amp;quot; and &amp;quot;b&amp;quot; not in ioargs.mode:\n                # python-zstandard defaults to text mode, but we always expect\n                # compression libraries to use binary mode.\n                ioargs.mode += &amp;quot;b&amp;quot;\n    \n            # GZ Compression\n            if compression == &amp;quot;gzip&amp;quot;:\n                if isinstance(handle, str):\n                    # error: Incompatible types in assignment (expression has type\n                    # &amp;quot;GzipFile&amp;quot;, variable has type &amp;quot;Union[str, BaseBuffer]&amp;quot;)\n                    handle = gzip.GzipFile(  # type: ignore[assignment]\n                        filename=handle,\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n                else:\n                    handle = gzip.GzipFile(\n                        # No overload variant of &amp;quot;GzipFile&amp;quot; matches argument types\n                        # &amp;quot;Union[str, BaseBuffer]&amp;quot;, &amp;quot;str&amp;quot;, &amp;quot;Dict[str, Any]&amp;quot;\n                        fileobj=handle,  # type: ignore[call-overload]\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n    \n            # BZ Compression\n            elif compression == &amp;quot;bz2&amp;quot;:\n                # Overload of &amp;quot;BZ2File&amp;quot; to handle pickle protocol 5\n                # &amp;quot;Union[str, BaseBuffer]&amp;quot;, &amp;quot;str&amp;quot;, &amp;quot;Dict[str, Any]&amp;quot;\n                handle = get_bz2_file()(  # type: ignore[call-overload]\n                    handle,\n                    mode=ioargs.mode,\n                    **compression_args,\n                )\n    \n            # ZIP Compression\n            elif compression == &amp;quot;zip&amp;quot;:\n                # error: Argument 1 to &amp;quot;_BytesZipFile&amp;quot; has incompatible type\n                # &amp;quot;Union[str, BaseBuffer]&amp;quot;; expected &amp;quot;Union[Union[str, PathLike[str]],\n                # ReadBuffer[bytes], WriteBuffer[bytes]]&amp;quot;\n                handle = _BytesZipFile(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n                if handle.buffer.mode == &amp;quot;r&amp;quot;:\n                    handles.append(handle)\n                    zip_names = handle.buffer.namelist()\n                    if len(zip_names) == 1:\n                        handle = handle.buffer.open(zip_names.pop())\n                    elif not zip_names:\n                        raise ValueError(f&amp;quot;Zero files found in ZIP file {path_or_buf}&amp;quot;)\n                    else:\n                        raise ValueError(\n                            &amp;quot;Multiple files found in ZIP file. &amp;quot;\n                            f&amp;quot;Only one file per ZIP: {zip_names}&amp;quot;\n                        )\n    \n            # TAR Encoding\n            elif compression == &amp;quot;tar&amp;quot;:\n                compression_args.setdefault(&amp;quot;mode&amp;quot;, ioargs.mode)\n                if isinstance(handle, str):\n                    handle = _BytesTarFile(name=handle, **compression_args)\n                else:\n                    # error: Argument &amp;quot;fileobj&amp;quot; to &amp;quot;_BytesTarFile&amp;quot; has incompatible\n                    # type &amp;quot;BaseBuffer&amp;quot;; expected &amp;quot;Union[ReadBuffer[bytes],\n                    # WriteBuffer[bytes], None]&amp;quot;\n                    handle = _BytesTarFile(\n                        fileobj=handle, **compression_args  # type: ignore[arg-type]\n                    )\n                assert isinstance(handle, _BytesTarFile)\n                if &amp;quot;r&amp;quot; in handle.buffer.mode:\n                    handles.append(handle)\n                    files = handle.buffer.getnames()\n                    if len(files) == 1:\n                        file = handle.buffer.extractfile(files[0])\n                        assert file is not None\n                        handle = file\n                    elif not files:\n                        raise ValueError(f&amp;quot;Zero files found in TAR archive {path_or_buf}&amp;quot;)\n                    else:\n                        raise ValueError(\n                            &amp;quot;Multiple files found in TAR archive. &amp;quot;\n                            f&amp;quot;Only one file per TAR archive: {files}&amp;quot;\n                        )\n    \n            # XZ Compression\n            elif compression == &amp;quot;xz&amp;quot;:\n                # error: Argument 1 to &amp;quot;LZMAFile&amp;quot; has incompatible type &amp;quot;Union[str,\n                # BaseBuffer]&amp;quot;; expected &amp;quot;Optional[Union[Union[str, bytes, PathLike[str],\n                # PathLike[bytes]], IO[bytes]], None]&amp;quot;\n                handle = get_lzma_file()(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n    \n            # Zstd Compression\n            elif compression == &amp;quot;zstd&amp;quot;:\n                zstd = import_optional_dependency(&amp;quot;zstandard&amp;quot;)\n                if &amp;quot;r&amp;quot; in ioargs.mode:\n                    open_args = {&amp;quot;dctx&amp;quot;: zstd.ZstdDecompressor(**compression_args)}\n                else:\n                    open_args = {&amp;quot;cctx&amp;quot;: zstd.ZstdCompressor(**compression_args)}\n                handle = zstd.open(\n                    handle,\n                    mode=ioargs.mode,\n                    **open_args,\n                )\n    \n            # Unrecognized Compression\n            else:\n                msg = f&amp;quot;Unrecognized compression type: {compression}&amp;quot;\n                raise ValueError(msg)\n    \n            assert not isinstance(handle, str)\n            handles.append(handle)\n    \n        elif isinstance(handle, str):\n            # Check whether the filename is to be opened in binary mode.\n            # Binary mode does not support &amp;#x27;encoding&amp;#x27; and &amp;#x27;newline&amp;#x27;.\n            if ioargs.encoding and &amp;quot;b&amp;quot; not in ioargs.mode:\n                # Encoding\n&amp;gt;               handle = open(\n                    handle,\n                    ioargs.mode,\n                    encoding=ioargs.encoding,\n                    errors=errors,\n                    newline=&amp;quot;&amp;quot;,\n                )\nE               FileNotFoundError: [Errno 2] No such file or directory: &amp;#x27;test_data/dataset_project_ds_nodata.csv&amp;#x27;\n\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/common.py:873: FileNotFoundError\n&#34;}], &#34;tests/test_classification_model.py::test_train_evaluate_random_forest_pos&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_classification_model.py::test_train_evaluate_random_forest_pos&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_classification_model.py::test_train_evaluate_random_forest_pos&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_train_evaluate_random_forest_pos():\n&amp;gt;       raw_data = pd.read_csv(&amp;quot;test_data/dataset_project_ds_nolabel.csv&amp;quot;)\n\ntests/test_classification_model.py:96: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026: in read_csv\n    return _read(filepath_or_buffer, kwds)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620: in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620: in __init__\n    self._engine = self._make_engine(f, self.engine)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880: in _make_engine\n    self.handles = get_handle(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\npath_or_buf = &amp;#x27;test_data/dataset_project_ds_nolabel.csv&amp;#x27;, mode = &amp;#x27;r&amp;#x27;\n\n    @doc(compression_options=_shared_docs[&amp;quot;compression_options&amp;quot;] % &amp;quot;path_or_buf&amp;quot;)\n    def get_handle(\n        path_or_buf: FilePath | BaseBuffer,\n        mode: str,\n        *,\n        encoding: str | None = None,\n        compression: CompressionOptions | None = None,\n        memory_map: bool = False,\n        is_text: bool = True,\n        errors: str | None = None,\n        storage_options: StorageOptions | None = None,\n    ) -&amp;gt; IOHandles[str] | IOHandles[bytes]:\n        &amp;quot;&amp;quot;&amp;quot;\n        Get file handle for given path/buffer and mode.\n    \n        Parameters\n        ----------\n        path_or_buf : str or file handle\n            File path or object.\n        mode : str\n            Mode to open path_or_buf with.\n        encoding : str or None\n            Encoding to use.\n        {compression_options}\n    \n               May be a dict with key &amp;#x27;method&amp;#x27; as compression mode\n               and other keys as compression options if compression\n               mode is &amp;#x27;zip&amp;#x27;.\n    \n               Passing compression options as keys in dict is\n               supported for compression modes &amp;#x27;gzip&amp;#x27;, &amp;#x27;bz2&amp;#x27;, &amp;#x27;zstd&amp;#x27; and &amp;#x27;zip&amp;#x27;.\n    \n            .. versionchanged:: 1.4.0 Zstandard support.\n    \n        memory_map : bool, default False\n            See parsers._parser_params for more information. Only used by read_csv.\n        is_text : bool, default True\n            Whether the type of the content passed to the file/buffer is string or\n            bytes. This is not the same as `&amp;quot;b&amp;quot; not in mode`. If a string content is\n            passed to a binary file/buffer, a wrapper is inserted.\n        errors : str, default &amp;#x27;strict&amp;#x27;\n            Specifies how encoding and decoding errors are to be handled.\n            See the errors argument for :func:`open` for a full list\n            of options.\n        storage_options: StorageOptions = None\n            Passed to _get_filepath_or_buffer\n    \n        Returns the dataclass IOHandles\n        &amp;quot;&amp;quot;&amp;quot;\n        # Windows does not default to utf-8. Set to utf-8 for a consistent behavior\n        encoding = encoding or &amp;quot;utf-8&amp;quot;\n    \n        errors = errors or &amp;quot;strict&amp;quot;\n    \n        # read_csv does not know whether the buffer is opened in binary/text mode\n        if _is_binary_mode(path_or_buf, mode) and &amp;quot;b&amp;quot; not in mode:\n            mode += &amp;quot;b&amp;quot;\n    \n        # validate encoding and errors\n        codecs.lookup(encoding)\n        if isinstance(errors, str):\n            codecs.lookup_error(errors)\n    \n        # open URLs\n        ioargs = _get_filepath_or_buffer(\n            path_or_buf,\n            encoding=encoding,\n            compression=compression,\n            mode=mode,\n            storage_options=storage_options,\n        )\n    \n        handle = ioargs.filepath_or_buffer\n        handles: list[BaseBuffer]\n    \n        # memory mapping needs to be the first step\n        # only used for read_csv\n        handle, memory_map, handles = _maybe_memory_map(handle, memory_map)\n    \n        is_path = isinstance(handle, str)\n        compression_args = dict(ioargs.compression)\n        compression = compression_args.pop(&amp;quot;method&amp;quot;)\n    \n        # Only for write methods\n        if &amp;quot;r&amp;quot; not in mode and is_path:\n            check_parent_directory(str(handle))\n    \n        if compression:\n            if compression != &amp;quot;zstd&amp;quot;:\n                # compression libraries do not like an explicit text-mode\n                ioargs.mode = ioargs.mode.replace(&amp;quot;t&amp;quot;, &amp;quot;&amp;quot;)\n            elif compression == &amp;quot;zstd&amp;quot; and &amp;quot;b&amp;quot; not in ioargs.mode:\n                # python-zstandard defaults to text mode, but we always expect\n                # compression libraries to use binary mode.\n                ioargs.mode += &amp;quot;b&amp;quot;\n    \n            # GZ Compression\n            if compression == &amp;quot;gzip&amp;quot;:\n                if isinstance(handle, str):\n                    # error: Incompatible types in assignment (expression has type\n                    # &amp;quot;GzipFile&amp;quot;, variable has type &amp;quot;Union[str, BaseBuffer]&amp;quot;)\n                    handle = gzip.GzipFile(  # type: ignore[assignment]\n                        filename=handle,\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n                else:\n                    handle = gzip.GzipFile(\n                        # No overload variant of &amp;quot;GzipFile&amp;quot; matches argument types\n                        # &amp;quot;Union[str, BaseBuffer]&amp;quot;, &amp;quot;str&amp;quot;, &amp;quot;Dict[str, Any]&amp;quot;\n                        fileobj=handle,  # type: ignore[call-overload]\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n    \n            # BZ Compression\n            elif compression == &amp;quot;bz2&amp;quot;:\n                # Overload of &amp;quot;BZ2File&amp;quot; to handle pickle protocol 5\n                # &amp;quot;Union[str, BaseBuffer]&amp;quot;, &amp;quot;str&amp;quot;, &amp;quot;Dict[str, Any]&amp;quot;\n                handle = get_bz2_file()(  # type: ignore[call-overload]\n                    handle,\n                    mode=ioargs.mode,\n                    **compression_args,\n                )\n    \n            # ZIP Compression\n            elif compression == &amp;quot;zip&amp;quot;:\n                # error: Argument 1 to &amp;quot;_BytesZipFile&amp;quot; has incompatible type\n                # &amp;quot;Union[str, BaseBuffer]&amp;quot;; expected &amp;quot;Union[Union[str, PathLike[str]],\n                # ReadBuffer[bytes], WriteBuffer[bytes]]&amp;quot;\n                handle = _BytesZipFile(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n                if handle.buffer.mode == &amp;quot;r&amp;quot;:\n                    handles.append(handle)\n                    zip_names = handle.buffer.namelist()\n                    if len(zip_names) == 1:\n                        handle = handle.buffer.open(zip_names.pop())\n                    elif not zip_names:\n                        raise ValueError(f&amp;quot;Zero files found in ZIP file {path_or_buf}&amp;quot;)\n                    else:\n                        raise ValueError(\n                            &amp;quot;Multiple files found in ZIP file. &amp;quot;\n                            f&amp;quot;Only one file per ZIP: {zip_names}&amp;quot;\n                        )\n    \n            # TAR Encoding\n            elif compression == &amp;quot;tar&amp;quot;:\n                compression_args.setdefault(&amp;quot;mode&amp;quot;, ioargs.mode)\n                if isinstance(handle, str):\n                    handle = _BytesTarFile(name=handle, **compression_args)\n                else:\n                    # error: Argument &amp;quot;fileobj&amp;quot; to &amp;quot;_BytesTarFile&amp;quot; has incompatible\n                    # type &amp;quot;BaseBuffer&amp;quot;; expected &amp;quot;Union[ReadBuffer[bytes],\n                    # WriteBuffer[bytes], None]&amp;quot;\n                    handle = _BytesTarFile(\n                        fileobj=handle, **compression_args  # type: ignore[arg-type]\n                    )\n                assert isinstance(handle, _BytesTarFile)\n                if &amp;quot;r&amp;quot; in handle.buffer.mode:\n                    handles.append(handle)\n                    files = handle.buffer.getnames()\n                    if len(files) == 1:\n                        file = handle.buffer.extractfile(files[0])\n                        assert file is not None\n                        handle = file\n                    elif not files:\n                        raise ValueError(f&amp;quot;Zero files found in TAR archive {path_or_buf}&amp;quot;)\n                    else:\n                        raise ValueError(\n                            &amp;quot;Multiple files found in TAR archive. &amp;quot;\n                            f&amp;quot;Only one file per TAR archive: {files}&amp;quot;\n                        )\n    \n            # XZ Compression\n            elif compression == &amp;quot;xz&amp;quot;:\n                # error: Argument 1 to &amp;quot;LZMAFile&amp;quot; has incompatible type &amp;quot;Union[str,\n                # BaseBuffer]&amp;quot;; expected &amp;quot;Optional[Union[Union[str, bytes, PathLike[str],\n                # PathLike[bytes]], IO[bytes]], None]&amp;quot;\n                handle = get_lzma_file()(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n    \n            # Zstd Compression\n            elif compression == &amp;quot;zstd&amp;quot;:\n                zstd = import_optional_dependency(&amp;quot;zstandard&amp;quot;)\n                if &amp;quot;r&amp;quot; in ioargs.mode:\n                    open_args = {&amp;quot;dctx&amp;quot;: zstd.ZstdDecompressor(**compression_args)}\n                else:\n                    open_args = {&amp;quot;cctx&amp;quot;: zstd.ZstdCompressor(**compression_args)}\n                handle = zstd.open(\n                    handle,\n                    mode=ioargs.mode,\n                    **open_args,\n                )\n    \n            # Unrecognized Compression\n            else:\n                msg = f&amp;quot;Unrecognized compression type: {compression}&amp;quot;\n                raise ValueError(msg)\n    \n            assert not isinstance(handle, str)\n            handles.append(handle)\n    \n        elif isinstance(handle, str):\n            # Check whether the filename is to be opened in binary mode.\n            # Binary mode does not support &amp;#x27;encoding&amp;#x27; and &amp;#x27;newline&amp;#x27;.\n            if ioargs.encoding and &amp;quot;b&amp;quot; not in ioargs.mode:\n                # Encoding\n&amp;gt;               handle = open(\n                    handle,\n                    ioargs.mode,\n                    encoding=ioargs.encoding,\n                    errors=errors,\n                    newline=&amp;quot;&amp;quot;,\n                )\nE               FileNotFoundError: [Errno 2] No such file or directory: &amp;#x27;test_data/dataset_project_ds_nolabel.csv&amp;#x27;\n\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/common.py:873: FileNotFoundError\n&#34;}], &#34;tests/test_classification_model.py::test_train_evaluate_random_forest_neg&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_classification_model.py::test_train_evaluate_random_forest_neg&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_classification_model.py::test_train_evaluate_random_forest_neg&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_train_evaluate_random_forest_neg():\n&amp;gt;       raw_data = pd.read_csv(&amp;quot;test_data/dataset_project_ds_nodata.csv&amp;quot;)\n\ntests/test_classification_model.py:107: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026: in read_csv\n    return _read(filepath_or_buffer, kwds)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620: in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620: in __init__\n    self._engine = self._make_engine(f, self.engine)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880: in _make_engine\n    self.handles = get_handle(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\npath_or_buf = &amp;#x27;test_data/dataset_project_ds_nodata.csv&amp;#x27;, mode = &amp;#x27;r&amp;#x27;\n\n    @doc(compression_options=_shared_docs[&amp;quot;compression_options&amp;quot;] % &amp;quot;path_or_buf&amp;quot;)\n    def get_handle(\n        path_or_buf: FilePath | BaseBuffer,\n        mode: str,\n        *,\n        encoding: str | None = None,\n        compression: CompressionOptions | None = None,\n        memory_map: bool = False,\n        is_text: bool = True,\n        errors: str | None = None,\n        storage_options: StorageOptions | None = None,\n    ) -&amp;gt; IOHandles[str] | IOHandles[bytes]:\n        &amp;quot;&amp;quot;&amp;quot;\n        Get file handle for given path/buffer and mode.\n    \n        Parameters\n        ----------\n        path_or_buf : str or file handle\n            File path or object.\n        mode : str\n            Mode to open path_or_buf with.\n        encoding : str or None\n            Encoding to use.\n        {compression_options}\n    \n               May be a dict with key &amp;#x27;method&amp;#x27; as compression mode\n               and other keys as compression options if compression\n               mode is &amp;#x27;zip&amp;#x27;.\n    \n               Passing compression options as keys in dict is\n               supported for compression modes &amp;#x27;gzip&amp;#x27;, &amp;#x27;bz2&amp;#x27;, &amp;#x27;zstd&amp;#x27; and &amp;#x27;zip&amp;#x27;.\n    \n            .. versionchanged:: 1.4.0 Zstandard support.\n    \n        memory_map : bool, default False\n            See parsers._parser_params for more information. Only used by read_csv.\n        is_text : bool, default True\n            Whether the type of the content passed to the file/buffer is string or\n            bytes. This is not the same as `&amp;quot;b&amp;quot; not in mode`. If a string content is\n            passed to a binary file/buffer, a wrapper is inserted.\n        errors : str, default &amp;#x27;strict&amp;#x27;\n            Specifies how encoding and decoding errors are to be handled.\n            See the errors argument for :func:`open` for a full list\n            of options.\n        storage_options: StorageOptions = None\n            Passed to _get_filepath_or_buffer\n    \n        Returns the dataclass IOHandles\n        &amp;quot;&amp;quot;&amp;quot;\n        # Windows does not default to utf-8. Set to utf-8 for a consistent behavior\n        encoding = encoding or &amp;quot;utf-8&amp;quot;\n    \n        errors = errors or &amp;quot;strict&amp;quot;\n    \n        # read_csv does not know whether the buffer is opened in binary/text mode\n        if _is_binary_mode(path_or_buf, mode) and &amp;quot;b&amp;quot; not in mode:\n            mode += &amp;quot;b&amp;quot;\n    \n        # validate encoding and errors\n        codecs.lookup(encoding)\n        if isinstance(errors, str):\n            codecs.lookup_error(errors)\n    \n        # open URLs\n        ioargs = _get_filepath_or_buffer(\n            path_or_buf,\n            encoding=encoding,\n            compression=compression,\n            mode=mode,\n            storage_options=storage_options,\n        )\n    \n        handle = ioargs.filepath_or_buffer\n        handles: list[BaseBuffer]\n    \n        # memory mapping needs to be the first step\n        # only used for read_csv\n        handle, memory_map, handles = _maybe_memory_map(handle, memory_map)\n    \n        is_path = isinstance(handle, str)\n        compression_args = dict(ioargs.compression)\n        compression = compression_args.pop(&amp;quot;method&amp;quot;)\n    \n        # Only for write methods\n        if &amp;quot;r&amp;quot; not in mode and is_path:\n            check_parent_directory(str(handle))\n    \n        if compression:\n            if compression != &amp;quot;zstd&amp;quot;:\n                # compression libraries do not like an explicit text-mode\n                ioargs.mode = ioargs.mode.replace(&amp;quot;t&amp;quot;, &amp;quot;&amp;quot;)\n            elif compression == &amp;quot;zstd&amp;quot; and &amp;quot;b&amp;quot; not in ioargs.mode:\n                # python-zstandard defaults to text mode, but we always expect\n                # compression libraries to use binary mode.\n                ioargs.mode += &amp;quot;b&amp;quot;\n    \n            # GZ Compression\n            if compression == &amp;quot;gzip&amp;quot;:\n                if isinstance(handle, str):\n                    # error: Incompatible types in assignment (expression has type\n                    # &amp;quot;GzipFile&amp;quot;, variable has type &amp;quot;Union[str, BaseBuffer]&amp;quot;)\n                    handle = gzip.GzipFile(  # type: ignore[assignment]\n                        filename=handle,\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n                else:\n                    handle = gzip.GzipFile(\n                        # No overload variant of &amp;quot;GzipFile&amp;quot; matches argument types\n                        # &amp;quot;Union[str, BaseBuffer]&amp;quot;, &amp;quot;str&amp;quot;, &amp;quot;Dict[str, Any]&amp;quot;\n                        fileobj=handle,  # type: ignore[call-overload]\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n    \n            # BZ Compression\n            elif compression == &amp;quot;bz2&amp;quot;:\n                # Overload of &amp;quot;BZ2File&amp;quot; to handle pickle protocol 5\n                # &amp;quot;Union[str, BaseBuffer]&amp;quot;, &amp;quot;str&amp;quot;, &amp;quot;Dict[str, Any]&amp;quot;\n                handle = get_bz2_file()(  # type: ignore[call-overload]\n                    handle,\n                    mode=ioargs.mode,\n                    **compression_args,\n                )\n    \n            # ZIP Compression\n            elif compression == &amp;quot;zip&amp;quot;:\n                # error: Argument 1 to &amp;quot;_BytesZipFile&amp;quot; has incompatible type\n                # &amp;quot;Union[str, BaseBuffer]&amp;quot;; expected &amp;quot;Union[Union[str, PathLike[str]],\n                # ReadBuffer[bytes], WriteBuffer[bytes]]&amp;quot;\n                handle = _BytesZipFile(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n                if handle.buffer.mode == &amp;quot;r&amp;quot;:\n                    handles.append(handle)\n                    zip_names = handle.buffer.namelist()\n                    if len(zip_names) == 1:\n                        handle = handle.buffer.open(zip_names.pop())\n                    elif not zip_names:\n                        raise ValueError(f&amp;quot;Zero files found in ZIP file {path_or_buf}&amp;quot;)\n                    else:\n                        raise ValueError(\n                            &amp;quot;Multiple files found in ZIP file. &amp;quot;\n                            f&amp;quot;Only one file per ZIP: {zip_names}&amp;quot;\n                        )\n    \n            # TAR Encoding\n            elif compression == &amp;quot;tar&amp;quot;:\n                compression_args.setdefault(&amp;quot;mode&amp;quot;, ioargs.mode)\n                if isinstance(handle, str):\n                    handle = _BytesTarFile(name=handle, **compression_args)\n                else:\n                    # error: Argument &amp;quot;fileobj&amp;quot; to &amp;quot;_BytesTarFile&amp;quot; has incompatible\n                    # type &amp;quot;BaseBuffer&amp;quot;; expected &amp;quot;Union[ReadBuffer[bytes],\n                    # WriteBuffer[bytes], None]&amp;quot;\n                    handle = _BytesTarFile(\n                        fileobj=handle, **compression_args  # type: ignore[arg-type]\n                    )\n                assert isinstance(handle, _BytesTarFile)\n                if &amp;quot;r&amp;quot; in handle.buffer.mode:\n                    handles.append(handle)\n                    files = handle.buffer.getnames()\n                    if len(files) == 1:\n                        file = handle.buffer.extractfile(files[0])\n                        assert file is not None\n                        handle = file\n                    elif not files:\n                        raise ValueError(f&amp;quot;Zero files found in TAR archive {path_or_buf}&amp;quot;)\n                    else:\n                        raise ValueError(\n                            &amp;quot;Multiple files found in TAR archive. &amp;quot;\n                            f&amp;quot;Only one file per TAR archive: {files}&amp;quot;\n                        )\n    \n            # XZ Compression\n            elif compression == &amp;quot;xz&amp;quot;:\n                # error: Argument 1 to &amp;quot;LZMAFile&amp;quot; has incompatible type &amp;quot;Union[str,\n                # BaseBuffer]&amp;quot;; expected &amp;quot;Optional[Union[Union[str, bytes, PathLike[str],\n                # PathLike[bytes]], IO[bytes]], None]&amp;quot;\n                handle = get_lzma_file()(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n    \n            # Zstd Compression\n            elif compression == &amp;quot;zstd&amp;quot;:\n                zstd = import_optional_dependency(&amp;quot;zstandard&amp;quot;)\n                if &amp;quot;r&amp;quot; in ioargs.mode:\n                    open_args = {&amp;quot;dctx&amp;quot;: zstd.ZstdDecompressor(**compression_args)}\n                else:\n                    open_args = {&amp;quot;cctx&amp;quot;: zstd.ZstdCompressor(**compression_args)}\n                handle = zstd.open(\n                    handle,\n                    mode=ioargs.mode,\n                    **open_args,\n                )\n    \n            # Unrecognized Compression\n            else:\n                msg = f&amp;quot;Unrecognized compression type: {compression}&amp;quot;\n                raise ValueError(msg)\n    \n            assert not isinstance(handle, str)\n            handles.append(handle)\n    \n        elif isinstance(handle, str):\n            # Check whether the filename is to be opened in binary mode.\n            # Binary mode does not support &amp;#x27;encoding&amp;#x27; and &amp;#x27;newline&amp;#x27;.\n            if ioargs.encoding and &amp;quot;b&amp;quot; not in ioargs.mode:\n                # Encoding\n&amp;gt;               handle = open(\n                    handle,\n                    ioargs.mode,\n                    encoding=ioargs.encoding,\n                    errors=errors,\n                    newline=&amp;quot;&amp;quot;,\n                )\nE               FileNotFoundError: [Errno 2] No such file or directory: &amp;#x27;test_data/dataset_project_ds_nodata.csv&amp;#x27;\n\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/common.py:873: FileNotFoundError\n&#34;}], &#34;tests/test_classification_model.py::test_train_evaluate_svm_pos&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_classification_model.py::test_train_evaluate_svm_pos&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_classification_model.py::test_train_evaluate_svm_pos&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_train_evaluate_svm_pos():\n&amp;gt;       raw_data = pd.read_csv(&amp;quot;test_data/dataset_project_ds_nolabel.csv&amp;quot;)\n\ntests/test_classification_model.py:118: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026: in read_csv\n    return _read(filepath_or_buffer, kwds)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620: in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620: in __init__\n    self._engine = self._make_engine(f, self.engine)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880: in _make_engine\n    self.handles = get_handle(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\npath_or_buf = &amp;#x27;test_data/dataset_project_ds_nolabel.csv&amp;#x27;, mode = &amp;#x27;r&amp;#x27;\n\n    @doc(compression_options=_shared_docs[&amp;quot;compression_options&amp;quot;] % &amp;quot;path_or_buf&amp;quot;)\n    def get_handle(\n        path_or_buf: FilePath | BaseBuffer,\n        mode: str,\n        *,\n        encoding: str | None = None,\n        compression: CompressionOptions | None = None,\n        memory_map: bool = False,\n        is_text: bool = True,\n        errors: str | None = None,\n        storage_options: StorageOptions | None = None,\n    ) -&amp;gt; IOHandles[str] | IOHandles[bytes]:\n        &amp;quot;&amp;quot;&amp;quot;\n        Get file handle for given path/buffer and mode.\n    \n        Parameters\n        ----------\n        path_or_buf : str or file handle\n            File path or object.\n        mode : str\n            Mode to open path_or_buf with.\n        encoding : str or None\n            Encoding to use.\n        {compression_options}\n    \n               May be a dict with key &amp;#x27;method&amp;#x27; as compression mode\n               and other keys as compression options if compression\n               mode is &amp;#x27;zip&amp;#x27;.\n    \n               Passing compression options as keys in dict is\n               supported for compression modes &amp;#x27;gzip&amp;#x27;, &amp;#x27;bz2&amp;#x27;, &amp;#x27;zstd&amp;#x27; and &amp;#x27;zip&amp;#x27;.\n    \n            .. versionchanged:: 1.4.0 Zstandard support.\n    \n        memory_map : bool, default False\n            See parsers._parser_params for more information. Only used by read_csv.\n        is_text : bool, default True\n            Whether the type of the content passed to the file/buffer is string or\n            bytes. This is not the same as `&amp;quot;b&amp;quot; not in mode`. If a string content is\n            passed to a binary file/buffer, a wrapper is inserted.\n        errors : str, default &amp;#x27;strict&amp;#x27;\n            Specifies how encoding and decoding errors are to be handled.\n            See the errors argument for :func:`open` for a full list\n            of options.\n        storage_options: StorageOptions = None\n            Passed to _get_filepath_or_buffer\n    \n        Returns the dataclass IOHandles\n        &amp;quot;&amp;quot;&amp;quot;\n        # Windows does not default to utf-8. Set to utf-8 for a consistent behavior\n        encoding = encoding or &amp;quot;utf-8&amp;quot;\n    \n        errors = errors or &amp;quot;strict&amp;quot;\n    \n        # read_csv does not know whether the buffer is opened in binary/text mode\n        if _is_binary_mode(path_or_buf, mode) and &amp;quot;b&amp;quot; not in mode:\n            mode += &amp;quot;b&amp;quot;\n    \n        # validate encoding and errors\n        codecs.lookup(encoding)\n        if isinstance(errors, str):\n            codecs.lookup_error(errors)\n    \n        # open URLs\n        ioargs = _get_filepath_or_buffer(\n            path_or_buf,\n            encoding=encoding,\n            compression=compression,\n            mode=mode,\n            storage_options=storage_options,\n        )\n    \n        handle = ioargs.filepath_or_buffer\n        handles: list[BaseBuffer]\n    \n        # memory mapping needs to be the first step\n        # only used for read_csv\n        handle, memory_map, handles = _maybe_memory_map(handle, memory_map)\n    \n        is_path = isinstance(handle, str)\n        compression_args = dict(ioargs.compression)\n        compression = compression_args.pop(&amp;quot;method&amp;quot;)\n    \n        # Only for write methods\n        if &amp;quot;r&amp;quot; not in mode and is_path:\n            check_parent_directory(str(handle))\n    \n        if compression:\n            if compression != &amp;quot;zstd&amp;quot;:\n                # compression libraries do not like an explicit text-mode\n                ioargs.mode = ioargs.mode.replace(&amp;quot;t&amp;quot;, &amp;quot;&amp;quot;)\n            elif compression == &amp;quot;zstd&amp;quot; and &amp;quot;b&amp;quot; not in ioargs.mode:\n                # python-zstandard defaults to text mode, but we always expect\n                # compression libraries to use binary mode.\n                ioargs.mode += &amp;quot;b&amp;quot;\n    \n            # GZ Compression\n            if compression == &amp;quot;gzip&amp;quot;:\n                if isinstance(handle, str):\n                    # error: Incompatible types in assignment (expression has type\n                    # &amp;quot;GzipFile&amp;quot;, variable has type &amp;quot;Union[str, BaseBuffer]&amp;quot;)\n                    handle = gzip.GzipFile(  # type: ignore[assignment]\n                        filename=handle,\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n                else:\n                    handle = gzip.GzipFile(\n                        # No overload variant of &amp;quot;GzipFile&amp;quot; matches argument types\n                        # &amp;quot;Union[str, BaseBuffer]&amp;quot;, &amp;quot;str&amp;quot;, &amp;quot;Dict[str, Any]&amp;quot;\n                        fileobj=handle,  # type: ignore[call-overload]\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n    \n            # BZ Compression\n            elif compression == &amp;quot;bz2&amp;quot;:\n                # Overload of &amp;quot;BZ2File&amp;quot; to handle pickle protocol 5\n                # &amp;quot;Union[str, BaseBuffer]&amp;quot;, &amp;quot;str&amp;quot;, &amp;quot;Dict[str, Any]&amp;quot;\n                handle = get_bz2_file()(  # type: ignore[call-overload]\n                    handle,\n                    mode=ioargs.mode,\n                    **compression_args,\n                )\n    \n            # ZIP Compression\n            elif compression == &amp;quot;zip&amp;quot;:\n                # error: Argument 1 to &amp;quot;_BytesZipFile&amp;quot; has incompatible type\n                # &amp;quot;Union[str, BaseBuffer]&amp;quot;; expected &amp;quot;Union[Union[str, PathLike[str]],\n                # ReadBuffer[bytes], WriteBuffer[bytes]]&amp;quot;\n                handle = _BytesZipFile(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n                if handle.buffer.mode == &amp;quot;r&amp;quot;:\n                    handles.append(handle)\n                    zip_names = handle.buffer.namelist()\n                    if len(zip_names) == 1:\n                        handle = handle.buffer.open(zip_names.pop())\n                    elif not zip_names:\n                        raise ValueError(f&amp;quot;Zero files found in ZIP file {path_or_buf}&amp;quot;)\n                    else:\n                        raise ValueError(\n                            &amp;quot;Multiple files found in ZIP file. &amp;quot;\n                            f&amp;quot;Only one file per ZIP: {zip_names}&amp;quot;\n                        )\n    \n            # TAR Encoding\n            elif compression == &amp;quot;tar&amp;quot;:\n                compression_args.setdefault(&amp;quot;mode&amp;quot;, ioargs.mode)\n                if isinstance(handle, str):\n                    handle = _BytesTarFile(name=handle, **compression_args)\n                else:\n                    # error: Argument &amp;quot;fileobj&amp;quot; to &amp;quot;_BytesTarFile&amp;quot; has incompatible\n                    # type &amp;quot;BaseBuffer&amp;quot;; expected &amp;quot;Union[ReadBuffer[bytes],\n                    # WriteBuffer[bytes], None]&amp;quot;\n                    handle = _BytesTarFile(\n                        fileobj=handle, **compression_args  # type: ignore[arg-type]\n                    )\n                assert isinstance(handle, _BytesTarFile)\n                if &amp;quot;r&amp;quot; in handle.buffer.mode:\n                    handles.append(handle)\n                    files = handle.buffer.getnames()\n                    if len(files) == 1:\n                        file = handle.buffer.extractfile(files[0])\n                        assert file is not None\n                        handle = file\n                    elif not files:\n                        raise ValueError(f&amp;quot;Zero files found in TAR archive {path_or_buf}&amp;quot;)\n                    else:\n                        raise ValueError(\n                            &amp;quot;Multiple files found in TAR archive. &amp;quot;\n                            f&amp;quot;Only one file per TAR archive: {files}&amp;quot;\n                        )\n    \n            # XZ Compression\n            elif compression == &amp;quot;xz&amp;quot;:\n                # error: Argument 1 to &amp;quot;LZMAFile&amp;quot; has incompatible type &amp;quot;Union[str,\n                # BaseBuffer]&amp;quot;; expected &amp;quot;Optional[Union[Union[str, bytes, PathLike[str],\n                # PathLike[bytes]], IO[bytes]], None]&amp;quot;\n                handle = get_lzma_file()(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n    \n            # Zstd Compression\n            elif compression == &amp;quot;zstd&amp;quot;:\n                zstd = import_optional_dependency(&amp;quot;zstandard&amp;quot;)\n                if &amp;quot;r&amp;quot; in ioargs.mode:\n                    open_args = {&amp;quot;dctx&amp;quot;: zstd.ZstdDecompressor(**compression_args)}\n                else:\n                    open_args = {&amp;quot;cctx&amp;quot;: zstd.ZstdCompressor(**compression_args)}\n                handle = zstd.open(\n                    handle,\n                    mode=ioargs.mode,\n                    **open_args,\n                )\n    \n            # Unrecognized Compression\n            else:\n                msg = f&amp;quot;Unrecognized compression type: {compression}&amp;quot;\n                raise ValueError(msg)\n    \n            assert not isinstance(handle, str)\n            handles.append(handle)\n    \n        elif isinstance(handle, str):\n            # Check whether the filename is to be opened in binary mode.\n            # Binary mode does not support &amp;#x27;encoding&amp;#x27; and &amp;#x27;newline&amp;#x27;.\n            if ioargs.encoding and &amp;quot;b&amp;quot; not in ioargs.mode:\n                # Encoding\n&amp;gt;               handle = open(\n                    handle,\n                    ioargs.mode,\n                    encoding=ioargs.encoding,\n                    errors=errors,\n                    newline=&amp;quot;&amp;quot;,\n                )\nE               FileNotFoundError: [Errno 2] No such file or directory: &amp;#x27;test_data/dataset_project_ds_nolabel.csv&amp;#x27;\n\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/common.py:873: FileNotFoundError\n&#34;}], &#34;tests/test_classification_model.py::test_train_evaluate_svm_neg&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_classification_model.py::test_train_evaluate_svm_neg&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_classification_model.py::test_train_evaluate_svm_neg&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_train_evaluate_svm_neg():\n&amp;gt;       raw_data = pd.read_csv(&amp;quot;test_data/dataset_project_ds_nodata.csv&amp;quot;)\n\ntests/test_classification_model.py:130: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026: in read_csv\n    return _read(filepath_or_buffer, kwds)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620: in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620: in __init__\n    self._engine = self._make_engine(f, self.engine)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880: in _make_engine\n    self.handles = get_handle(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\npath_or_buf = &amp;#x27;test_data/dataset_project_ds_nodata.csv&amp;#x27;, mode = &amp;#x27;r&amp;#x27;\n\n    @doc(compression_options=_shared_docs[&amp;quot;compression_options&amp;quot;] % &amp;quot;path_or_buf&amp;quot;)\n    def get_handle(\n        path_or_buf: FilePath | BaseBuffer,\n        mode: str,\n        *,\n        encoding: str | None = None,\n        compression: CompressionOptions | None = None,\n        memory_map: bool = False,\n        is_text: bool = True,\n        errors: str | None = None,\n        storage_options: StorageOptions | None = None,\n    ) -&amp;gt; IOHandles[str] | IOHandles[bytes]:\n        &amp;quot;&amp;quot;&amp;quot;\n        Get file handle for given path/buffer and mode.\n    \n        Parameters\n        ----------\n        path_or_buf : str or file handle\n            File path or object.\n        mode : str\n            Mode to open path_or_buf with.\n        encoding : str or None\n            Encoding to use.\n        {compression_options}\n    \n               May be a dict with key &amp;#x27;method&amp;#x27; as compression mode\n               and other keys as compression options if compression\n               mode is &amp;#x27;zip&amp;#x27;.\n    \n               Passing compression options as keys in dict is\n               supported for compression modes &amp;#x27;gzip&amp;#x27;, &amp;#x27;bz2&amp;#x27;, &amp;#x27;zstd&amp;#x27; and &amp;#x27;zip&amp;#x27;.\n    \n            .. versionchanged:: 1.4.0 Zstandard support.\n    \n        memory_map : bool, default False\n            See parsers._parser_params for more information. Only used by read_csv.\n        is_text : bool, default True\n            Whether the type of the content passed to the file/buffer is string or\n            bytes. This is not the same as `&amp;quot;b&amp;quot; not in mode`. If a string content is\n            passed to a binary file/buffer, a wrapper is inserted.\n        errors : str, default &amp;#x27;strict&amp;#x27;\n            Specifies how encoding and decoding errors are to be handled.\n            See the errors argument for :func:`open` for a full list\n            of options.\n        storage_options: StorageOptions = None\n            Passed to _get_filepath_or_buffer\n    \n        Returns the dataclass IOHandles\n        &amp;quot;&amp;quot;&amp;quot;\n        # Windows does not default to utf-8. Set to utf-8 for a consistent behavior\n        encoding = encoding or &amp;quot;utf-8&amp;quot;\n    \n        errors = errors or &amp;quot;strict&amp;quot;\n    \n        # read_csv does not know whether the buffer is opened in binary/text mode\n        if _is_binary_mode(path_or_buf, mode) and &amp;quot;b&amp;quot; not in mode:\n            mode += &amp;quot;b&amp;quot;\n    \n        # validate encoding and errors\n        codecs.lookup(encoding)\n        if isinstance(errors, str):\n            codecs.lookup_error(errors)\n    \n        # open URLs\n        ioargs = _get_filepath_or_buffer(\n            path_or_buf,\n            encoding=encoding,\n            compression=compression,\n            mode=mode,\n            storage_options=storage_options,\n        )\n    \n        handle = ioargs.filepath_or_buffer\n        handles: list[BaseBuffer]\n    \n        # memory mapping needs to be the first step\n        # only used for read_csv\n        handle, memory_map, handles = _maybe_memory_map(handle, memory_map)\n    \n        is_path = isinstance(handle, str)\n        compression_args = dict(ioargs.compression)\n        compression = compression_args.pop(&amp;quot;method&amp;quot;)\n    \n        # Only for write methods\n        if &amp;quot;r&amp;quot; not in mode and is_path:\n            check_parent_directory(str(handle))\n    \n        if compression:\n            if compression != &amp;quot;zstd&amp;quot;:\n                # compression libraries do not like an explicit text-mode\n                ioargs.mode = ioargs.mode.replace(&amp;quot;t&amp;quot;, &amp;quot;&amp;quot;)\n            elif compression == &amp;quot;zstd&amp;quot; and &amp;quot;b&amp;quot; not in ioargs.mode:\n                # python-zstandard defaults to text mode, but we always expect\n                # compression libraries to use binary mode.\n                ioargs.mode += &amp;quot;b&amp;quot;\n    \n            # GZ Compression\n            if compression == &amp;quot;gzip&amp;quot;:\n                if isinstance(handle, str):\n                    # error: Incompatible types in assignment (expression has type\n                    # &amp;quot;GzipFile&amp;quot;, variable has type &amp;quot;Union[str, BaseBuffer]&amp;quot;)\n                    handle = gzip.GzipFile(  # type: ignore[assignment]\n                        filename=handle,\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n                else:\n                    handle = gzip.GzipFile(\n                        # No overload variant of &amp;quot;GzipFile&amp;quot; matches argument types\n                        # &amp;quot;Union[str, BaseBuffer]&amp;quot;, &amp;quot;str&amp;quot;, &amp;quot;Dict[str, Any]&amp;quot;\n                        fileobj=handle,  # type: ignore[call-overload]\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n    \n            # BZ Compression\n            elif compression == &amp;quot;bz2&amp;quot;:\n                # Overload of &amp;quot;BZ2File&amp;quot; to handle pickle protocol 5\n                # &amp;quot;Union[str, BaseBuffer]&amp;quot;, &amp;quot;str&amp;quot;, &amp;quot;Dict[str, Any]&amp;quot;\n                handle = get_bz2_file()(  # type: ignore[call-overload]\n                    handle,\n                    mode=ioargs.mode,\n                    **compression_args,\n                )\n    \n            # ZIP Compression\n            elif compression == &amp;quot;zip&amp;quot;:\n                # error: Argument 1 to &amp;quot;_BytesZipFile&amp;quot; has incompatible type\n                # &amp;quot;Union[str, BaseBuffer]&amp;quot;; expected &amp;quot;Union[Union[str, PathLike[str]],\n                # ReadBuffer[bytes], WriteBuffer[bytes]]&amp;quot;\n                handle = _BytesZipFile(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n                if handle.buffer.mode == &amp;quot;r&amp;quot;:\n                    handles.append(handle)\n                    zip_names = handle.buffer.namelist()\n                    if len(zip_names) == 1:\n                        handle = handle.buffer.open(zip_names.pop())\n                    elif not zip_names:\n                        raise ValueError(f&amp;quot;Zero files found in ZIP file {path_or_buf}&amp;quot;)\n                    else:\n                        raise ValueError(\n                            &amp;quot;Multiple files found in ZIP file. &amp;quot;\n                            f&amp;quot;Only one file per ZIP: {zip_names}&amp;quot;\n                        )\n    \n            # TAR Encoding\n            elif compression == &amp;quot;tar&amp;quot;:\n                compression_args.setdefault(&amp;quot;mode&amp;quot;, ioargs.mode)\n                if isinstance(handle, str):\n                    handle = _BytesTarFile(name=handle, **compression_args)\n                else:\n                    # error: Argument &amp;quot;fileobj&amp;quot; to &amp;quot;_BytesTarFile&amp;quot; has incompatible\n                    # type &amp;quot;BaseBuffer&amp;quot;; expected &amp;quot;Union[ReadBuffer[bytes],\n                    # WriteBuffer[bytes], None]&amp;quot;\n                    handle = _BytesTarFile(\n                        fileobj=handle, **compression_args  # type: ignore[arg-type]\n                    )\n                assert isinstance(handle, _BytesTarFile)\n                if &amp;quot;r&amp;quot; in handle.buffer.mode:\n                    handles.append(handle)\n                    files = handle.buffer.getnames()\n                    if len(files) == 1:\n                        file = handle.buffer.extractfile(files[0])\n                        assert file is not None\n                        handle = file\n                    elif not files:\n                        raise ValueError(f&amp;quot;Zero files found in TAR archive {path_or_buf}&amp;quot;)\n                    else:\n                        raise ValueError(\n                            &amp;quot;Multiple files found in TAR archive. &amp;quot;\n                            f&amp;quot;Only one file per TAR archive: {files}&amp;quot;\n                        )\n    \n            # XZ Compression\n            elif compression == &amp;quot;xz&amp;quot;:\n                # error: Argument 1 to &amp;quot;LZMAFile&amp;quot; has incompatible type &amp;quot;Union[str,\n                # BaseBuffer]&amp;quot;; expected &amp;quot;Optional[Union[Union[str, bytes, PathLike[str],\n                # PathLike[bytes]], IO[bytes]], None]&amp;quot;\n                handle = get_lzma_file()(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n    \n            # Zstd Compression\n            elif compression == &amp;quot;zstd&amp;quot;:\n                zstd = import_optional_dependency(&amp;quot;zstandard&amp;quot;)\n                if &amp;quot;r&amp;quot; in ioargs.mode:\n                    open_args = {&amp;quot;dctx&amp;quot;: zstd.ZstdDecompressor(**compression_args)}\n                else:\n                    open_args = {&amp;quot;cctx&amp;quot;: zstd.ZstdCompressor(**compression_args)}\n                handle = zstd.open(\n                    handle,\n                    mode=ioargs.mode,\n                    **open_args,\n                )\n    \n            # Unrecognized Compression\n            else:\n                msg = f&amp;quot;Unrecognized compression type: {compression}&amp;quot;\n                raise ValueError(msg)\n    \n            assert not isinstance(handle, str)\n            handles.append(handle)\n    \n        elif isinstance(handle, str):\n            # Check whether the filename is to be opened in binary mode.\n            # Binary mode does not support &amp;#x27;encoding&amp;#x27; and &amp;#x27;newline&amp;#x27;.\n            if ioargs.encoding and &amp;quot;b&amp;quot; not in ioargs.mode:\n                # Encoding\n&amp;gt;               handle = open(\n                    handle,\n                    ioargs.mode,\n                    encoding=ioargs.encoding,\n                    errors=errors,\n                    newline=&amp;quot;&amp;quot;,\n                )\nE               FileNotFoundError: [Errno 2] No such file or directory: &amp;#x27;test_data/dataset_project_ds_nodata.csv&amp;#x27;\n\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/common.py:873: FileNotFoundError\n&#34;}], &#34;tests/test_classification_model.py::test_train_evaluate_lightgbm_pos&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_classification_model.py::test_train_evaluate_lightgbm_pos&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_classification_model.py::test_train_evaluate_lightgbm_pos&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_train_evaluate_lightgbm_pos():\n&amp;gt;       raw_data = pd.read_csv(&amp;quot;test_data/dataset_project_ds_nolabel.csv&amp;quot;)\n\ntests/test_classification_model.py:141: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026: in read_csv\n    return _read(filepath_or_buffer, kwds)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620: in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620: in __init__\n    self._engine = self._make_engine(f, self.engine)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880: in _make_engine\n    self.handles = get_handle(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\npath_or_buf = &amp;#x27;test_data/dataset_project_ds_nolabel.csv&amp;#x27;, mode = &amp;#x27;r&amp;#x27;\n\n    @doc(compression_options=_shared_docs[&amp;quot;compression_options&amp;quot;] % &amp;quot;path_or_buf&amp;quot;)\n    def get_handle(\n        path_or_buf: FilePath | BaseBuffer,\n        mode: str,\n        *,\n        encoding: str | None = None,\n        compression: CompressionOptions | None = None,\n        memory_map: bool = False,\n        is_text: bool = True,\n        errors: str | None = None,\n        storage_options: StorageOptions | None = None,\n    ) -&amp;gt; IOHandles[str] | IOHandles[bytes]:\n        &amp;quot;&amp;quot;&amp;quot;\n        Get file handle for given path/buffer and mode.\n    \n        Parameters\n        ----------\n        path_or_buf : str or file handle\n            File path or object.\n        mode : str\n            Mode to open path_or_buf with.\n        encoding : str or None\n            Encoding to use.\n        {compression_options}\n    \n               May be a dict with key &amp;#x27;method&amp;#x27; as compression mode\n               and other keys as compression options if compression\n               mode is &amp;#x27;zip&amp;#x27;.\n    \n               Passing compression options as keys in dict is\n               supported for compression modes &amp;#x27;gzip&amp;#x27;, &amp;#x27;bz2&amp;#x27;, &amp;#x27;zstd&amp;#x27; and &amp;#x27;zip&amp;#x27;.\n    \n            .. versionchanged:: 1.4.0 Zstandard support.\n    \n        memory_map : bool, default False\n            See parsers._parser_params for more information. Only used by read_csv.\n        is_text : bool, default True\n            Whether the type of the content passed to the file/buffer is string or\n            bytes. This is not the same as `&amp;quot;b&amp;quot; not in mode`. If a string content is\n            passed to a binary file/buffer, a wrapper is inserted.\n        errors : str, default &amp;#x27;strict&amp;#x27;\n            Specifies how encoding and decoding errors are to be handled.\n            See the errors argument for :func:`open` for a full list\n            of options.\n        storage_options: StorageOptions = None\n            Passed to _get_filepath_or_buffer\n    \n        Returns the dataclass IOHandles\n        &amp;quot;&amp;quot;&amp;quot;\n        # Windows does not default to utf-8. Set to utf-8 for a consistent behavior\n        encoding = encoding or &amp;quot;utf-8&amp;quot;\n    \n        errors = errors or &amp;quot;strict&amp;quot;\n    \n        # read_csv does not know whether the buffer is opened in binary/text mode\n        if _is_binary_mode(path_or_buf, mode) and &amp;quot;b&amp;quot; not in mode:\n            mode += &amp;quot;b&amp;quot;\n    \n        # validate encoding and errors\n        codecs.lookup(encoding)\n        if isinstance(errors, str):\n            codecs.lookup_error(errors)\n    \n        # open URLs\n        ioargs = _get_filepath_or_buffer(\n            path_or_buf,\n            encoding=encoding,\n            compression=compression,\n            mode=mode,\n            storage_options=storage_options,\n        )\n    \n        handle = ioargs.filepath_or_buffer\n        handles: list[BaseBuffer]\n    \n        # memory mapping needs to be the first step\n        # only used for read_csv\n        handle, memory_map, handles = _maybe_memory_map(handle, memory_map)\n    \n        is_path = isinstance(handle, str)\n        compression_args = dict(ioargs.compression)\n        compression = compression_args.pop(&amp;quot;method&amp;quot;)\n    \n        # Only for write methods\n        if &amp;quot;r&amp;quot; not in mode and is_path:\n            check_parent_directory(str(handle))\n    \n        if compression:\n            if compression != &amp;quot;zstd&amp;quot;:\n                # compression libraries do not like an explicit text-mode\n                ioargs.mode = ioargs.mode.replace(&amp;quot;t&amp;quot;, &amp;quot;&amp;quot;)\n            elif compression == &amp;quot;zstd&amp;quot; and &amp;quot;b&amp;quot; not in ioargs.mode:\n                # python-zstandard defaults to text mode, but we always expect\n                # compression libraries to use binary mode.\n                ioargs.mode += &amp;quot;b&amp;quot;\n    \n            # GZ Compression\n            if compression == &amp;quot;gzip&amp;quot;:\n                if isinstance(handle, str):\n                    # error: Incompatible types in assignment (expression has type\n                    # &amp;quot;GzipFile&amp;quot;, variable has type &amp;quot;Union[str, BaseBuffer]&amp;quot;)\n                    handle = gzip.GzipFile(  # type: ignore[assignment]\n                        filename=handle,\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n                else:\n                    handle = gzip.GzipFile(\n                        # No overload variant of &amp;quot;GzipFile&amp;quot; matches argument types\n                        # &amp;quot;Union[str, BaseBuffer]&amp;quot;, &amp;quot;str&amp;quot;, &amp;quot;Dict[str, Any]&amp;quot;\n                        fileobj=handle,  # type: ignore[call-overload]\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n    \n            # BZ Compression\n            elif compression == &amp;quot;bz2&amp;quot;:\n                # Overload of &amp;quot;BZ2File&amp;quot; to handle pickle protocol 5\n                # &amp;quot;Union[str, BaseBuffer]&amp;quot;, &amp;quot;str&amp;quot;, &amp;quot;Dict[str, Any]&amp;quot;\n                handle = get_bz2_file()(  # type: ignore[call-overload]\n                    handle,\n                    mode=ioargs.mode,\n                    **compression_args,\n                )\n    \n            # ZIP Compression\n            elif compression == &amp;quot;zip&amp;quot;:\n                # error: Argument 1 to &amp;quot;_BytesZipFile&amp;quot; has incompatible type\n                # &amp;quot;Union[str, BaseBuffer]&amp;quot;; expected &amp;quot;Union[Union[str, PathLike[str]],\n                # ReadBuffer[bytes], WriteBuffer[bytes]]&amp;quot;\n                handle = _BytesZipFile(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n                if handle.buffer.mode == &amp;quot;r&amp;quot;:\n                    handles.append(handle)\n                    zip_names = handle.buffer.namelist()\n                    if len(zip_names) == 1:\n                        handle = handle.buffer.open(zip_names.pop())\n                    elif not zip_names:\n                        raise ValueError(f&amp;quot;Zero files found in ZIP file {path_or_buf}&amp;quot;)\n                    else:\n                        raise ValueError(\n                            &amp;quot;Multiple files found in ZIP file. &amp;quot;\n                            f&amp;quot;Only one file per ZIP: {zip_names}&amp;quot;\n                        )\n    \n            # TAR Encoding\n            elif compression == &amp;quot;tar&amp;quot;:\n                compression_args.setdefault(&amp;quot;mode&amp;quot;, ioargs.mode)\n                if isinstance(handle, str):\n                    handle = _BytesTarFile(name=handle, **compression_args)\n                else:\n                    # error: Argument &amp;quot;fileobj&amp;quot; to &amp;quot;_BytesTarFile&amp;quot; has incompatible\n                    # type &amp;quot;BaseBuffer&amp;quot;; expected &amp;quot;Union[ReadBuffer[bytes],\n                    # WriteBuffer[bytes], None]&amp;quot;\n                    handle = _BytesTarFile(\n                        fileobj=handle, **compression_args  # type: ignore[arg-type]\n                    )\n                assert isinstance(handle, _BytesTarFile)\n                if &amp;quot;r&amp;quot; in handle.buffer.mode:\n                    handles.append(handle)\n                    files = handle.buffer.getnames()\n                    if len(files) == 1:\n                        file = handle.buffer.extractfile(files[0])\n                        assert file is not None\n                        handle = file\n                    elif not files:\n                        raise ValueError(f&amp;quot;Zero files found in TAR archive {path_or_buf}&amp;quot;)\n                    else:\n                        raise ValueError(\n                            &amp;quot;Multiple files found in TAR archive. &amp;quot;\n                            f&amp;quot;Only one file per TAR archive: {files}&amp;quot;\n                        )\n    \n            # XZ Compression\n            elif compression == &amp;quot;xz&amp;quot;:\n                # error: Argument 1 to &amp;quot;LZMAFile&amp;quot; has incompatible type &amp;quot;Union[str,\n                # BaseBuffer]&amp;quot;; expected &amp;quot;Optional[Union[Union[str, bytes, PathLike[str],\n                # PathLike[bytes]], IO[bytes]], None]&amp;quot;\n                handle = get_lzma_file()(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n    \n            # Zstd Compression\n            elif compression == &amp;quot;zstd&amp;quot;:\n                zstd = import_optional_dependency(&amp;quot;zstandard&amp;quot;)\n                if &amp;quot;r&amp;quot; in ioargs.mode:\n                    open_args = {&amp;quot;dctx&amp;quot;: zstd.ZstdDecompressor(**compression_args)}\n                else:\n                    open_args = {&amp;quot;cctx&amp;quot;: zstd.ZstdCompressor(**compression_args)}\n                handle = zstd.open(\n                    handle,\n                    mode=ioargs.mode,\n                    **open_args,\n                )\n    \n            # Unrecognized Compression\n            else:\n                msg = f&amp;quot;Unrecognized compression type: {compression}&amp;quot;\n                raise ValueError(msg)\n    \n            assert not isinstance(handle, str)\n            handles.append(handle)\n    \n        elif isinstance(handle, str):\n            # Check whether the filename is to be opened in binary mode.\n            # Binary mode does not support &amp;#x27;encoding&amp;#x27; and &amp;#x27;newline&amp;#x27;.\n            if ioargs.encoding and &amp;quot;b&amp;quot; not in ioargs.mode:\n                # Encoding\n&amp;gt;               handle = open(\n                    handle,\n                    ioargs.mode,\n                    encoding=ioargs.encoding,\n                    errors=errors,\n                    newline=&amp;quot;&amp;quot;,\n                )\nE               FileNotFoundError: [Errno 2] No such file or directory: &amp;#x27;test_data/dataset_project_ds_nolabel.csv&amp;#x27;\n\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/common.py:873: FileNotFoundError\n&#34;}], &#34;tests/test_classification_model.py::test_train_evaluate_lightgbm_neg&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_classification_model.py::test_train_evaluate_lightgbm_neg&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_classification_model.py::test_train_evaluate_lightgbm_neg&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_train_evaluate_lightgbm_neg():\n&amp;gt;       raw_data = pd.read_csv(&amp;quot;test_data/dataset_project_ds_nodata.csv&amp;quot;)\n\ntests/test_classification_model.py:152: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026: in read_csv\n    return _read(filepath_or_buffer, kwds)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620: in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620: in __init__\n    self._engine = self._make_engine(f, self.engine)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880: in _make_engine\n    self.handles = get_handle(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\npath_or_buf = &amp;#x27;test_data/dataset_project_ds_nodata.csv&amp;#x27;, mode = &amp;#x27;r&amp;#x27;\n\n    @doc(compression_options=_shared_docs[&amp;quot;compression_options&amp;quot;] % &amp;quot;path_or_buf&amp;quot;)\n    def get_handle(\n        path_or_buf: FilePath | BaseBuffer,\n        mode: str,\n        *,\n        encoding: str | None = None,\n        compression: CompressionOptions | None = None,\n        memory_map: bool = False,\n        is_text: bool = True,\n        errors: str | None = None,\n        storage_options: StorageOptions | None = None,\n    ) -&amp;gt; IOHandles[str] | IOHandles[bytes]:\n        &amp;quot;&amp;quot;&amp;quot;\n        Get file handle for given path/buffer and mode.\n    \n        Parameters\n        ----------\n        path_or_buf : str or file handle\n            File path or object.\n        mode : str\n            Mode to open path_or_buf with.\n        encoding : str or None\n            Encoding to use.\n        {compression_options}\n    \n               May be a dict with key &amp;#x27;method&amp;#x27; as compression mode\n               and other keys as compression options if compression\n               mode is &amp;#x27;zip&amp;#x27;.\n    \n               Passing compression options as keys in dict is\n               supported for compression modes &amp;#x27;gzip&amp;#x27;, &amp;#x27;bz2&amp;#x27;, &amp;#x27;zstd&amp;#x27; and &amp;#x27;zip&amp;#x27;.\n    \n            .. versionchanged:: 1.4.0 Zstandard support.\n    \n        memory_map : bool, default False\n            See parsers._parser_params for more information. Only used by read_csv.\n        is_text : bool, default True\n            Whether the type of the content passed to the file/buffer is string or\n            bytes. This is not the same as `&amp;quot;b&amp;quot; not in mode`. If a string content is\n            passed to a binary file/buffer, a wrapper is inserted.\n        errors : str, default &amp;#x27;strict&amp;#x27;\n            Specifies how encoding and decoding errors are to be handled.\n            See the errors argument for :func:`open` for a full list\n            of options.\n        storage_options: StorageOptions = None\n            Passed to _get_filepath_or_buffer\n    \n        Returns the dataclass IOHandles\n        &amp;quot;&amp;quot;&amp;quot;\n        # Windows does not default to utf-8. Set to utf-8 for a consistent behavior\n        encoding = encoding or &amp;quot;utf-8&amp;quot;\n    \n        errors = errors or &amp;quot;strict&amp;quot;\n    \n        # read_csv does not know whether the buffer is opened in binary/text mode\n        if _is_binary_mode(path_or_buf, mode) and &amp;quot;b&amp;quot; not in mode:\n            mode += &amp;quot;b&amp;quot;\n    \n        # validate encoding and errors\n        codecs.lookup(encoding)\n        if isinstance(errors, str):\n            codecs.lookup_error(errors)\n    \n        # open URLs\n        ioargs = _get_filepath_or_buffer(\n            path_or_buf,\n            encoding=encoding,\n            compression=compression,\n            mode=mode,\n            storage_options=storage_options,\n        )\n    \n        handle = ioargs.filepath_or_buffer\n        handles: list[BaseBuffer]\n    \n        # memory mapping needs to be the first step\n        # only used for read_csv\n        handle, memory_map, handles = _maybe_memory_map(handle, memory_map)\n    \n        is_path = isinstance(handle, str)\n        compression_args = dict(ioargs.compression)\n        compression = compression_args.pop(&amp;quot;method&amp;quot;)\n    \n        # Only for write methods\n        if &amp;quot;r&amp;quot; not in mode and is_path:\n            check_parent_directory(str(handle))\n    \n        if compression:\n            if compression != &amp;quot;zstd&amp;quot;:\n                # compression libraries do not like an explicit text-mode\n                ioargs.mode = ioargs.mode.replace(&amp;quot;t&amp;quot;, &amp;quot;&amp;quot;)\n            elif compression == &amp;quot;zstd&amp;quot; and &amp;quot;b&amp;quot; not in ioargs.mode:\n                # python-zstandard defaults to text mode, but we always expect\n                # compression libraries to use binary mode.\n                ioargs.mode += &amp;quot;b&amp;quot;\n    \n            # GZ Compression\n            if compression == &amp;quot;gzip&amp;quot;:\n                if isinstance(handle, str):\n                    # error: Incompatible types in assignment (expression has type\n                    # &amp;quot;GzipFile&amp;quot;, variable has type &amp;quot;Union[str, BaseBuffer]&amp;quot;)\n                    handle = gzip.GzipFile(  # type: ignore[assignment]\n                        filename=handle,\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n                else:\n                    handle = gzip.GzipFile(\n                        # No overload variant of &amp;quot;GzipFile&amp;quot; matches argument types\n                        # &amp;quot;Union[str, BaseBuffer]&amp;quot;, &amp;quot;str&amp;quot;, &amp;quot;Dict[str, Any]&amp;quot;\n                        fileobj=handle,  # type: ignore[call-overload]\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n    \n            # BZ Compression\n            elif compression == &amp;quot;bz2&amp;quot;:\n                # Overload of &amp;quot;BZ2File&amp;quot; to handle pickle protocol 5\n                # &amp;quot;Union[str, BaseBuffer]&amp;quot;, &amp;quot;str&amp;quot;, &amp;quot;Dict[str, Any]&amp;quot;\n                handle = get_bz2_file()(  # type: ignore[call-overload]\n                    handle,\n                    mode=ioargs.mode,\n                    **compression_args,\n                )\n    \n            # ZIP Compression\n            elif compression == &amp;quot;zip&amp;quot;:\n                # error: Argument 1 to &amp;quot;_BytesZipFile&amp;quot; has incompatible type\n                # &amp;quot;Union[str, BaseBuffer]&amp;quot;; expected &amp;quot;Union[Union[str, PathLike[str]],\n                # ReadBuffer[bytes], WriteBuffer[bytes]]&amp;quot;\n                handle = _BytesZipFile(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n                if handle.buffer.mode == &amp;quot;r&amp;quot;:\n                    handles.append(handle)\n                    zip_names = handle.buffer.namelist()\n                    if len(zip_names) == 1:\n                        handle = handle.buffer.open(zip_names.pop())\n                    elif not zip_names:\n                        raise ValueError(f&amp;quot;Zero files found in ZIP file {path_or_buf}&amp;quot;)\n                    else:\n                        raise ValueError(\n                            &amp;quot;Multiple files found in ZIP file. &amp;quot;\n                            f&amp;quot;Only one file per ZIP: {zip_names}&amp;quot;\n                        )\n    \n            # TAR Encoding\n            elif compression == &amp;quot;tar&amp;quot;:\n                compression_args.setdefault(&amp;quot;mode&amp;quot;, ioargs.mode)\n                if isinstance(handle, str):\n                    handle = _BytesTarFile(name=handle, **compression_args)\n                else:\n                    # error: Argument &amp;quot;fileobj&amp;quot; to &amp;quot;_BytesTarFile&amp;quot; has incompatible\n                    # type &amp;quot;BaseBuffer&amp;quot;; expected &amp;quot;Union[ReadBuffer[bytes],\n                    # WriteBuffer[bytes], None]&amp;quot;\n                    handle = _BytesTarFile(\n                        fileobj=handle, **compression_args  # type: ignore[arg-type]\n                    )\n                assert isinstance(handle, _BytesTarFile)\n                if &amp;quot;r&amp;quot; in handle.buffer.mode:\n                    handles.append(handle)\n                    files = handle.buffer.getnames()\n                    if len(files) == 1:\n                        file = handle.buffer.extractfile(files[0])\n                        assert file is not None\n                        handle = file\n                    elif not files:\n                        raise ValueError(f&amp;quot;Zero files found in TAR archive {path_or_buf}&amp;quot;)\n                    else:\n                        raise ValueError(\n                            &amp;quot;Multiple files found in TAR archive. &amp;quot;\n                            f&amp;quot;Only one file per TAR archive: {files}&amp;quot;\n                        )\n    \n            # XZ Compression\n            elif compression == &amp;quot;xz&amp;quot;:\n                # error: Argument 1 to &amp;quot;LZMAFile&amp;quot; has incompatible type &amp;quot;Union[str,\n                # BaseBuffer]&amp;quot;; expected &amp;quot;Optional[Union[Union[str, bytes, PathLike[str],\n                # PathLike[bytes]], IO[bytes]], None]&amp;quot;\n                handle = get_lzma_file()(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n    \n            # Zstd Compression\n            elif compression == &amp;quot;zstd&amp;quot;:\n                zstd = import_optional_dependency(&amp;quot;zstandard&amp;quot;)\n                if &amp;quot;r&amp;quot; in ioargs.mode:\n                    open_args = {&amp;quot;dctx&amp;quot;: zstd.ZstdDecompressor(**compression_args)}\n                else:\n                    open_args = {&amp;quot;cctx&amp;quot;: zstd.ZstdCompressor(**compression_args)}\n                handle = zstd.open(\n                    handle,\n                    mode=ioargs.mode,\n                    **open_args,\n                )\n    \n            # Unrecognized Compression\n            else:\n                msg = f&amp;quot;Unrecognized compression type: {compression}&amp;quot;\n                raise ValueError(msg)\n    \n            assert not isinstance(handle, str)\n            handles.append(handle)\n    \n        elif isinstance(handle, str):\n            # Check whether the filename is to be opened in binary mode.\n            # Binary mode does not support &amp;#x27;encoding&amp;#x27; and &amp;#x27;newline&amp;#x27;.\n            if ioargs.encoding and &amp;quot;b&amp;quot; not in ioargs.mode:\n                # Encoding\n&amp;gt;               handle = open(\n                    handle,\n                    ioargs.mode,\n                    encoding=ioargs.encoding,\n                    errors=errors,\n                    newline=&amp;quot;&amp;quot;,\n                )\nE               FileNotFoundError: [Errno 2] No such file or directory: &amp;#x27;test_data/dataset_project_ds_nodata.csv&amp;#x27;\n\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/common.py:873: FileNotFoundError\n&#34;}], &#34;tests/test_classification_model.py::test_cross_validate_model&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_classification_model.py::test_cross_validate_model&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_classification_model.py::test_cross_validate_model&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_cross_validate_model():\n&amp;gt;       raw_data = pd.read_csv(&amp;quot;test_data/dataset_project_ds_nolabel.csv&amp;quot;)\n\ntests/test_classification_model.py:163: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026: in read_csv\n    return _read(filepath_or_buffer, kwds)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620: in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620: in __init__\n    self._engine = self._make_engine(f, self.engine)\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880: in _make_engine\n    self.handles = get_handle(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\npath_or_buf = &amp;#x27;test_data/dataset_project_ds_nolabel.csv&amp;#x27;, mode = &amp;#x27;r&amp;#x27;\n\n    @doc(compression_options=_shared_docs[&amp;quot;compression_options&amp;quot;] % &amp;quot;path_or_buf&amp;quot;)\n    def get_handle(\n        path_or_buf: FilePath | BaseBuffer,\n        mode: str,\n        *,\n        encoding: str | None = None,\n        compression: CompressionOptions | None = None,\n        memory_map: bool = False,\n        is_text: bool = True,\n        errors: str | None = None,\n        storage_options: StorageOptions | None = None,\n    ) -&amp;gt; IOHandles[str] | IOHandles[bytes]:\n        &amp;quot;&amp;quot;&amp;quot;\n        Get file handle for given path/buffer and mode.\n    \n        Parameters\n        ----------\n        path_or_buf : str or file handle\n            File path or object.\n        mode : str\n            Mode to open path_or_buf with.\n        encoding : str or None\n            Encoding to use.\n        {compression_options}\n    \n               May be a dict with key &amp;#x27;method&amp;#x27; as compression mode\n               and other keys as compression options if compression\n               mode is &amp;#x27;zip&amp;#x27;.\n    \n               Passing compression options as keys in dict is\n               supported for compression modes &amp;#x27;gzip&amp;#x27;, &amp;#x27;bz2&amp;#x27;, &amp;#x27;zstd&amp;#x27; and &amp;#x27;zip&amp;#x27;.\n    \n            .. versionchanged:: 1.4.0 Zstandard support.\n    \n        memory_map : bool, default False\n            See parsers._parser_params for more information. Only used by read_csv.\n        is_text : bool, default True\n            Whether the type of the content passed to the file/buffer is string or\n            bytes. This is not the same as `&amp;quot;b&amp;quot; not in mode`. If a string content is\n            passed to a binary file/buffer, a wrapper is inserted.\n        errors : str, default &amp;#x27;strict&amp;#x27;\n            Specifies how encoding and decoding errors are to be handled.\n            See the errors argument for :func:`open` for a full list\n            of options.\n        storage_options: StorageOptions = None\n            Passed to _get_filepath_or_buffer\n    \n        Returns the dataclass IOHandles\n        &amp;quot;&amp;quot;&amp;quot;\n        # Windows does not default to utf-8. Set to utf-8 for a consistent behavior\n        encoding = encoding or &amp;quot;utf-8&amp;quot;\n    \n        errors = errors or &amp;quot;strict&amp;quot;\n    \n        # read_csv does not know whether the buffer is opened in binary/text mode\n        if _is_binary_mode(path_or_buf, mode) and &amp;quot;b&amp;quot; not in mode:\n            mode += &amp;quot;b&amp;quot;\n    \n        # validate encoding and errors\n        codecs.lookup(encoding)\n        if isinstance(errors, str):\n            codecs.lookup_error(errors)\n    \n        # open URLs\n        ioargs = _get_filepath_or_buffer(\n            path_or_buf,\n            encoding=encoding,\n            compression=compression,\n            mode=mode,\n            storage_options=storage_options,\n        )\n    \n        handle = ioargs.filepath_or_buffer\n        handles: list[BaseBuffer]\n    \n        # memory mapping needs to be the first step\n        # only used for read_csv\n        handle, memory_map, handles = _maybe_memory_map(handle, memory_map)\n    \n        is_path = isinstance(handle, str)\n        compression_args = dict(ioargs.compression)\n        compression = compression_args.pop(&amp;quot;method&amp;quot;)\n    \n        # Only for write methods\n        if &amp;quot;r&amp;quot; not in mode and is_path:\n            check_parent_directory(str(handle))\n    \n        if compression:\n            if compression != &amp;quot;zstd&amp;quot;:\n                # compression libraries do not like an explicit text-mode\n                ioargs.mode = ioargs.mode.replace(&amp;quot;t&amp;quot;, &amp;quot;&amp;quot;)\n            elif compression == &amp;quot;zstd&amp;quot; and &amp;quot;b&amp;quot; not in ioargs.mode:\n                # python-zstandard defaults to text mode, but we always expect\n                # compression libraries to use binary mode.\n                ioargs.mode += &amp;quot;b&amp;quot;\n    \n            # GZ Compression\n            if compression == &amp;quot;gzip&amp;quot;:\n                if isinstance(handle, str):\n                    # error: Incompatible types in assignment (expression has type\n                    # &amp;quot;GzipFile&amp;quot;, variable has type &amp;quot;Union[str, BaseBuffer]&amp;quot;)\n                    handle = gzip.GzipFile(  # type: ignore[assignment]\n                        filename=handle,\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n                else:\n                    handle = gzip.GzipFile(\n                        # No overload variant of &amp;quot;GzipFile&amp;quot; matches argument types\n                        # &amp;quot;Union[str, BaseBuffer]&amp;quot;, &amp;quot;str&amp;quot;, &amp;quot;Dict[str, Any]&amp;quot;\n                        fileobj=handle,  # type: ignore[call-overload]\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n    \n            # BZ Compression\n            elif compression == &amp;quot;bz2&amp;quot;:\n                # Overload of &amp;quot;BZ2File&amp;quot; to handle pickle protocol 5\n                # &amp;quot;Union[str, BaseBuffer]&amp;quot;, &amp;quot;str&amp;quot;, &amp;quot;Dict[str, Any]&amp;quot;\n                handle = get_bz2_file()(  # type: ignore[call-overload]\n                    handle,\n                    mode=ioargs.mode,\n                    **compression_args,\n                )\n    \n            # ZIP Compression\n            elif compression == &amp;quot;zip&amp;quot;:\n                # error: Argument 1 to &amp;quot;_BytesZipFile&amp;quot; has incompatible type\n                # &amp;quot;Union[str, BaseBuffer]&amp;quot;; expected &amp;quot;Union[Union[str, PathLike[str]],\n                # ReadBuffer[bytes], WriteBuffer[bytes]]&amp;quot;\n                handle = _BytesZipFile(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n                if handle.buffer.mode == &amp;quot;r&amp;quot;:\n                    handles.append(handle)\n                    zip_names = handle.buffer.namelist()\n                    if len(zip_names) == 1:\n                        handle = handle.buffer.open(zip_names.pop())\n                    elif not zip_names:\n                        raise ValueError(f&amp;quot;Zero files found in ZIP file {path_or_buf}&amp;quot;)\n                    else:\n                        raise ValueError(\n                            &amp;quot;Multiple files found in ZIP file. &amp;quot;\n                            f&amp;quot;Only one file per ZIP: {zip_names}&amp;quot;\n                        )\n    \n            # TAR Encoding\n            elif compression == &amp;quot;tar&amp;quot;:\n                compression_args.setdefault(&amp;quot;mode&amp;quot;, ioargs.mode)\n                if isinstance(handle, str):\n                    handle = _BytesTarFile(name=handle, **compression_args)\n                else:\n                    # error: Argument &amp;quot;fileobj&amp;quot; to &amp;quot;_BytesTarFile&amp;quot; has incompatible\n                    # type &amp;quot;BaseBuffer&amp;quot;; expected &amp;quot;Union[ReadBuffer[bytes],\n                    # WriteBuffer[bytes], None]&amp;quot;\n                    handle = _BytesTarFile(\n                        fileobj=handle, **compression_args  # type: ignore[arg-type]\n                    )\n                assert isinstance(handle, _BytesTarFile)\n                if &amp;quot;r&amp;quot; in handle.buffer.mode:\n                    handles.append(handle)\n                    files = handle.buffer.getnames()\n                    if len(files) == 1:\n                        file = handle.buffer.extractfile(files[0])\n                        assert file is not None\n                        handle = file\n                    elif not files:\n                        raise ValueError(f&amp;quot;Zero files found in TAR archive {path_or_buf}&amp;quot;)\n                    else:\n                        raise ValueError(\n                            &amp;quot;Multiple files found in TAR archive. &amp;quot;\n                            f&amp;quot;Only one file per TAR archive: {files}&amp;quot;\n                        )\n    \n            # XZ Compression\n            elif compression == &amp;quot;xz&amp;quot;:\n                # error: Argument 1 to &amp;quot;LZMAFile&amp;quot; has incompatible type &amp;quot;Union[str,\n                # BaseBuffer]&amp;quot;; expected &amp;quot;Optional[Union[Union[str, bytes, PathLike[str],\n                # PathLike[bytes]], IO[bytes]], None]&amp;quot;\n                handle = get_lzma_file()(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n    \n            # Zstd Compression\n            elif compression == &amp;quot;zstd&amp;quot;:\n                zstd = import_optional_dependency(&amp;quot;zstandard&amp;quot;)\n                if &amp;quot;r&amp;quot; in ioargs.mode:\n                    open_args = {&amp;quot;dctx&amp;quot;: zstd.ZstdDecompressor(**compression_args)}\n                else:\n                    open_args = {&amp;quot;cctx&amp;quot;: zstd.ZstdCompressor(**compression_args)}\n                handle = zstd.open(\n                    handle,\n                    mode=ioargs.mode,\n                    **open_args,\n                )\n    \n            # Unrecognized Compression\n            else:\n                msg = f&amp;quot;Unrecognized compression type: {compression}&amp;quot;\n                raise ValueError(msg)\n    \n            assert not isinstance(handle, str)\n            handles.append(handle)\n    \n        elif isinstance(handle, str):\n            # Check whether the filename is to be opened in binary mode.\n            # Binary mode does not support &amp;#x27;encoding&amp;#x27; and &amp;#x27;newline&amp;#x27;.\n            if ioargs.encoding and &amp;quot;b&amp;quot; not in ioargs.mode:\n                # Encoding\n&amp;gt;               handle = open(\n                    handle,\n                    ioargs.mode,\n                    encoding=ioargs.encoding,\n                    errors=errors,\n                    newline=&amp;quot;&amp;quot;,\n                )\nE               FileNotFoundError: [Errno 2] No such file or directory: &amp;#x27;test_data/dataset_project_ds_nolabel.csv&amp;#x27;\n\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/common.py:873: FileNotFoundError\n&#34;}], &#34;tests/test_complexity_metrics_extraction.py::test_extract_entropy_features_pos&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_complexity_metrics_extraction.py::test_extract_entropy_features_pos&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_complexity_metrics_extraction.py::test_extract_entropy_features_pos&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_extract_entropy_features_pos():\n        epoch_file = &amp;quot;/test_data/epochs_overlap/sub-001_epoch-001.npy&amp;quot;\n&amp;gt;       epoch = np.load(epoch_file)\n\ntests/test_complexity_metrics_extraction.py:10: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nfile = &amp;#x27;/test_data/epochs_overlap/sub-001_epoch-001.npy&amp;#x27;, mmap_mode = None, allow_pickle = False, fix_imports = True, encoding = &amp;#x27;ASCII&amp;#x27;\n\n    @set_module(&amp;#x27;numpy&amp;#x27;)\n    def load(file, mmap_mode=None, allow_pickle=False, fix_imports=True,\n             encoding=&amp;#x27;ASCII&amp;#x27;, *, max_header_size=format._MAX_HEADER_SIZE):\n        &amp;quot;&amp;quot;&amp;quot;\n        Load arrays or pickled objects from ``.npy``, ``.npz`` or pickled files.\n    \n        .. warning:: Loading files that contain object arrays uses the ``pickle``\n                     module, which is not secure against erroneous or maliciously\n                     constructed data. Consider passing ``allow_pickle=False`` to\n                     load data that is known not to contain object arrays for the\n                     safer handling of untrusted sources.\n    \n        Parameters\n        ----------\n        file : file-like object, string, or pathlib.Path\n            The file to read. File-like objects must support the\n            ``seek()`` and ``read()`` methods and must always\n            be opened in binary mode.  Pickled files require that the\n            file-like object support the ``readline()`` method as well.\n        mmap_mode : {None, &amp;#x27;r+&amp;#x27;, &amp;#x27;r&amp;#x27;, &amp;#x27;w+&amp;#x27;, &amp;#x27;c&amp;#x27;}, optional\n            If not None, then memory-map the file, using the given mode (see\n            `numpy.memmap` for a detailed description of the modes).  A\n            memory-mapped array is kept on disk. However, it can be accessed\n            and sliced like any ndarray.  Memory mapping is especially useful\n            for accessing small fragments of large files without reading the\n            entire file into memory.\n        allow_pickle : bool, optional\n            Allow loading pickled object arrays stored in npy files. Reasons for\n            disallowing pickles include security, as loading pickled data can\n            execute arbitrary code. If pickles are disallowed, loading object\n            arrays will fail. Default: False\n        fix_imports : bool, optional\n            Only useful when loading Python 2 generated pickled files on Python 3,\n            which includes npy/npz files containing object arrays. If `fix_imports`\n            is True, pickle will try to map the old Python 2 names to the new names\n            used in Python 3.\n        encoding : str, optional\n            What encoding to use when reading Python 2 strings. Only useful when\n            loading Python 2 generated pickled files in Python 3, which includes\n            npy/npz files containing object arrays. Values other than &amp;#x27;latin1&amp;#x27;,\n            &amp;#x27;ASCII&amp;#x27;, and &amp;#x27;bytes&amp;#x27; are not allowed, as they can corrupt numerical\n            data. Default: &amp;#x27;ASCII&amp;#x27;\n        max_header_size : int, optional\n            Maximum allowed size of the header.  Large headers may not be safe\n            to load securely and thus require explicitly passing a larger value.\n            See :py:func:`ast.literal_eval()` for details.\n            This option is ignored when `allow_pickle` is passed.  In that case\n            the file is by definition trusted and the limit is unnecessary.\n    \n        Returns\n        -------\n        result : array, tuple, dict, etc.\n            Data stored in the file. For ``.npz`` files, the returned instance\n            of NpzFile class must be closed to avoid leaking file descriptors.\n    \n        Raises\n        ------\n        OSError\n            If the input file does not exist or cannot be read.\n        UnpicklingError\n            If ``allow_pickle=True``, but the file cannot be loaded as a pickle.\n        ValueError\n            The file contains an object array, but ``allow_pickle=False`` given.\n        EOFError\n            When calling ``np.load`` multiple times on the same file handle,\n            if all data has already been read\n    \n        See Also\n        --------\n        save, savez, savez_compressed, loadtxt\n        memmap : Create a memory-map to an array stored in a file on disk.\n        lib.format.open_memmap : Create or load a memory-mapped ``.npy`` file.\n    \n        Notes\n        -----\n        - If the file contains pickle data, then whatever object is stored\n          in the pickle is returned.\n        - If the file is a ``.npy`` file, then a single array is returned.\n        - If the file is a ``.npz`` file, then a dictionary-like object is\n          returned, containing ``{filename: array}`` key-value pairs, one for\n          each file in the archive.\n        - If the file is a ``.npz`` file, the returned value supports the\n          context manager protocol in a similar fashion to the open function::\n    \n            with load(&amp;#x27;foo.npz&amp;#x27;) as data:\n                a = data[&amp;#x27;a&amp;#x27;]\n    \n          The underlying file descriptor is closed when exiting the &amp;#x27;with&amp;#x27;\n          block.\n    \n        Examples\n        --------\n        &amp;gt;&amp;gt;&amp;gt; import numpy as np\n    \n        Store data to disk, and load it again:\n    \n        &amp;gt;&amp;gt;&amp;gt; np.save(&amp;#x27;/tmp/123&amp;#x27;, np.array([[1, 2, 3], [4, 5, 6]]))\n        &amp;gt;&amp;gt;&amp;gt; np.load(&amp;#x27;/tmp/123.npy&amp;#x27;)\n        array([[1, 2, 3],\n               [4, 5, 6]])\n    \n        Store compressed data to disk, and load it again:\n    \n        &amp;gt;&amp;gt;&amp;gt; a=np.array([[1, 2, 3], [4, 5, 6]])\n        &amp;gt;&amp;gt;&amp;gt; b=np.array([1, 2])\n        &amp;gt;&amp;gt;&amp;gt; np.savez(&amp;#x27;/tmp/123.npz&amp;#x27;, a=a, b=b)\n        &amp;gt;&amp;gt;&amp;gt; data = np.load(&amp;#x27;/tmp/123.npz&amp;#x27;)\n        &amp;gt;&amp;gt;&amp;gt; data[&amp;#x27;a&amp;#x27;]\n        array([[1, 2, 3],\n               [4, 5, 6]])\n        &amp;gt;&amp;gt;&amp;gt; data[&amp;#x27;b&amp;#x27;]\n        array([1, 2])\n        &amp;gt;&amp;gt;&amp;gt; data.close()\n    \n        Mem-map the stored array, and then access the second row\n        directly from disk:\n    \n        &amp;gt;&amp;gt;&amp;gt; X = np.load(&amp;#x27;/tmp/123.npy&amp;#x27;, mmap_mode=&amp;#x27;r&amp;#x27;)\n        &amp;gt;&amp;gt;&amp;gt; X[1, :]\n        memmap([4, 5, 6])\n    \n        &amp;quot;&amp;quot;&amp;quot;\n        if encoding not in (&amp;#x27;ASCII&amp;#x27;, &amp;#x27;latin1&amp;#x27;, &amp;#x27;bytes&amp;#x27;):\n            # The &amp;#x27;encoding&amp;#x27; value for pickle also affects what encoding\n            # the serialized binary data of NumPy arrays is loaded\n            # in. Pickle does not pass on the encoding information to\n            # NumPy. The unpickling code in numpy._core.multiarray is\n            # written to assume that unicode data appearing where binary\n            # should be is in &amp;#x27;latin1&amp;#x27;. &amp;#x27;bytes&amp;#x27; is also safe, as is &amp;#x27;ASCII&amp;#x27;.\n            #\n            # Other encoding values can corrupt binary data, and we\n            # purposefully disallow them. For the same reason, the errors=\n            # argument is not exposed, as values other than &amp;#x27;strict&amp;#x27;\n            # result can similarly silently corrupt numerical data.\n            raise ValueError(&amp;quot;encoding must be &amp;#x27;ASCII&amp;#x27;, &amp;#x27;latin1&amp;#x27;, or &amp;#x27;bytes&amp;#x27;&amp;quot;)\n    \n        pickle_kwargs = dict(encoding=encoding, fix_imports=fix_imports)\n    \n        with contextlib.ExitStack() as stack:\n            if hasattr(file, &amp;#x27;read&amp;#x27;):\n                fid = file\n                own_fid = False\n            else:\n&amp;gt;               fid = stack.enter_context(open(os.fspath(file), &amp;quot;rb&amp;quot;))\nE               FileNotFoundError: [Errno 2] No such file or directory: &amp;#x27;/test_data/epochs_overlap/sub-001_epoch-001.npy&amp;#x27;\n\n.tox/py312-test/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:451: FileNotFoundError\n&#34;}], &#34;tests/test_complexity_metrics_extraction.py::test_extract_entropy_features_neg&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_complexity_metrics_extraction.py::test_extract_entropy_features_neg&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_complexity_metrics_extraction.py::test_extract_entropy_features_neg&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_extract_entropy_features_neg():\n        epoch_file = &amp;quot;/test_data/epochs_overlap/001_epo-001.npy&amp;quot;\n&amp;gt;       epoch = np.load(epoch_file)\n\ntests/test_complexity_metrics_extraction.py:18: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nfile = &amp;#x27;/test_data/epochs_overlap/001_epo-001.npy&amp;#x27;, mmap_mode = None, allow_pickle = False, fix_imports = True, encoding = &amp;#x27;ASCII&amp;#x27;\n\n    @set_module(&amp;#x27;numpy&amp;#x27;)\n    def load(file, mmap_mode=None, allow_pickle=False, fix_imports=True,\n             encoding=&amp;#x27;ASCII&amp;#x27;, *, max_header_size=format._MAX_HEADER_SIZE):\n        &amp;quot;&amp;quot;&amp;quot;\n        Load arrays or pickled objects from ``.npy``, ``.npz`` or pickled files.\n    \n        .. warning:: Loading files that contain object arrays uses the ``pickle``\n                     module, which is not secure against erroneous or maliciously\n                     constructed data. Consider passing ``allow_pickle=False`` to\n                     load data that is known not to contain object arrays for the\n                     safer handling of untrusted sources.\n    \n        Parameters\n        ----------\n        file : file-like object, string, or pathlib.Path\n            The file to read. File-like objects must support the\n            ``seek()`` and ``read()`` methods and must always\n            be opened in binary mode.  Pickled files require that the\n            file-like object support the ``readline()`` method as well.\n        mmap_mode : {None, &amp;#x27;r+&amp;#x27;, &amp;#x27;r&amp;#x27;, &amp;#x27;w+&amp;#x27;, &amp;#x27;c&amp;#x27;}, optional\n            If not None, then memory-map the file, using the given mode (see\n            `numpy.memmap` for a detailed description of the modes).  A\n            memory-mapped array is kept on disk. However, it can be accessed\n            and sliced like any ndarray.  Memory mapping is especially useful\n            for accessing small fragments of large files without reading the\n            entire file into memory.\n        allow_pickle : bool, optional\n            Allow loading pickled object arrays stored in npy files. Reasons for\n            disallowing pickles include security, as loading pickled data can\n            execute arbitrary code. If pickles are disallowed, loading object\n            arrays will fail. Default: False\n        fix_imports : bool, optional\n            Only useful when loading Python 2 generated pickled files on Python 3,\n            which includes npy/npz files containing object arrays. If `fix_imports`\n            is True, pickle will try to map the old Python 2 names to the new names\n            used in Python 3.\n        encoding : str, optional\n            What encoding to use when reading Python 2 strings. Only useful when\n            loading Python 2 generated pickled files in Python 3, which includes\n            npy/npz files containing object arrays. Values other than &amp;#x27;latin1&amp;#x27;,\n            &amp;#x27;ASCII&amp;#x27;, and &amp;#x27;bytes&amp;#x27; are not allowed, as they can corrupt numerical\n            data. Default: &amp;#x27;ASCII&amp;#x27;\n        max_header_size : int, optional\n            Maximum allowed size of the header.  Large headers may not be safe\n            to load securely and thus require explicitly passing a larger value.\n            See :py:func:`ast.literal_eval()` for details.\n            This option is ignored when `allow_pickle` is passed.  In that case\n            the file is by definition trusted and the limit is unnecessary.\n    \n        Returns\n        -------\n        result : array, tuple, dict, etc.\n            Data stored in the file. For ``.npz`` files, the returned instance\n            of NpzFile class must be closed to avoid leaking file descriptors.\n    \n        Raises\n        ------\n        OSError\n            If the input file does not exist or cannot be read.\n        UnpicklingError\n            If ``allow_pickle=True``, but the file cannot be loaded as a pickle.\n        ValueError\n            The file contains an object array, but ``allow_pickle=False`` given.\n        EOFError\n            When calling ``np.load`` multiple times on the same file handle,\n            if all data has already been read\n    \n        See Also\n        --------\n        save, savez, savez_compressed, loadtxt\n        memmap : Create a memory-map to an array stored in a file on disk.\n        lib.format.open_memmap : Create or load a memory-mapped ``.npy`` file.\n    \n        Notes\n        -----\n        - If the file contains pickle data, then whatever object is stored\n          in the pickle is returned.\n        - If the file is a ``.npy`` file, then a single array is returned.\n        - If the file is a ``.npz`` file, then a dictionary-like object is\n          returned, containing ``{filename: array}`` key-value pairs, one for\n          each file in the archive.\n        - If the file is a ``.npz`` file, the returned value supports the\n          context manager protocol in a similar fashion to the open function::\n    \n            with load(&amp;#x27;foo.npz&amp;#x27;) as data:\n                a = data[&amp;#x27;a&amp;#x27;]\n    \n          The underlying file descriptor is closed when exiting the &amp;#x27;with&amp;#x27;\n          block.\n    \n        Examples\n        --------\n        &amp;gt;&amp;gt;&amp;gt; import numpy as np\n    \n        Store data to disk, and load it again:\n    \n        &amp;gt;&amp;gt;&amp;gt; np.save(&amp;#x27;/tmp/123&amp;#x27;, np.array([[1, 2, 3], [4, 5, 6]]))\n        &amp;gt;&amp;gt;&amp;gt; np.load(&amp;#x27;/tmp/123.npy&amp;#x27;)\n        array([[1, 2, 3],\n               [4, 5, 6]])\n    \n        Store compressed data to disk, and load it again:\n    \n        &amp;gt;&amp;gt;&amp;gt; a=np.array([[1, 2, 3], [4, 5, 6]])\n        &amp;gt;&amp;gt;&amp;gt; b=np.array([1, 2])\n        &amp;gt;&amp;gt;&amp;gt; np.savez(&amp;#x27;/tmp/123.npz&amp;#x27;, a=a, b=b)\n        &amp;gt;&amp;gt;&amp;gt; data = np.load(&amp;#x27;/tmp/123.npz&amp;#x27;)\n        &amp;gt;&amp;gt;&amp;gt; data[&amp;#x27;a&amp;#x27;]\n        array([[1, 2, 3],\n               [4, 5, 6]])\n        &amp;gt;&amp;gt;&amp;gt; data[&amp;#x27;b&amp;#x27;]\n        array([1, 2])\n        &amp;gt;&amp;gt;&amp;gt; data.close()\n    \n        Mem-map the stored array, and then access the second row\n        directly from disk:\n    \n        &amp;gt;&amp;gt;&amp;gt; X = np.load(&amp;#x27;/tmp/123.npy&amp;#x27;, mmap_mode=&amp;#x27;r&amp;#x27;)\n        &amp;gt;&amp;gt;&amp;gt; X[1, :]\n        memmap([4, 5, 6])\n    \n        &amp;quot;&amp;quot;&amp;quot;\n        if encoding not in (&amp;#x27;ASCII&amp;#x27;, &amp;#x27;latin1&amp;#x27;, &amp;#x27;bytes&amp;#x27;):\n            # The &amp;#x27;encoding&amp;#x27; value for pickle also affects what encoding\n            # the serialized binary data of NumPy arrays is loaded\n            # in. Pickle does not pass on the encoding information to\n            # NumPy. The unpickling code in numpy._core.multiarray is\n            # written to assume that unicode data appearing where binary\n            # should be is in &amp;#x27;latin1&amp;#x27;. &amp;#x27;bytes&amp;#x27; is also safe, as is &amp;#x27;ASCII&amp;#x27;.\n            #\n            # Other encoding values can corrupt binary data, and we\n            # purposefully disallow them. For the same reason, the errors=\n            # argument is not exposed, as values other than &amp;#x27;strict&amp;#x27;\n            # result can similarly silently corrupt numerical data.\n            raise ValueError(&amp;quot;encoding must be &amp;#x27;ASCII&amp;#x27;, &amp;#x27;latin1&amp;#x27;, or &amp;#x27;bytes&amp;#x27;&amp;quot;)\n    \n        pickle_kwargs = dict(encoding=encoding, fix_imports=fix_imports)\n    \n        with contextlib.ExitStack() as stack:\n            if hasattr(file, &amp;#x27;read&amp;#x27;):\n                fid = file\n                own_fid = False\n            else:\n&amp;gt;               fid = stack.enter_context(open(os.fspath(file), &amp;quot;rb&amp;quot;))\nE               FileNotFoundError: [Errno 2] No such file or directory: &amp;#x27;/test_data/epochs_overlap/001_epo-001.npy&amp;#x27;\n\n.tox/py312-test/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:451: FileNotFoundError\n&#34;}], &#34;tests/test_complexity_metrics_extraction.py::test_process_epoch_file_pos&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_complexity_metrics_extraction.py::test_process_epoch_file_pos&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_complexity_metrics_extraction.py::test_process_epoch_file_pos&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_process_epoch_file_pos():\n        complexity_file_csv = &amp;quot;test_data/complex.csv&amp;quot;\n        output_folder = &amp;quot;test_data/epochs_overlap&amp;quot;\n&amp;gt;       files_to_process, processed_set = checkpoint(complexity_file_csv, output_folder)\n\ntests/test_complexity_metrics_extraction.py:27: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ncsv_file = &amp;#x27;test_data/complex.csv&amp;#x27;, output_folder = &amp;#x27;test_data/epochs_overlap&amp;#x27;\n\n    def checkpoint(csv_file, output_folder):\n        &amp;quot;&amp;quot;&amp;quot;\n        Checkpoint the processed data to disk.\n        &amp;quot;&amp;quot;&amp;quot;\n        # Precompute processed files upfront\n        if os.path.exists(csv_file) and os.path.getsize(csv_file) &amp;gt; 0:\n            existing_df = pd.read_csv(csv_file)[[&amp;#x27;subject_id&amp;#x27;, &amp;#x27;epoch_number&amp;#x27;]]\n            processed_set = set(\n                (row[&amp;#x27;subject_id&amp;#x27;], row[&amp;#x27;epoch_number&amp;#x27;]) for _, row in existing_df.iterrows())\n        else:\n            processed_set = set()\n    \n        # List files to process\n        files_to_process = [\n&amp;gt;           f for f in os.listdir(output_folder) if f.endswith(&amp;#x27;.npy&amp;#x27;)\n        ]\nE       FileNotFoundError: [Errno 2] No such file or directory: &amp;#x27;test_data/epochs_overlap&amp;#x27;\n\nsrc/utilities.py:18: FileNotFoundError\n&#34;}], &#34;tests/test_complexity_metrics_extraction.py::test_process_epoch_file_neg&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_complexity_metrics_extraction.py::test_process_epoch_file_neg&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_complexity_metrics_extraction.py::test_process_epoch_file_neg&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_process_epoch_file_neg():\n        complexity_file_csv = &amp;quot;test_data/comp.csv&amp;quot;\n        output_folder = &amp;quot;test_data/epochs_overlap&amp;quot;\n&amp;gt;       files_to_process, processed_set = checkpoint(complexity_file_csv, output_folder)\n\ntests/test_complexity_metrics_extraction.py:37: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ncsv_file = &amp;#x27;test_data/comp.csv&amp;#x27;, output_folder = &amp;#x27;test_data/epochs_overlap&amp;#x27;\n\n    def checkpoint(csv_file, output_folder):\n        &amp;quot;&amp;quot;&amp;quot;\n        Checkpoint the processed data to disk.\n        &amp;quot;&amp;quot;&amp;quot;\n        # Precompute processed files upfront\n        if os.path.exists(csv_file) and os.path.getsize(csv_file) &amp;gt; 0:\n            existing_df = pd.read_csv(csv_file)[[&amp;#x27;subject_id&amp;#x27;, &amp;#x27;epoch_number&amp;#x27;]]\n            processed_set = set(\n                (row[&amp;#x27;subject_id&amp;#x27;], row[&amp;#x27;epoch_number&amp;#x27;]) for _, row in existing_df.iterrows())\n        else:\n            processed_set = set()\n    \n        # List files to process\n        files_to_process = [\n&amp;gt;           f for f in os.listdir(output_folder) if f.endswith(&amp;#x27;.npy&amp;#x27;)\n        ]\nE       FileNotFoundError: [Errno 2] No such file or directory: &amp;#x27;test_data/epochs_overlap&amp;#x27;\n\nsrc/utilities.py:18: FileNotFoundError\n&#34;}], &#34;tests/test_epoch_extraction.py::test_pos_extract_epochs&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/test_epoch_extraction.py::test_pos_extract_epochs&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_epoch_extraction.py::test_pos_extract_epochs&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;------------------------------ Captured log call -------------------------------\nERROR    root:preprocessing.py:21 Error reading data: input_fname does not exist: &amp;quot;/Users/folasewaabdulsalam/DS_Alzheimer_Project/test_data/sub-001/eeg/sub-001_task-eyesclosed_eeg.set&amp;quot;\nERROR    logger:epoch_extraction.py:46 Error extracting epochs &amp;#x27;NoneType&amp;#x27; object has no attribute &amp;#x27;info&amp;#x27;\n\n&#34;}], &#34;tests/test_epoch_extraction.py::test_neg_extract_epochs&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_epoch_extraction.py::test_neg_extract_epochs&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_epoch_extraction.py::test_neg_extract_epochs&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_neg_extract_epochs():\n        data_path = &amp;quot;test_data/suc-001/eeg/sub-001_task-eyesclosed_eeg.set&amp;quot;\n        raw = read_data(data_path)\n        epoch = extract_epochs(raw, epoch_length=4, overlap=0.5)\n&amp;gt;       assert epoch is None\nE       assert array([], dtype=float64) is None\n\ntests/test_epoch_extraction.py:14: AssertionError\n\n------------------------------ Captured log call -------------------------------\nERROR    root:preprocessing.py:21 Error reading data: input_fname does not exist: &amp;quot;/Users/folasewaabdulsalam/DS_Alzheimer_Project/test_data/suc-001/eeg/sub-001_task-eyesclosed_eeg.set&amp;quot;\nERROR    logger:epoch_extraction.py:46 Error extracting epochs &amp;#x27;NoneType&amp;#x27; object has no attribute &amp;#x27;info&amp;#x27;\n\n&#34;}], &#34;tests/test_epoch_extraction.py::test_pos_batch_extract_epochs&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_epoch_extraction.py::test_pos_batch_extract_epochs&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_epoch_extraction.py::test_pos_batch_extract_epochs&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_pos_batch_extract_epochs():\n        data_folder_path = &amp;quot;/preprocessed_filtered&amp;quot;\n        output_folder_path = &amp;quot;/epochs_overlap&amp;quot;\n        epochs_data = batch_extract_epochs(data_folder_path, output_folder_path)\n    \n&amp;gt;       assert len (epochs_data) &amp;gt; 0\nE       assert 0 &amp;gt; 0\nE        +  where 0 = len(array([], dtype=float64))\n\ntests/test_epoch_extraction.py:21: AssertionError\n\n------------------------------ Captured log call -------------------------------\nERROR    logger:epoch_extraction.py:80 Error extracting Epoch [Errno 2] No such file or directory: &amp;#x27;/preprocessed_filtered&amp;#x27;\n\n&#34;}], &#34;tests/test_epoch_extraction.py::test_neg_batch__extract_epochs&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_epoch_extraction.py::test_neg_batch__extract_epochs&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_epoch_extraction.py::test_neg_batch__extract_epochs&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_neg_batch__extract_epochs():\n    \n        data_folder_path = &amp;quot;/preproce_filtered&amp;quot;\n        output_folder_path = &amp;quot;/epochs_overlap&amp;quot;\n        epochs_data = batch_extract_epochs(data_folder_path, output_folder_path)\n    \n&amp;gt;       assert epochs_data is None\nE       assert array([], dtype=float64) is None\n\ntests/test_epoch_extraction.py:29: AssertionError\n\n------------------------------ Captured log call -------------------------------\nERROR    logger:epoch_extraction.py:80 Error extracting Epoch [Errno 2] No such file or directory: &amp;#x27;/preproce_filtered&amp;#x27;\n\n&#34;}], &#34;tests/test_preprocessing.py::test_pos_read_data&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_preprocessing.py::test_pos_read_data&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_preprocessing.py::test_pos_read_data&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_pos_read_data():\n        data_path = &amp;quot;test_data/sub-001/eeg/sub-001_task-eyesclosed_eeg.set&amp;quot;\n        data = read_data(data_path)\n&amp;gt;       assert data is not None\nE       assert None is not None\n\ntests/test_preprocessing.py:9: AssertionError\n\n------------------------------ Captured log call -------------------------------\nERROR    root:preprocessing.py:21 Error reading data: input_fname does not exist: &amp;quot;/Users/folasewaabdulsalam/DS_Alzheimer_Project/test_data/sub-001/eeg/sub-001_task-eyesclosed_eeg.set&amp;quot;\n\n&#34;}], &#34;tests/test_preprocessing.py::test_neg_read_data&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/test_preprocessing.py::test_neg_read_data&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_preprocessing.py::test_neg_read_data&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;------------------------------ Captured log call -------------------------------\nERROR    root:preprocessing.py:21 Error reading data: input_fname does not exist: &amp;quot;/Users/folasewaabdulsalam/DS_Alzheimer_Project/test_data/sub-001/eeg/sub-001_task-eyesclosed_eeg.set&amp;quot;\n\n&#34;}], &#34;tests/test_preprocessing.py::test_pos_copy_data&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_preprocessing.py::test_pos_copy_data&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_preprocessing.py::test_pos_copy_data&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_pos_copy_data():\n        source_path = &amp;quot;test_data&amp;quot;\n        destination_path = &amp;quot;filtered_subjects&amp;quot;\n&amp;gt;       os.path.makedirs(destination_path, exist_ok=True)\nE       AttributeError: module &amp;#x27;posixpath&amp;#x27; has no attribute &amp;#x27;makedirs&amp;#x27;\n\ntests/test_preprocessing.py:19: AttributeError\n&#34;}], &#34;tests/test_preprocessing.py::test_neg_copy_data&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_preprocessing.py::test_neg_copy_data&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_preprocessing.py::test_neg_copy_data&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_neg_copy_data():\n        source_path = &amp;quot;test_dataz&amp;quot;\n        destination_path = &amp;quot;filtered_subjects&amp;quot;\n&amp;gt;       os.path.makedirs(destination_path, exist_ok=True)\nE       AttributeError: module &amp;#x27;posixpath&amp;#x27; has no attribute &amp;#x27;makedirs&amp;#x27;\n\ntests/test_preprocessing.py:29: AttributeError\n&#34;}], &#34;tests/test_preprocessing.py::test_pos_check_noise_present&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_preprocessing.py::test_pos_check_noise_present&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_preprocessing.py::test_pos_check_noise_present&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_pos_check_noise_present():\n        dataset = read_data(&amp;quot;test_data/sub-001/eeg/sub-001_task-eyesclosed_eeg.set&amp;quot;)\n        noise_threshold = 1e-6\n        n_components = 20\n        noise_dataset = check_noise(dataset, noise_threshold, n_components)\n&amp;gt;       assert noise_dataset is not None\nE       assert None is not None\n\ntests/test_preprocessing.py:40: AssertionError\n\n------------------------------ Captured log call -------------------------------\nERROR    root:preprocessing.py:21 Error reading data: input_fname does not exist: &amp;quot;/Users/folasewaabdulsalam/DS_Alzheimer_Project/test_data/sub-001/eeg/sub-001_task-eyesclosed_eeg.set&amp;quot;\nERROR    root:preprocessing.py:65 Error checking noise: &amp;#x27;NoneType&amp;#x27; object has no attribute &amp;#x27;get_data&amp;#x27;\n\n&#34;}], &#34;tests/test_preprocessing.py::test_pos_check_noise_absent&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_preprocessing.py::test_pos_check_noise_absent&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_preprocessing.py::test_pos_check_noise_absent&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_pos_check_noise_absent():\n        dataset = read_data(&amp;quot;test_data/sub-001/eeg/sub-001_task-eyesclosed_eeg.set&amp;quot;)\n        noise_threshold = 1e-6\n        n_components = 20\n        noise_dataset = check_noise(dataset, noise_threshold, n_components)\n&amp;gt;       assert noise_dataset is not None\nE       assert None is not None\n\ntests/test_preprocessing.py:48: AssertionError\n\n------------------------------ Captured log call -------------------------------\nERROR    root:preprocessing.py:21 Error reading data: input_fname does not exist: &amp;quot;/Users/folasewaabdulsalam/DS_Alzheimer_Project/test_data/sub-001/eeg/sub-001_task-eyesclosed_eeg.set&amp;quot;\nERROR    root:preprocessing.py:65 Error checking noise: &amp;#x27;NoneType&amp;#x27; object has no attribute &amp;#x27;get_data&amp;#x27;\n\n&#34;}], &#34;tests/test_preprocessing.py::test_neg_check_noise&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Passed&#34;, &#34;testId&#34;: &#34;tests/test_preprocessing.py::test_neg_check_noise&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Passed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_preprocessing.py::test_neg_check_noise&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;------------------------------ Captured log call -------------------------------\nERROR    root:preprocessing.py:21 Error reading data: input_fname does not exist: &amp;quot;/Users/folasewaabdulsalam/DS_Alzheimer_Project/test_data/sub-001/eeg/sub-001_task-eyesclosed_eeg.set&amp;quot;\nERROR    root:preprocessing.py:65 Error checking noise: &amp;#x27;NoneType&amp;#x27; object has no attribute &amp;#x27;get_data&amp;#x27;\n\n&#34;}], &#34;tests/test_spectrum_metrics_extraction.py::test_compute_basic_statistics&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_spectrum_metrics_extraction.py::test_compute_basic_statistics&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_spectrum_metrics_extraction.py::test_compute_basic_statistics&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_compute_basic_statistics():\n    \n        epoch_file = &amp;quot;/test_data/epochs_overlap/sub-001_epoch-001.npy&amp;quot;\n&amp;gt;       epoch = np.load(epoch_file)\n\ntests/test_spectrum_metrics_extraction.py:8: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nfile = &amp;#x27;/test_data/epochs_overlap/sub-001_epoch-001.npy&amp;#x27;, mmap_mode = None, allow_pickle = False, fix_imports = True, encoding = &amp;#x27;ASCII&amp;#x27;\n\n    @set_module(&amp;#x27;numpy&amp;#x27;)\n    def load(file, mmap_mode=None, allow_pickle=False, fix_imports=True,\n             encoding=&amp;#x27;ASCII&amp;#x27;, *, max_header_size=format._MAX_HEADER_SIZE):\n        &amp;quot;&amp;quot;&amp;quot;\n        Load arrays or pickled objects from ``.npy``, ``.npz`` or pickled files.\n    \n        .. warning:: Loading files that contain object arrays uses the ``pickle``\n                     module, which is not secure against erroneous or maliciously\n                     constructed data. Consider passing ``allow_pickle=False`` to\n                     load data that is known not to contain object arrays for the\n                     safer handling of untrusted sources.\n    \n        Parameters\n        ----------\n        file : file-like object, string, or pathlib.Path\n            The file to read. File-like objects must support the\n            ``seek()`` and ``read()`` methods and must always\n            be opened in binary mode.  Pickled files require that the\n            file-like object support the ``readline()`` method as well.\n        mmap_mode : {None, &amp;#x27;r+&amp;#x27;, &amp;#x27;r&amp;#x27;, &amp;#x27;w+&amp;#x27;, &amp;#x27;c&amp;#x27;}, optional\n            If not None, then memory-map the file, using the given mode (see\n            `numpy.memmap` for a detailed description of the modes).  A\n            memory-mapped array is kept on disk. However, it can be accessed\n            and sliced like any ndarray.  Memory mapping is especially useful\n            for accessing small fragments of large files without reading the\n            entire file into memory.\n        allow_pickle : bool, optional\n            Allow loading pickled object arrays stored in npy files. Reasons for\n            disallowing pickles include security, as loading pickled data can\n            execute arbitrary code. If pickles are disallowed, loading object\n            arrays will fail. Default: False\n        fix_imports : bool, optional\n            Only useful when loading Python 2 generated pickled files on Python 3,\n            which includes npy/npz files containing object arrays. If `fix_imports`\n            is True, pickle will try to map the old Python 2 names to the new names\n            used in Python 3.\n        encoding : str, optional\n            What encoding to use when reading Python 2 strings. Only useful when\n            loading Python 2 generated pickled files in Python 3, which includes\n            npy/npz files containing object arrays. Values other than &amp;#x27;latin1&amp;#x27;,\n            &amp;#x27;ASCII&amp;#x27;, and &amp;#x27;bytes&amp;#x27; are not allowed, as they can corrupt numerical\n            data. Default: &amp;#x27;ASCII&amp;#x27;\n        max_header_size : int, optional\n            Maximum allowed size of the header.  Large headers may not be safe\n            to load securely and thus require explicitly passing a larger value.\n            See :py:func:`ast.literal_eval()` for details.\n            This option is ignored when `allow_pickle` is passed.  In that case\n            the file is by definition trusted and the limit is unnecessary.\n    \n        Returns\n        -------\n        result : array, tuple, dict, etc.\n            Data stored in the file. For ``.npz`` files, the returned instance\n            of NpzFile class must be closed to avoid leaking file descriptors.\n    \n        Raises\n        ------\n        OSError\n            If the input file does not exist or cannot be read.\n        UnpicklingError\n            If ``allow_pickle=True``, but the file cannot be loaded as a pickle.\n        ValueError\n            The file contains an object array, but ``allow_pickle=False`` given.\n        EOFError\n            When calling ``np.load`` multiple times on the same file handle,\n            if all data has already been read\n    \n        See Also\n        --------\n        save, savez, savez_compressed, loadtxt\n        memmap : Create a memory-map to an array stored in a file on disk.\n        lib.format.open_memmap : Create or load a memory-mapped ``.npy`` file.\n    \n        Notes\n        -----\n        - If the file contains pickle data, then whatever object is stored\n          in the pickle is returned.\n        - If the file is a ``.npy`` file, then a single array is returned.\n        - If the file is a ``.npz`` file, then a dictionary-like object is\n          returned, containing ``{filename: array}`` key-value pairs, one for\n          each file in the archive.\n        - If the file is a ``.npz`` file, the returned value supports the\n          context manager protocol in a similar fashion to the open function::\n    \n            with load(&amp;#x27;foo.npz&amp;#x27;) as data:\n                a = data[&amp;#x27;a&amp;#x27;]\n    \n          The underlying file descriptor is closed when exiting the &amp;#x27;with&amp;#x27;\n          block.\n    \n        Examples\n        --------\n        &amp;gt;&amp;gt;&amp;gt; import numpy as np\n    \n        Store data to disk, and load it again:\n    \n        &amp;gt;&amp;gt;&amp;gt; np.save(&amp;#x27;/tmp/123&amp;#x27;, np.array([[1, 2, 3], [4, 5, 6]]))\n        &amp;gt;&amp;gt;&amp;gt; np.load(&amp;#x27;/tmp/123.npy&amp;#x27;)\n        array([[1, 2, 3],\n               [4, 5, 6]])\n    \n        Store compressed data to disk, and load it again:\n    \n        &amp;gt;&amp;gt;&amp;gt; a=np.array([[1, 2, 3], [4, 5, 6]])\n        &amp;gt;&amp;gt;&amp;gt; b=np.array([1, 2])\n        &amp;gt;&amp;gt;&amp;gt; np.savez(&amp;#x27;/tmp/123.npz&amp;#x27;, a=a, b=b)\n        &amp;gt;&amp;gt;&amp;gt; data = np.load(&amp;#x27;/tmp/123.npz&amp;#x27;)\n        &amp;gt;&amp;gt;&amp;gt; data[&amp;#x27;a&amp;#x27;]\n        array([[1, 2, 3],\n               [4, 5, 6]])\n        &amp;gt;&amp;gt;&amp;gt; data[&amp;#x27;b&amp;#x27;]\n        array([1, 2])\n        &amp;gt;&amp;gt;&amp;gt; data.close()\n    \n        Mem-map the stored array, and then access the second row\n        directly from disk:\n    \n        &amp;gt;&amp;gt;&amp;gt; X = np.load(&amp;#x27;/tmp/123.npy&amp;#x27;, mmap_mode=&amp;#x27;r&amp;#x27;)\n        &amp;gt;&amp;gt;&amp;gt; X[1, :]\n        memmap([4, 5, 6])\n    \n        &amp;quot;&amp;quot;&amp;quot;\n        if encoding not in (&amp;#x27;ASCII&amp;#x27;, &amp;#x27;latin1&amp;#x27;, &amp;#x27;bytes&amp;#x27;):\n            # The &amp;#x27;encoding&amp;#x27; value for pickle also affects what encoding\n            # the serialized binary data of NumPy arrays is loaded\n            # in. Pickle does not pass on the encoding information to\n            # NumPy. The unpickling code in numpy._core.multiarray is\n            # written to assume that unicode data appearing where binary\n            # should be is in &amp;#x27;latin1&amp;#x27;. &amp;#x27;bytes&amp;#x27; is also safe, as is &amp;#x27;ASCII&amp;#x27;.\n            #\n            # Other encoding values can corrupt binary data, and we\n            # purposefully disallow them. For the same reason, the errors=\n            # argument is not exposed, as values other than &amp;#x27;strict&amp;#x27;\n            # result can similarly silently corrupt numerical data.\n            raise ValueError(&amp;quot;encoding must be &amp;#x27;ASCII&amp;#x27;, &amp;#x27;latin1&amp;#x27;, or &amp;#x27;bytes&amp;#x27;&amp;quot;)\n    \n        pickle_kwargs = dict(encoding=encoding, fix_imports=fix_imports)\n    \n        with contextlib.ExitStack() as stack:\n            if hasattr(file, &amp;#x27;read&amp;#x27;):\n                fid = file\n                own_fid = False\n            else:\n&amp;gt;               fid = stack.enter_context(open(os.fspath(file), &amp;quot;rb&amp;quot;))\nE               FileNotFoundError: [Errno 2] No such file or directory: &amp;#x27;/test_data/epochs_overlap/sub-001_epoch-001.npy&amp;#x27;\n\n.tox/py312-test/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:451: FileNotFoundError\n&#34;}], &#34;tests/test_spectrum_metrics_extraction.py::test_compute_psd&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_spectrum_metrics_extraction.py::test_compute_psd&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_spectrum_metrics_extraction.py::test_compute_psd&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_compute_psd():\n        sfreq = 500\n        epoch_file = &amp;quot;/test_data/epochs_overlap/sub-001_epoch-001.npy&amp;quot;\n&amp;gt;       epoch = np.load(epoch_file)\n\ntests/test_spectrum_metrics_extraction.py:15: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nfile = &amp;#x27;/test_data/epochs_overlap/sub-001_epoch-001.npy&amp;#x27;, mmap_mode = None, allow_pickle = False, fix_imports = True, encoding = &amp;#x27;ASCII&amp;#x27;\n\n    @set_module(&amp;#x27;numpy&amp;#x27;)\n    def load(file, mmap_mode=None, allow_pickle=False, fix_imports=True,\n             encoding=&amp;#x27;ASCII&amp;#x27;, *, max_header_size=format._MAX_HEADER_SIZE):\n        &amp;quot;&amp;quot;&amp;quot;\n        Load arrays or pickled objects from ``.npy``, ``.npz`` or pickled files.\n    \n        .. warning:: Loading files that contain object arrays uses the ``pickle``\n                     module, which is not secure against erroneous or maliciously\n                     constructed data. Consider passing ``allow_pickle=False`` to\n                     load data that is known not to contain object arrays for the\n                     safer handling of untrusted sources.\n    \n        Parameters\n        ----------\n        file : file-like object, string, or pathlib.Path\n            The file to read. File-like objects must support the\n            ``seek()`` and ``read()`` methods and must always\n            be opened in binary mode.  Pickled files require that the\n            file-like object support the ``readline()`` method as well.\n        mmap_mode : {None, &amp;#x27;r+&amp;#x27;, &amp;#x27;r&amp;#x27;, &amp;#x27;w+&amp;#x27;, &amp;#x27;c&amp;#x27;}, optional\n            If not None, then memory-map the file, using the given mode (see\n            `numpy.memmap` for a detailed description of the modes).  A\n            memory-mapped array is kept on disk. However, it can be accessed\n            and sliced like any ndarray.  Memory mapping is especially useful\n            for accessing small fragments of large files without reading the\n            entire file into memory.\n        allow_pickle : bool, optional\n            Allow loading pickled object arrays stored in npy files. Reasons for\n            disallowing pickles include security, as loading pickled data can\n            execute arbitrary code. If pickles are disallowed, loading object\n            arrays will fail. Default: False\n        fix_imports : bool, optional\n            Only useful when loading Python 2 generated pickled files on Python 3,\n            which includes npy/npz files containing object arrays. If `fix_imports`\n            is True, pickle will try to map the old Python 2 names to the new names\n            used in Python 3.\n        encoding : str, optional\n            What encoding to use when reading Python 2 strings. Only useful when\n            loading Python 2 generated pickled files in Python 3, which includes\n            npy/npz files containing object arrays. Values other than &amp;#x27;latin1&amp;#x27;,\n            &amp;#x27;ASCII&amp;#x27;, and &amp;#x27;bytes&amp;#x27; are not allowed, as they can corrupt numerical\n            data. Default: &amp;#x27;ASCII&amp;#x27;\n        max_header_size : int, optional\n            Maximum allowed size of the header.  Large headers may not be safe\n            to load securely and thus require explicitly passing a larger value.\n            See :py:func:`ast.literal_eval()` for details.\n            This option is ignored when `allow_pickle` is passed.  In that case\n            the file is by definition trusted and the limit is unnecessary.\n    \n        Returns\n        -------\n        result : array, tuple, dict, etc.\n            Data stored in the file. For ``.npz`` files, the returned instance\n            of NpzFile class must be closed to avoid leaking file descriptors.\n    \n        Raises\n        ------\n        OSError\n            If the input file does not exist or cannot be read.\n        UnpicklingError\n            If ``allow_pickle=True``, but the file cannot be loaded as a pickle.\n        ValueError\n            The file contains an object array, but ``allow_pickle=False`` given.\n        EOFError\n            When calling ``np.load`` multiple times on the same file handle,\n            if all data has already been read\n    \n        See Also\n        --------\n        save, savez, savez_compressed, loadtxt\n        memmap : Create a memory-map to an array stored in a file on disk.\n        lib.format.open_memmap : Create or load a memory-mapped ``.npy`` file.\n    \n        Notes\n        -----\n        - If the file contains pickle data, then whatever object is stored\n          in the pickle is returned.\n        - If the file is a ``.npy`` file, then a single array is returned.\n        - If the file is a ``.npz`` file, then a dictionary-like object is\n          returned, containing ``{filename: array}`` key-value pairs, one for\n          each file in the archive.\n        - If the file is a ``.npz`` file, the returned value supports the\n          context manager protocol in a similar fashion to the open function::\n    \n            with load(&amp;#x27;foo.npz&amp;#x27;) as data:\n                a = data[&amp;#x27;a&amp;#x27;]\n    \n          The underlying file descriptor is closed when exiting the &amp;#x27;with&amp;#x27;\n          block.\n    \n        Examples\n        --------\n        &amp;gt;&amp;gt;&amp;gt; import numpy as np\n    \n        Store data to disk, and load it again:\n    \n        &amp;gt;&amp;gt;&amp;gt; np.save(&amp;#x27;/tmp/123&amp;#x27;, np.array([[1, 2, 3], [4, 5, 6]]))\n        &amp;gt;&amp;gt;&amp;gt; np.load(&amp;#x27;/tmp/123.npy&amp;#x27;)\n        array([[1, 2, 3],\n               [4, 5, 6]])\n    \n        Store compressed data to disk, and load it again:\n    \n        &amp;gt;&amp;gt;&amp;gt; a=np.array([[1, 2, 3], [4, 5, 6]])\n        &amp;gt;&amp;gt;&amp;gt; b=np.array([1, 2])\n        &amp;gt;&amp;gt;&amp;gt; np.savez(&amp;#x27;/tmp/123.npz&amp;#x27;, a=a, b=b)\n        &amp;gt;&amp;gt;&amp;gt; data = np.load(&amp;#x27;/tmp/123.npz&amp;#x27;)\n        &amp;gt;&amp;gt;&amp;gt; data[&amp;#x27;a&amp;#x27;]\n        array([[1, 2, 3],\n               [4, 5, 6]])\n        &amp;gt;&amp;gt;&amp;gt; data[&amp;#x27;b&amp;#x27;]\n        array([1, 2])\n        &amp;gt;&amp;gt;&amp;gt; data.close()\n    \n        Mem-map the stored array, and then access the second row\n        directly from disk:\n    \n        &amp;gt;&amp;gt;&amp;gt; X = np.load(&amp;#x27;/tmp/123.npy&amp;#x27;, mmap_mode=&amp;#x27;r&amp;#x27;)\n        &amp;gt;&amp;gt;&amp;gt; X[1, :]\n        memmap([4, 5, 6])\n    \n        &amp;quot;&amp;quot;&amp;quot;\n        if encoding not in (&amp;#x27;ASCII&amp;#x27;, &amp;#x27;latin1&amp;#x27;, &amp;#x27;bytes&amp;#x27;):\n            # The &amp;#x27;encoding&amp;#x27; value for pickle also affects what encoding\n            # the serialized binary data of NumPy arrays is loaded\n            # in. Pickle does not pass on the encoding information to\n            # NumPy. The unpickling code in numpy._core.multiarray is\n            # written to assume that unicode data appearing where binary\n            # should be is in &amp;#x27;latin1&amp;#x27;. &amp;#x27;bytes&amp;#x27; is also safe, as is &amp;#x27;ASCII&amp;#x27;.\n            #\n            # Other encoding values can corrupt binary data, and we\n            # purposefully disallow them. For the same reason, the errors=\n            # argument is not exposed, as values other than &amp;#x27;strict&amp;#x27;\n            # result can similarly silently corrupt numerical data.\n            raise ValueError(&amp;quot;encoding must be &amp;#x27;ASCII&amp;#x27;, &amp;#x27;latin1&amp;#x27;, or &amp;#x27;bytes&amp;#x27;&amp;quot;)\n    \n        pickle_kwargs = dict(encoding=encoding, fix_imports=fix_imports)\n    \n        with contextlib.ExitStack() as stack:\n            if hasattr(file, &amp;#x27;read&amp;#x27;):\n                fid = file\n                own_fid = False\n            else:\n&amp;gt;               fid = stack.enter_context(open(os.fspath(file), &amp;quot;rb&amp;quot;))\nE               FileNotFoundError: [Errno 2] No such file or directory: &amp;#x27;/test_data/epochs_overlap/sub-001_epoch-001.npy&amp;#x27;\n\n.tox/py312-test/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:451: FileNotFoundError\n&#34;}], &#34;tests/test_spectrum_metrics_extraction.py::test_compute_relative_band_power&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_spectrum_metrics_extraction.py::test_compute_relative_band_power&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_spectrum_metrics_extraction.py::test_compute_relative_band_power&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_compute_relative_band_power():\n        sfreq = 500\n        n_channels = 19\n        epoch_file = &amp;quot;/test_data/epochs_overlap/sub-001_epoch-001.npy&amp;quot;\n&amp;gt;       epoch = np.load(epoch_file)\n\ntests/test_spectrum_metrics_extraction.py:23: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nfile = &amp;#x27;/test_data/epochs_overlap/sub-001_epoch-001.npy&amp;#x27;, mmap_mode = None, allow_pickle = False, fix_imports = True, encoding = &amp;#x27;ASCII&amp;#x27;\n\n    @set_module(&amp;#x27;numpy&amp;#x27;)\n    def load(file, mmap_mode=None, allow_pickle=False, fix_imports=True,\n             encoding=&amp;#x27;ASCII&amp;#x27;, *, max_header_size=format._MAX_HEADER_SIZE):\n        &amp;quot;&amp;quot;&amp;quot;\n        Load arrays or pickled objects from ``.npy``, ``.npz`` or pickled files.\n    \n        .. warning:: Loading files that contain object arrays uses the ``pickle``\n                     module, which is not secure against erroneous or maliciously\n                     constructed data. Consider passing ``allow_pickle=False`` to\n                     load data that is known not to contain object arrays for the\n                     safer handling of untrusted sources.\n    \n        Parameters\n        ----------\n        file : file-like object, string, or pathlib.Path\n            The file to read. File-like objects must support the\n            ``seek()`` and ``read()`` methods and must always\n            be opened in binary mode.  Pickled files require that the\n            file-like object support the ``readline()`` method as well.\n        mmap_mode : {None, &amp;#x27;r+&amp;#x27;, &amp;#x27;r&amp;#x27;, &amp;#x27;w+&amp;#x27;, &amp;#x27;c&amp;#x27;}, optional\n            If not None, then memory-map the file, using the given mode (see\n            `numpy.memmap` for a detailed description of the modes).  A\n            memory-mapped array is kept on disk. However, it can be accessed\n            and sliced like any ndarray.  Memory mapping is especially useful\n            for accessing small fragments of large files without reading the\n            entire file into memory.\n        allow_pickle : bool, optional\n            Allow loading pickled object arrays stored in npy files. Reasons for\n            disallowing pickles include security, as loading pickled data can\n            execute arbitrary code. If pickles are disallowed, loading object\n            arrays will fail. Default: False\n        fix_imports : bool, optional\n            Only useful when loading Python 2 generated pickled files on Python 3,\n            which includes npy/npz files containing object arrays. If `fix_imports`\n            is True, pickle will try to map the old Python 2 names to the new names\n            used in Python 3.\n        encoding : str, optional\n            What encoding to use when reading Python 2 strings. Only useful when\n            loading Python 2 generated pickled files in Python 3, which includes\n            npy/npz files containing object arrays. Values other than &amp;#x27;latin1&amp;#x27;,\n            &amp;#x27;ASCII&amp;#x27;, and &amp;#x27;bytes&amp;#x27; are not allowed, as they can corrupt numerical\n            data. Default: &amp;#x27;ASCII&amp;#x27;\n        max_header_size : int, optional\n            Maximum allowed size of the header.  Large headers may not be safe\n            to load securely and thus require explicitly passing a larger value.\n            See :py:func:`ast.literal_eval()` for details.\n            This option is ignored when `allow_pickle` is passed.  In that case\n            the file is by definition trusted and the limit is unnecessary.\n    \n        Returns\n        -------\n        result : array, tuple, dict, etc.\n            Data stored in the file. For ``.npz`` files, the returned instance\n            of NpzFile class must be closed to avoid leaking file descriptors.\n    \n        Raises\n        ------\n        OSError\n            If the input file does not exist or cannot be read.\n        UnpicklingError\n            If ``allow_pickle=True``, but the file cannot be loaded as a pickle.\n        ValueError\n            The file contains an object array, but ``allow_pickle=False`` given.\n        EOFError\n            When calling ``np.load`` multiple times on the same file handle,\n            if all data has already been read\n    \n        See Also\n        --------\n        save, savez, savez_compressed, loadtxt\n        memmap : Create a memory-map to an array stored in a file on disk.\n        lib.format.open_memmap : Create or load a memory-mapped ``.npy`` file.\n    \n        Notes\n        -----\n        - If the file contains pickle data, then whatever object is stored\n          in the pickle is returned.\n        - If the file is a ``.npy`` file, then a single array is returned.\n        - If the file is a ``.npz`` file, then a dictionary-like object is\n          returned, containing ``{filename: array}`` key-value pairs, one for\n          each file in the archive.\n        - If the file is a ``.npz`` file, the returned value supports the\n          context manager protocol in a similar fashion to the open function::\n    \n            with load(&amp;#x27;foo.npz&amp;#x27;) as data:\n                a = data[&amp;#x27;a&amp;#x27;]\n    \n          The underlying file descriptor is closed when exiting the &amp;#x27;with&amp;#x27;\n          block.\n    \n        Examples\n        --------\n        &amp;gt;&amp;gt;&amp;gt; import numpy as np\n    \n        Store data to disk, and load it again:\n    \n        &amp;gt;&amp;gt;&amp;gt; np.save(&amp;#x27;/tmp/123&amp;#x27;, np.array([[1, 2, 3], [4, 5, 6]]))\n        &amp;gt;&amp;gt;&amp;gt; np.load(&amp;#x27;/tmp/123.npy&amp;#x27;)\n        array([[1, 2, 3],\n               [4, 5, 6]])\n    \n        Store compressed data to disk, and load it again:\n    \n        &amp;gt;&amp;gt;&amp;gt; a=np.array([[1, 2, 3], [4, 5, 6]])\n        &amp;gt;&amp;gt;&amp;gt; b=np.array([1, 2])\n        &amp;gt;&amp;gt;&amp;gt; np.savez(&amp;#x27;/tmp/123.npz&amp;#x27;, a=a, b=b)\n        &amp;gt;&amp;gt;&amp;gt; data = np.load(&amp;#x27;/tmp/123.npz&amp;#x27;)\n        &amp;gt;&amp;gt;&amp;gt; data[&amp;#x27;a&amp;#x27;]\n        array([[1, 2, 3],\n               [4, 5, 6]])\n        &amp;gt;&amp;gt;&amp;gt; data[&amp;#x27;b&amp;#x27;]\n        array([1, 2])\n        &amp;gt;&amp;gt;&amp;gt; data.close()\n    \n        Mem-map the stored array, and then access the second row\n        directly from disk:\n    \n        &amp;gt;&amp;gt;&amp;gt; X = np.load(&amp;#x27;/tmp/123.npy&amp;#x27;, mmap_mode=&amp;#x27;r&amp;#x27;)\n        &amp;gt;&amp;gt;&amp;gt; X[1, :]\n        memmap([4, 5, 6])\n    \n        &amp;quot;&amp;quot;&amp;quot;\n        if encoding not in (&amp;#x27;ASCII&amp;#x27;, &amp;#x27;latin1&amp;#x27;, &amp;#x27;bytes&amp;#x27;):\n            # The &amp;#x27;encoding&amp;#x27; value for pickle also affects what encoding\n            # the serialized binary data of NumPy arrays is loaded\n            # in. Pickle does not pass on the encoding information to\n            # NumPy. The unpickling code in numpy._core.multiarray is\n            # written to assume that unicode data appearing where binary\n            # should be is in &amp;#x27;latin1&amp;#x27;. &amp;#x27;bytes&amp;#x27; is also safe, as is &amp;#x27;ASCII&amp;#x27;.\n            #\n            # Other encoding values can corrupt binary data, and we\n            # purposefully disallow them. For the same reason, the errors=\n            # argument is not exposed, as values other than &amp;#x27;strict&amp;#x27;\n            # result can similarly silently corrupt numerical data.\n            raise ValueError(&amp;quot;encoding must be &amp;#x27;ASCII&amp;#x27;, &amp;#x27;latin1&amp;#x27;, or &amp;#x27;bytes&amp;#x27;&amp;quot;)\n    \n        pickle_kwargs = dict(encoding=encoding, fix_imports=fix_imports)\n    \n        with contextlib.ExitStack() as stack:\n            if hasattr(file, &amp;#x27;read&amp;#x27;):\n                fid = file\n                own_fid = False\n            else:\n&amp;gt;               fid = stack.enter_context(open(os.fspath(file), &amp;quot;rb&amp;quot;))\nE               FileNotFoundError: [Errno 2] No such file or directory: &amp;#x27;/test_data/epochs_overlap/sub-001_epoch-001.npy&amp;#x27;\n\n.tox/py312-test/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:451: FileNotFoundError\n&#34;}], &#34;tests/test_spectrum_metrics_extraction.py::test_extract_spectrum_features&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_spectrum_metrics_extraction.py::test_extract_spectrum_features&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_spectrum_metrics_extraction.py::test_extract_spectrum_features&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_extract_spectrum_features():\n        sfreq = 500\n        epoch_file = &amp;quot;/test_data/epochs_overlap/sub-001_epoch-001.npy&amp;quot;\n&amp;gt;       epoch = np.load(epoch_file)\n\ntests/test_spectrum_metrics_extraction.py:31: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nfile = &amp;#x27;/test_data/epochs_overlap/sub-001_epoch-001.npy&amp;#x27;, mmap_mode = None, allow_pickle = False, fix_imports = True, encoding = &amp;#x27;ASCII&amp;#x27;\n\n    @set_module(&amp;#x27;numpy&amp;#x27;)\n    def load(file, mmap_mode=None, allow_pickle=False, fix_imports=True,\n             encoding=&amp;#x27;ASCII&amp;#x27;, *, max_header_size=format._MAX_HEADER_SIZE):\n        &amp;quot;&amp;quot;&amp;quot;\n        Load arrays or pickled objects from ``.npy``, ``.npz`` or pickled files.\n    \n        .. warning:: Loading files that contain object arrays uses the ``pickle``\n                     module, which is not secure against erroneous or maliciously\n                     constructed data. Consider passing ``allow_pickle=False`` to\n                     load data that is known not to contain object arrays for the\n                     safer handling of untrusted sources.\n    \n        Parameters\n        ----------\n        file : file-like object, string, or pathlib.Path\n            The file to read. File-like objects must support the\n            ``seek()`` and ``read()`` methods and must always\n            be opened in binary mode.  Pickled files require that the\n            file-like object support the ``readline()`` method as well.\n        mmap_mode : {None, &amp;#x27;r+&amp;#x27;, &amp;#x27;r&amp;#x27;, &amp;#x27;w+&amp;#x27;, &amp;#x27;c&amp;#x27;}, optional\n            If not None, then memory-map the file, using the given mode (see\n            `numpy.memmap` for a detailed description of the modes).  A\n            memory-mapped array is kept on disk. However, it can be accessed\n            and sliced like any ndarray.  Memory mapping is especially useful\n            for accessing small fragments of large files without reading the\n            entire file into memory.\n        allow_pickle : bool, optional\n            Allow loading pickled object arrays stored in npy files. Reasons for\n            disallowing pickles include security, as loading pickled data can\n            execute arbitrary code. If pickles are disallowed, loading object\n            arrays will fail. Default: False\n        fix_imports : bool, optional\n            Only useful when loading Python 2 generated pickled files on Python 3,\n            which includes npy/npz files containing object arrays. If `fix_imports`\n            is True, pickle will try to map the old Python 2 names to the new names\n            used in Python 3.\n        encoding : str, optional\n            What encoding to use when reading Python 2 strings. Only useful when\n            loading Python 2 generated pickled files in Python 3, which includes\n            npy/npz files containing object arrays. Values other than &amp;#x27;latin1&amp;#x27;,\n            &amp;#x27;ASCII&amp;#x27;, and &amp;#x27;bytes&amp;#x27; are not allowed, as they can corrupt numerical\n            data. Default: &amp;#x27;ASCII&amp;#x27;\n        max_header_size : int, optional\n            Maximum allowed size of the header.  Large headers may not be safe\n            to load securely and thus require explicitly passing a larger value.\n            See :py:func:`ast.literal_eval()` for details.\n            This option is ignored when `allow_pickle` is passed.  In that case\n            the file is by definition trusted and the limit is unnecessary.\n    \n        Returns\n        -------\n        result : array, tuple, dict, etc.\n            Data stored in the file. For ``.npz`` files, the returned instance\n            of NpzFile class must be closed to avoid leaking file descriptors.\n    \n        Raises\n        ------\n        OSError\n            If the input file does not exist or cannot be read.\n        UnpicklingError\n            If ``allow_pickle=True``, but the file cannot be loaded as a pickle.\n        ValueError\n            The file contains an object array, but ``allow_pickle=False`` given.\n        EOFError\n            When calling ``np.load`` multiple times on the same file handle,\n            if all data has already been read\n    \n        See Also\n        --------\n        save, savez, savez_compressed, loadtxt\n        memmap : Create a memory-map to an array stored in a file on disk.\n        lib.format.open_memmap : Create or load a memory-mapped ``.npy`` file.\n    \n        Notes\n        -----\n        - If the file contains pickle data, then whatever object is stored\n          in the pickle is returned.\n        - If the file is a ``.npy`` file, then a single array is returned.\n        - If the file is a ``.npz`` file, then a dictionary-like object is\n          returned, containing ``{filename: array}`` key-value pairs, one for\n          each file in the archive.\n        - If the file is a ``.npz`` file, the returned value supports the\n          context manager protocol in a similar fashion to the open function::\n    \n            with load(&amp;#x27;foo.npz&amp;#x27;) as data:\n                a = data[&amp;#x27;a&amp;#x27;]\n    \n          The underlying file descriptor is closed when exiting the &amp;#x27;with&amp;#x27;\n          block.\n    \n        Examples\n        --------\n        &amp;gt;&amp;gt;&amp;gt; import numpy as np\n    \n        Store data to disk, and load it again:\n    \n        &amp;gt;&amp;gt;&amp;gt; np.save(&amp;#x27;/tmp/123&amp;#x27;, np.array([[1, 2, 3], [4, 5, 6]]))\n        &amp;gt;&amp;gt;&amp;gt; np.load(&amp;#x27;/tmp/123.npy&amp;#x27;)\n        array([[1, 2, 3],\n               [4, 5, 6]])\n    \n        Store compressed data to disk, and load it again:\n    \n        &amp;gt;&amp;gt;&amp;gt; a=np.array([[1, 2, 3], [4, 5, 6]])\n        &amp;gt;&amp;gt;&amp;gt; b=np.array([1, 2])\n        &amp;gt;&amp;gt;&amp;gt; np.savez(&amp;#x27;/tmp/123.npz&amp;#x27;, a=a, b=b)\n        &amp;gt;&amp;gt;&amp;gt; data = np.load(&amp;#x27;/tmp/123.npz&amp;#x27;)\n        &amp;gt;&amp;gt;&amp;gt; data[&amp;#x27;a&amp;#x27;]\n        array([[1, 2, 3],\n               [4, 5, 6]])\n        &amp;gt;&amp;gt;&amp;gt; data[&amp;#x27;b&amp;#x27;]\n        array([1, 2])\n        &amp;gt;&amp;gt;&amp;gt; data.close()\n    \n        Mem-map the stored array, and then access the second row\n        directly from disk:\n    \n        &amp;gt;&amp;gt;&amp;gt; X = np.load(&amp;#x27;/tmp/123.npy&amp;#x27;, mmap_mode=&amp;#x27;r&amp;#x27;)\n        &amp;gt;&amp;gt;&amp;gt; X[1, :]\n        memmap([4, 5, 6])\n    \n        &amp;quot;&amp;quot;&amp;quot;\n        if encoding not in (&amp;#x27;ASCII&amp;#x27;, &amp;#x27;latin1&amp;#x27;, &amp;#x27;bytes&amp;#x27;):\n            # The &amp;#x27;encoding&amp;#x27; value for pickle also affects what encoding\n            # the serialized binary data of NumPy arrays is loaded\n            # in. Pickle does not pass on the encoding information to\n            # NumPy. The unpickling code in numpy._core.multiarray is\n            # written to assume that unicode data appearing where binary\n            # should be is in &amp;#x27;latin1&amp;#x27;. &amp;#x27;bytes&amp;#x27; is also safe, as is &amp;#x27;ASCII&amp;#x27;.\n            #\n            # Other encoding values can corrupt binary data, and we\n            # purposefully disallow them. For the same reason, the errors=\n            # argument is not exposed, as values other than &amp;#x27;strict&amp;#x27;\n            # result can similarly silently corrupt numerical data.\n            raise ValueError(&amp;quot;encoding must be &amp;#x27;ASCII&amp;#x27;, &amp;#x27;latin1&amp;#x27;, or &amp;#x27;bytes&amp;#x27;&amp;quot;)\n    \n        pickle_kwargs = dict(encoding=encoding, fix_imports=fix_imports)\n    \n        with contextlib.ExitStack() as stack:\n            if hasattr(file, &amp;#x27;read&amp;#x27;):\n                fid = file\n                own_fid = False\n            else:\n&amp;gt;               fid = stack.enter_context(open(os.fspath(file), &amp;quot;rb&amp;quot;))\nE               FileNotFoundError: [Errno 2] No such file or directory: &amp;#x27;/test_data/epochs_overlap/sub-001_epoch-001.npy&amp;#x27;\n\n.tox/py312-test/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:451: FileNotFoundError\n&#34;}], &#34;tests/test_spectrum_metrics_extraction.py::test_batch_extract_spectrum_features&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_spectrum_metrics_extraction.py::test_batch_extract_spectrum_features&#34;, &#34;duration&#34;: &#34;3 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_spectrum_metrics_extraction.py::test_batch_extract_spectrum_features&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;3 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_batch_extract_spectrum_features():\n        output_folder = &amp;quot;test_data/epochs_overlap&amp;quot;\n        spectrum_file_csv = &amp;quot;test_data/spectrum.csv&amp;quot;\n        sfreq = 500\n&amp;gt;       spectrum_features_df = batch_extract_spectrum_features(output_folder, spectrum_file_csv, sfreq)\n\ntests/test_spectrum_metrics_extraction.py:39: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsrc/spectrum_metrics_extraction.py:189: in batch_extract_spectrum_features\n    spectrum_features_df.to_csv(spectrum_file_csv, index=False)\n.tox/py312-test/lib/python3.12/site-packages/pandas/util/_decorators.py:333: in wrapper\n    return func(*args, **kwargs)\n.tox/py312-test/lib/python3.12/site-packages/pandas/core/generic.py:3967: in to_csv\n    return DataFrameRenderer(formatter).to_csv(\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/formats/format.py:1014: in to_csv\n    csv_formatter.save()\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/formats/csvs.py:251: in save\n    with get_handle(\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/common.py:749: in get_handle\n    check_parent_directory(str(handle))\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\npath = &amp;#x27;test_data/spectrum.csv&amp;#x27;\n\n    def check_parent_directory(path: Path | str) -&amp;gt; None:\n        &amp;quot;&amp;quot;&amp;quot;\n        Check if parent directory of a file exists, raise OSError if it does not\n    \n        Parameters\n        ----------\n        path: Path or str\n            Path to check parent directory of\n        &amp;quot;&amp;quot;&amp;quot;\n        parent = Path(path).parent\n        if not parent.is_dir():\n&amp;gt;           raise OSError(rf&amp;quot;Cannot save file into a non-existent directory: &amp;#x27;{parent}&amp;#x27;&amp;quot;)\nE           OSError: Cannot save file into a non-existent directory: &amp;#x27;test_data&amp;#x27;\n\n.tox/py312-test/lib/python3.12/site-packages/pandas/io/common.py:616: OSError\n\n------------------------------ Captured log call -------------------------------\nERROR    logger:spectrum_metrics_extraction.py:181 Error extracting features [Errno 2] No such file or directory: &amp;#x27;test_data/epochs_overlap&amp;#x27;\n\n&#34;}], &#34;tests/test_synchronization_metrics_extraction.py::test_connectivity_matrix_pos&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_synchronization_metrics_extraction.py::test_connectivity_matrix_pos&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_synchronization_metrics_extraction.py::test_connectivity_matrix_pos&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_connectivity_matrix_pos():\n&amp;gt;       epoch = np.load(&amp;quot;/test_data/epochs_overlap/sub-001_epoch-001.npy&amp;quot;)\n\ntests/test_synchronization_metrics_extraction.py:15: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nfile = &amp;#x27;/test_data/epochs_overlap/sub-001_epoch-001.npy&amp;#x27;, mmap_mode = None, allow_pickle = False, fix_imports = True, encoding = &amp;#x27;ASCII&amp;#x27;\n\n    @set_module(&amp;#x27;numpy&amp;#x27;)\n    def load(file, mmap_mode=None, allow_pickle=False, fix_imports=True,\n             encoding=&amp;#x27;ASCII&amp;#x27;, *, max_header_size=format._MAX_HEADER_SIZE):\n        &amp;quot;&amp;quot;&amp;quot;\n        Load arrays or pickled objects from ``.npy``, ``.npz`` or pickled files.\n    \n        .. warning:: Loading files that contain object arrays uses the ``pickle``\n                     module, which is not secure against erroneous or maliciously\n                     constructed data. Consider passing ``allow_pickle=False`` to\n                     load data that is known not to contain object arrays for the\n                     safer handling of untrusted sources.\n    \n        Parameters\n        ----------\n        file : file-like object, string, or pathlib.Path\n            The file to read. File-like objects must support the\n            ``seek()`` and ``read()`` methods and must always\n            be opened in binary mode.  Pickled files require that the\n            file-like object support the ``readline()`` method as well.\n        mmap_mode : {None, &amp;#x27;r+&amp;#x27;, &amp;#x27;r&amp;#x27;, &amp;#x27;w+&amp;#x27;, &amp;#x27;c&amp;#x27;}, optional\n            If not None, then memory-map the file, using the given mode (see\n            `numpy.memmap` for a detailed description of the modes).  A\n            memory-mapped array is kept on disk. However, it can be accessed\n            and sliced like any ndarray.  Memory mapping is especially useful\n            for accessing small fragments of large files without reading the\n            entire file into memory.\n        allow_pickle : bool, optional\n            Allow loading pickled object arrays stored in npy files. Reasons for\n            disallowing pickles include security, as loading pickled data can\n            execute arbitrary code. If pickles are disallowed, loading object\n            arrays will fail. Default: False\n        fix_imports : bool, optional\n            Only useful when loading Python 2 generated pickled files on Python 3,\n            which includes npy/npz files containing object arrays. If `fix_imports`\n            is True, pickle will try to map the old Python 2 names to the new names\n            used in Python 3.\n        encoding : str, optional\n            What encoding to use when reading Python 2 strings. Only useful when\n            loading Python 2 generated pickled files in Python 3, which includes\n            npy/npz files containing object arrays. Values other than &amp;#x27;latin1&amp;#x27;,\n            &amp;#x27;ASCII&amp;#x27;, and &amp;#x27;bytes&amp;#x27; are not allowed, as they can corrupt numerical\n            data. Default: &amp;#x27;ASCII&amp;#x27;\n        max_header_size : int, optional\n            Maximum allowed size of the header.  Large headers may not be safe\n            to load securely and thus require explicitly passing a larger value.\n            See :py:func:`ast.literal_eval()` for details.\n            This option is ignored when `allow_pickle` is passed.  In that case\n            the file is by definition trusted and the limit is unnecessary.\n    \n        Returns\n        -------\n        result : array, tuple, dict, etc.\n            Data stored in the file. For ``.npz`` files, the returned instance\n            of NpzFile class must be closed to avoid leaking file descriptors.\n    \n        Raises\n        ------\n        OSError\n            If the input file does not exist or cannot be read.\n        UnpicklingError\n            If ``allow_pickle=True``, but the file cannot be loaded as a pickle.\n        ValueError\n            The file contains an object array, but ``allow_pickle=False`` given.\n        EOFError\n            When calling ``np.load`` multiple times on the same file handle,\n            if all data has already been read\n    \n        See Also\n        --------\n        save, savez, savez_compressed, loadtxt\n        memmap : Create a memory-map to an array stored in a file on disk.\n        lib.format.open_memmap : Create or load a memory-mapped ``.npy`` file.\n    \n        Notes\n        -----\n        - If the file contains pickle data, then whatever object is stored\n          in the pickle is returned.\n        - If the file is a ``.npy`` file, then a single array is returned.\n        - If the file is a ``.npz`` file, then a dictionary-like object is\n          returned, containing ``{filename: array}`` key-value pairs, one for\n          each file in the archive.\n        - If the file is a ``.npz`` file, the returned value supports the\n          context manager protocol in a similar fashion to the open function::\n    \n            with load(&amp;#x27;foo.npz&amp;#x27;) as data:\n                a = data[&amp;#x27;a&amp;#x27;]\n    \n          The underlying file descriptor is closed when exiting the &amp;#x27;with&amp;#x27;\n          block.\n    \n        Examples\n        --------\n        &amp;gt;&amp;gt;&amp;gt; import numpy as np\n    \n        Store data to disk, and load it again:\n    \n        &amp;gt;&amp;gt;&amp;gt; np.save(&amp;#x27;/tmp/123&amp;#x27;, np.array([[1, 2, 3], [4, 5, 6]]))\n        &amp;gt;&amp;gt;&amp;gt; np.load(&amp;#x27;/tmp/123.npy&amp;#x27;)\n        array([[1, 2, 3],\n               [4, 5, 6]])\n    \n        Store compressed data to disk, and load it again:\n    \n        &amp;gt;&amp;gt;&amp;gt; a=np.array([[1, 2, 3], [4, 5, 6]])\n        &amp;gt;&amp;gt;&amp;gt; b=np.array([1, 2])\n        &amp;gt;&amp;gt;&amp;gt; np.savez(&amp;#x27;/tmp/123.npz&amp;#x27;, a=a, b=b)\n        &amp;gt;&amp;gt;&amp;gt; data = np.load(&amp;#x27;/tmp/123.npz&amp;#x27;)\n        &amp;gt;&amp;gt;&amp;gt; data[&amp;#x27;a&amp;#x27;]\n        array([[1, 2, 3],\n               [4, 5, 6]])\n        &amp;gt;&amp;gt;&amp;gt; data[&amp;#x27;b&amp;#x27;]\n        array([1, 2])\n        &amp;gt;&amp;gt;&amp;gt; data.close()\n    \n        Mem-map the stored array, and then access the second row\n        directly from disk:\n    \n        &amp;gt;&amp;gt;&amp;gt; X = np.load(&amp;#x27;/tmp/123.npy&amp;#x27;, mmap_mode=&amp;#x27;r&amp;#x27;)\n        &amp;gt;&amp;gt;&amp;gt; X[1, :]\n        memmap([4, 5, 6])\n    \n        &amp;quot;&amp;quot;&amp;quot;\n        if encoding not in (&amp;#x27;ASCII&amp;#x27;, &amp;#x27;latin1&amp;#x27;, &amp;#x27;bytes&amp;#x27;):\n            # The &amp;#x27;encoding&amp;#x27; value for pickle also affects what encoding\n            # the serialized binary data of NumPy arrays is loaded\n            # in. Pickle does not pass on the encoding information to\n            # NumPy. The unpickling code in numpy._core.multiarray is\n            # written to assume that unicode data appearing where binary\n            # should be is in &amp;#x27;latin1&amp;#x27;. &amp;#x27;bytes&amp;#x27; is also safe, as is &amp;#x27;ASCII&amp;#x27;.\n            #\n            # Other encoding values can corrupt binary data, and we\n            # purposefully disallow them. For the same reason, the errors=\n            # argument is not exposed, as values other than &amp;#x27;strict&amp;#x27;\n            # result can similarly silently corrupt numerical data.\n            raise ValueError(&amp;quot;encoding must be &amp;#x27;ASCII&amp;#x27;, &amp;#x27;latin1&amp;#x27;, or &amp;#x27;bytes&amp;#x27;&amp;quot;)\n    \n        pickle_kwargs = dict(encoding=encoding, fix_imports=fix_imports)\n    \n        with contextlib.ExitStack() as stack:\n            if hasattr(file, &amp;#x27;read&amp;#x27;):\n                fid = file\n                own_fid = False\n            else:\n&amp;gt;               fid = stack.enter_context(open(os.fspath(file), &amp;quot;rb&amp;quot;))\nE               FileNotFoundError: [Errno 2] No such file or directory: &amp;#x27;/test_data/epochs_overlap/sub-001_epoch-001.npy&amp;#x27;\n\n.tox/py312-test/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:451: FileNotFoundError\n&#34;}], &#34;tests/test_synchronization_metrics_extraction.py::test_connectivity_matrix_neg&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_synchronization_metrics_extraction.py::test_connectivity_matrix_neg&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_synchronization_metrics_extraction.py::test_connectivity_matrix_neg&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_connectivity_matrix_neg():\n        epoch_file = &amp;quot;/test_data/epochs_overlap/sub-001_epoch-001.npy&amp;quot;\n&amp;gt;       epoch = np.load(epoch_file)\n\ntests/test_synchronization_metrics_extraction.py:23: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nfile = &amp;#x27;/test_data/epochs_overlap/sub-001_epoch-001.npy&amp;#x27;, mmap_mode = None, allow_pickle = False, fix_imports = True, encoding = &amp;#x27;ASCII&amp;#x27;\n\n    @set_module(&amp;#x27;numpy&amp;#x27;)\n    def load(file, mmap_mode=None, allow_pickle=False, fix_imports=True,\n             encoding=&amp;#x27;ASCII&amp;#x27;, *, max_header_size=format._MAX_HEADER_SIZE):\n        &amp;quot;&amp;quot;&amp;quot;\n        Load arrays or pickled objects from ``.npy``, ``.npz`` or pickled files.\n    \n        .. warning:: Loading files that contain object arrays uses the ``pickle``\n                     module, which is not secure against erroneous or maliciously\n                     constructed data. Consider passing ``allow_pickle=False`` to\n                     load data that is known not to contain object arrays for the\n                     safer handling of untrusted sources.\n    \n        Parameters\n        ----------\n        file : file-like object, string, or pathlib.Path\n            The file to read. File-like objects must support the\n            ``seek()`` and ``read()`` methods and must always\n            be opened in binary mode.  Pickled files require that the\n            file-like object support the ``readline()`` method as well.\n        mmap_mode : {None, &amp;#x27;r+&amp;#x27;, &amp;#x27;r&amp;#x27;, &amp;#x27;w+&amp;#x27;, &amp;#x27;c&amp;#x27;}, optional\n            If not None, then memory-map the file, using the given mode (see\n            `numpy.memmap` for a detailed description of the modes).  A\n            memory-mapped array is kept on disk. However, it can be accessed\n            and sliced like any ndarray.  Memory mapping is especially useful\n            for accessing small fragments of large files without reading the\n            entire file into memory.\n        allow_pickle : bool, optional\n            Allow loading pickled object arrays stored in npy files. Reasons for\n            disallowing pickles include security, as loading pickled data can\n            execute arbitrary code. If pickles are disallowed, loading object\n            arrays will fail. Default: False\n        fix_imports : bool, optional\n            Only useful when loading Python 2 generated pickled files on Python 3,\n            which includes npy/npz files containing object arrays. If `fix_imports`\n            is True, pickle will try to map the old Python 2 names to the new names\n            used in Python 3.\n        encoding : str, optional\n            What encoding to use when reading Python 2 strings. Only useful when\n            loading Python 2 generated pickled files in Python 3, which includes\n            npy/npz files containing object arrays. Values other than &amp;#x27;latin1&amp;#x27;,\n            &amp;#x27;ASCII&amp;#x27;, and &amp;#x27;bytes&amp;#x27; are not allowed, as they can corrupt numerical\n            data. Default: &amp;#x27;ASCII&amp;#x27;\n        max_header_size : int, optional\n            Maximum allowed size of the header.  Large headers may not be safe\n            to load securely and thus require explicitly passing a larger value.\n            See :py:func:`ast.literal_eval()` for details.\n            This option is ignored when `allow_pickle` is passed.  In that case\n            the file is by definition trusted and the limit is unnecessary.\n    \n        Returns\n        -------\n        result : array, tuple, dict, etc.\n            Data stored in the file. For ``.npz`` files, the returned instance\n            of NpzFile class must be closed to avoid leaking file descriptors.\n    \n        Raises\n        ------\n        OSError\n            If the input file does not exist or cannot be read.\n        UnpicklingError\n            If ``allow_pickle=True``, but the file cannot be loaded as a pickle.\n        ValueError\n            The file contains an object array, but ``allow_pickle=False`` given.\n        EOFError\n            When calling ``np.load`` multiple times on the same file handle,\n            if all data has already been read\n    \n        See Also\n        --------\n        save, savez, savez_compressed, loadtxt\n        memmap : Create a memory-map to an array stored in a file on disk.\n        lib.format.open_memmap : Create or load a memory-mapped ``.npy`` file.\n    \n        Notes\n        -----\n        - If the file contains pickle data, then whatever object is stored\n          in the pickle is returned.\n        - If the file is a ``.npy`` file, then a single array is returned.\n        - If the file is a ``.npz`` file, then a dictionary-like object is\n          returned, containing ``{filename: array}`` key-value pairs, one for\n          each file in the archive.\n        - If the file is a ``.npz`` file, the returned value supports the\n          context manager protocol in a similar fashion to the open function::\n    \n            with load(&amp;#x27;foo.npz&amp;#x27;) as data:\n                a = data[&amp;#x27;a&amp;#x27;]\n    \n          The underlying file descriptor is closed when exiting the &amp;#x27;with&amp;#x27;\n          block.\n    \n        Examples\n        --------\n        &amp;gt;&amp;gt;&amp;gt; import numpy as np\n    \n        Store data to disk, and load it again:\n    \n        &amp;gt;&amp;gt;&amp;gt; np.save(&amp;#x27;/tmp/123&amp;#x27;, np.array([[1, 2, 3], [4, 5, 6]]))\n        &amp;gt;&amp;gt;&amp;gt; np.load(&amp;#x27;/tmp/123.npy&amp;#x27;)\n        array([[1, 2, 3],\n               [4, 5, 6]])\n    \n        Store compressed data to disk, and load it again:\n    \n        &amp;gt;&amp;gt;&amp;gt; a=np.array([[1, 2, 3], [4, 5, 6]])\n        &amp;gt;&amp;gt;&amp;gt; b=np.array([1, 2])\n        &amp;gt;&amp;gt;&amp;gt; np.savez(&amp;#x27;/tmp/123.npz&amp;#x27;, a=a, b=b)\n        &amp;gt;&amp;gt;&amp;gt; data = np.load(&amp;#x27;/tmp/123.npz&amp;#x27;)\n        &amp;gt;&amp;gt;&amp;gt; data[&amp;#x27;a&amp;#x27;]\n        array([[1, 2, 3],\n               [4, 5, 6]])\n        &amp;gt;&amp;gt;&amp;gt; data[&amp;#x27;b&amp;#x27;]\n        array([1, 2])\n        &amp;gt;&amp;gt;&amp;gt; data.close()\n    \n        Mem-map the stored array, and then access the second row\n        directly from disk:\n    \n        &amp;gt;&amp;gt;&amp;gt; X = np.load(&amp;#x27;/tmp/123.npy&amp;#x27;, mmap_mode=&amp;#x27;r&amp;#x27;)\n        &amp;gt;&amp;gt;&amp;gt; X[1, :]\n        memmap([4, 5, 6])\n    \n        &amp;quot;&amp;quot;&amp;quot;\n        if encoding not in (&amp;#x27;ASCII&amp;#x27;, &amp;#x27;latin1&amp;#x27;, &amp;#x27;bytes&amp;#x27;):\n            # The &amp;#x27;encoding&amp;#x27; value for pickle also affects what encoding\n            # the serialized binary data of NumPy arrays is loaded\n            # in. Pickle does not pass on the encoding information to\n            # NumPy. The unpickling code in numpy._core.multiarray is\n            # written to assume that unicode data appearing where binary\n            # should be is in &amp;#x27;latin1&amp;#x27;. &amp;#x27;bytes&amp;#x27; is also safe, as is &amp;#x27;ASCII&amp;#x27;.\n            #\n            # Other encoding values can corrupt binary data, and we\n            # purposefully disallow them. For the same reason, the errors=\n            # argument is not exposed, as values other than &amp;#x27;strict&amp;#x27;\n            # result can similarly silently corrupt numerical data.\n            raise ValueError(&amp;quot;encoding must be &amp;#x27;ASCII&amp;#x27;, &amp;#x27;latin1&amp;#x27;, or &amp;#x27;bytes&amp;#x27;&amp;quot;)\n    \n        pickle_kwargs = dict(encoding=encoding, fix_imports=fix_imports)\n    \n        with contextlib.ExitStack() as stack:\n            if hasattr(file, &amp;#x27;read&amp;#x27;):\n                fid = file\n                own_fid = False\n            else:\n&amp;gt;               fid = stack.enter_context(open(os.fspath(file), &amp;quot;rb&amp;quot;))\nE               FileNotFoundError: [Errno 2] No such file or directory: &amp;#x27;/test_data/epochs_overlap/sub-001_epoch-001.npy&amp;#x27;\n\n.tox/py312-test/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:451: FileNotFoundError\n&#34;}], &#34;tests/test_synchronization_metrics_extraction.py::test_threshold_matrix_pos&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_synchronization_metrics_extraction.py::test_threshold_matrix_pos&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_synchronization_metrics_extraction.py::test_threshold_matrix_pos&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_threshold_matrix_pos():\n&amp;gt;       epoch = np.load(&amp;quot;/test_data/epochs_overlap/sub-001_epoch-001.npy&amp;quot;)\n\ntests/test_synchronization_metrics_extraction.py:29: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nfile = &amp;#x27;/test_data/epochs_overlap/sub-001_epoch-001.npy&amp;#x27;, mmap_mode = None, allow_pickle = False, fix_imports = True, encoding = &amp;#x27;ASCII&amp;#x27;\n\n    @set_module(&amp;#x27;numpy&amp;#x27;)\n    def load(file, mmap_mode=None, allow_pickle=False, fix_imports=True,\n             encoding=&amp;#x27;ASCII&amp;#x27;, *, max_header_size=format._MAX_HEADER_SIZE):\n        &amp;quot;&amp;quot;&amp;quot;\n        Load arrays or pickled objects from ``.npy``, ``.npz`` or pickled files.\n    \n        .. warning:: Loading files that contain object arrays uses the ``pickle``\n                     module, which is not secure against erroneous or maliciously\n                     constructed data. Consider passing ``allow_pickle=False`` to\n                     load data that is known not to contain object arrays for the\n                     safer handling of untrusted sources.\n    \n        Parameters\n        ----------\n        file : file-like object, string, or pathlib.Path\n            The file to read. File-like objects must support the\n            ``seek()`` and ``read()`` methods and must always\n            be opened in binary mode.  Pickled files require that the\n            file-like object support the ``readline()`` method as well.\n        mmap_mode : {None, &amp;#x27;r+&amp;#x27;, &amp;#x27;r&amp;#x27;, &amp;#x27;w+&amp;#x27;, &amp;#x27;c&amp;#x27;}, optional\n            If not None, then memory-map the file, using the given mode (see\n            `numpy.memmap` for a detailed description of the modes).  A\n            memory-mapped array is kept on disk. However, it can be accessed\n            and sliced like any ndarray.  Memory mapping is especially useful\n            for accessing small fragments of large files without reading the\n            entire file into memory.\n        allow_pickle : bool, optional\n            Allow loading pickled object arrays stored in npy files. Reasons for\n            disallowing pickles include security, as loading pickled data can\n            execute arbitrary code. If pickles are disallowed, loading object\n            arrays will fail. Default: False\n        fix_imports : bool, optional\n            Only useful when loading Python 2 generated pickled files on Python 3,\n            which includes npy/npz files containing object arrays. If `fix_imports`\n            is True, pickle will try to map the old Python 2 names to the new names\n            used in Python 3.\n        encoding : str, optional\n            What encoding to use when reading Python 2 strings. Only useful when\n            loading Python 2 generated pickled files in Python 3, which includes\n            npy/npz files containing object arrays. Values other than &amp;#x27;latin1&amp;#x27;,\n            &amp;#x27;ASCII&amp;#x27;, and &amp;#x27;bytes&amp;#x27; are not allowed, as they can corrupt numerical\n            data. Default: &amp;#x27;ASCII&amp;#x27;\n        max_header_size : int, optional\n            Maximum allowed size of the header.  Large headers may not be safe\n            to load securely and thus require explicitly passing a larger value.\n            See :py:func:`ast.literal_eval()` for details.\n            This option is ignored when `allow_pickle` is passed.  In that case\n            the file is by definition trusted and the limit is unnecessary.\n    \n        Returns\n        -------\n        result : array, tuple, dict, etc.\n            Data stored in the file. For ``.npz`` files, the returned instance\n            of NpzFile class must be closed to avoid leaking file descriptors.\n    \n        Raises\n        ------\n        OSError\n            If the input file does not exist or cannot be read.\n        UnpicklingError\n            If ``allow_pickle=True``, but the file cannot be loaded as a pickle.\n        ValueError\n            The file contains an object array, but ``allow_pickle=False`` given.\n        EOFError\n            When calling ``np.load`` multiple times on the same file handle,\n            if all data has already been read\n    \n        See Also\n        --------\n        save, savez, savez_compressed, loadtxt\n        memmap : Create a memory-map to an array stored in a file on disk.\n        lib.format.open_memmap : Create or load a memory-mapped ``.npy`` file.\n    \n        Notes\n        -----\n        - If the file contains pickle data, then whatever object is stored\n          in the pickle is returned.\n        - If the file is a ``.npy`` file, then a single array is returned.\n        - If the file is a ``.npz`` file, then a dictionary-like object is\n          returned, containing ``{filename: array}`` key-value pairs, one for\n          each file in the archive.\n        - If the file is a ``.npz`` file, the returned value supports the\n          context manager protocol in a similar fashion to the open function::\n    \n            with load(&amp;#x27;foo.npz&amp;#x27;) as data:\n                a = data[&amp;#x27;a&amp;#x27;]\n    \n          The underlying file descriptor is closed when exiting the &amp;#x27;with&amp;#x27;\n          block.\n    \n        Examples\n        --------\n        &amp;gt;&amp;gt;&amp;gt; import numpy as np\n    \n        Store data to disk, and load it again:\n    \n        &amp;gt;&amp;gt;&amp;gt; np.save(&amp;#x27;/tmp/123&amp;#x27;, np.array([[1, 2, 3], [4, 5, 6]]))\n        &amp;gt;&amp;gt;&amp;gt; np.load(&amp;#x27;/tmp/123.npy&amp;#x27;)\n        array([[1, 2, 3],\n               [4, 5, 6]])\n    \n        Store compressed data to disk, and load it again:\n    \n        &amp;gt;&amp;gt;&amp;gt; a=np.array([[1, 2, 3], [4, 5, 6]])\n        &amp;gt;&amp;gt;&amp;gt; b=np.array([1, 2])\n        &amp;gt;&amp;gt;&amp;gt; np.savez(&amp;#x27;/tmp/123.npz&amp;#x27;, a=a, b=b)\n        &amp;gt;&amp;gt;&amp;gt; data = np.load(&amp;#x27;/tmp/123.npz&amp;#x27;)\n        &amp;gt;&amp;gt;&amp;gt; data[&amp;#x27;a&amp;#x27;]\n        array([[1, 2, 3],\n               [4, 5, 6]])\n        &amp;gt;&amp;gt;&amp;gt; data[&amp;#x27;b&amp;#x27;]\n        array([1, 2])\n        &amp;gt;&amp;gt;&amp;gt; data.close()\n    \n        Mem-map the stored array, and then access the second row\n        directly from disk:\n    \n        &amp;gt;&amp;gt;&amp;gt; X = np.load(&amp;#x27;/tmp/123.npy&amp;#x27;, mmap_mode=&amp;#x27;r&amp;#x27;)\n        &amp;gt;&amp;gt;&amp;gt; X[1, :]\n        memmap([4, 5, 6])\n    \n        &amp;quot;&amp;quot;&amp;quot;\n        if encoding not in (&amp;#x27;ASCII&amp;#x27;, &amp;#x27;latin1&amp;#x27;, &amp;#x27;bytes&amp;#x27;):\n            # The &amp;#x27;encoding&amp;#x27; value for pickle also affects what encoding\n            # the serialized binary data of NumPy arrays is loaded\n            # in. Pickle does not pass on the encoding information to\n            # NumPy. The unpickling code in numpy._core.multiarray is\n            # written to assume that unicode data appearing where binary\n            # should be is in &amp;#x27;latin1&amp;#x27;. &amp;#x27;bytes&amp;#x27; is also safe, as is &amp;#x27;ASCII&amp;#x27;.\n            #\n            # Other encoding values can corrupt binary data, and we\n            # purposefully disallow them. For the same reason, the errors=\n            # argument is not exposed, as values other than &amp;#x27;strict&amp;#x27;\n            # result can similarly silently corrupt numerical data.\n            raise ValueError(&amp;quot;encoding must be &amp;#x27;ASCII&amp;#x27;, &amp;#x27;latin1&amp;#x27;, or &amp;#x27;bytes&amp;#x27;&amp;quot;)\n    \n        pickle_kwargs = dict(encoding=encoding, fix_imports=fix_imports)\n    \n        with contextlib.ExitStack() as stack:\n            if hasattr(file, &amp;#x27;read&amp;#x27;):\n                fid = file\n                own_fid = False\n            else:\n&amp;gt;               fid = stack.enter_context(open(os.fspath(file), &amp;quot;rb&amp;quot;))\nE               FileNotFoundError: [Errno 2] No such file or directory: &amp;#x27;/test_data/epochs_overlap/sub-001_epoch-001.npy&amp;#x27;\n\n.tox/py312-test/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:451: FileNotFoundError\n&#34;}], &#34;tests/test_synchronization_metrics_extraction.py::test_threshold_matrix_neg&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_synchronization_metrics_extraction.py::test_threshold_matrix_neg&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_synchronization_metrics_extraction.py::test_threshold_matrix_neg&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_threshold_matrix_neg():\n&amp;gt;       epoch = np.load(&amp;quot;/test_data/epochs_overlap/sub-non-existent-path.npy&amp;quot;)\n\ntests/test_synchronization_metrics_extraction.py:40: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nfile = &amp;#x27;/test_data/epochs_overlap/sub-non-existent-path.npy&amp;#x27;, mmap_mode = None, allow_pickle = False, fix_imports = True, encoding = &amp;#x27;ASCII&amp;#x27;\n\n    @set_module(&amp;#x27;numpy&amp;#x27;)\n    def load(file, mmap_mode=None, allow_pickle=False, fix_imports=True,\n             encoding=&amp;#x27;ASCII&amp;#x27;, *, max_header_size=format._MAX_HEADER_SIZE):\n        &amp;quot;&amp;quot;&amp;quot;\n        Load arrays or pickled objects from ``.npy``, ``.npz`` or pickled files.\n    \n        .. warning:: Loading files that contain object arrays uses the ``pickle``\n                     module, which is not secure against erroneous or maliciously\n                     constructed data. Consider passing ``allow_pickle=False`` to\n                     load data that is known not to contain object arrays for the\n                     safer handling of untrusted sources.\n    \n        Parameters\n        ----------\n        file : file-like object, string, or pathlib.Path\n            The file to read. File-like objects must support the\n            ``seek()`` and ``read()`` methods and must always\n            be opened in binary mode.  Pickled files require that the\n            file-like object support the ``readline()`` method as well.\n        mmap_mode : {None, &amp;#x27;r+&amp;#x27;, &amp;#x27;r&amp;#x27;, &amp;#x27;w+&amp;#x27;, &amp;#x27;c&amp;#x27;}, optional\n            If not None, then memory-map the file, using the given mode (see\n            `numpy.memmap` for a detailed description of the modes).  A\n            memory-mapped array is kept on disk. However, it can be accessed\n            and sliced like any ndarray.  Memory mapping is especially useful\n            for accessing small fragments of large files without reading the\n            entire file into memory.\n        allow_pickle : bool, optional\n            Allow loading pickled object arrays stored in npy files. Reasons for\n            disallowing pickles include security, as loading pickled data can\n            execute arbitrary code. If pickles are disallowed, loading object\n            arrays will fail. Default: False\n        fix_imports : bool, optional\n            Only useful when loading Python 2 generated pickled files on Python 3,\n            which includes npy/npz files containing object arrays. If `fix_imports`\n            is True, pickle will try to map the old Python 2 names to the new names\n            used in Python 3.\n        encoding : str, optional\n            What encoding to use when reading Python 2 strings. Only useful when\n            loading Python 2 generated pickled files in Python 3, which includes\n            npy/npz files containing object arrays. Values other than &amp;#x27;latin1&amp;#x27;,\n            &amp;#x27;ASCII&amp;#x27;, and &amp;#x27;bytes&amp;#x27; are not allowed, as they can corrupt numerical\n            data. Default: &amp;#x27;ASCII&amp;#x27;\n        max_header_size : int, optional\n            Maximum allowed size of the header.  Large headers may not be safe\n            to load securely and thus require explicitly passing a larger value.\n            See :py:func:`ast.literal_eval()` for details.\n            This option is ignored when `allow_pickle` is passed.  In that case\n            the file is by definition trusted and the limit is unnecessary.\n    \n        Returns\n        -------\n        result : array, tuple, dict, etc.\n            Data stored in the file. For ``.npz`` files, the returned instance\n            of NpzFile class must be closed to avoid leaking file descriptors.\n    \n        Raises\n        ------\n        OSError\n            If the input file does not exist or cannot be read.\n        UnpicklingError\n            If ``allow_pickle=True``, but the file cannot be loaded as a pickle.\n        ValueError\n            The file contains an object array, but ``allow_pickle=False`` given.\n        EOFError\n            When calling ``np.load`` multiple times on the same file handle,\n            if all data has already been read\n    \n        See Also\n        --------\n        save, savez, savez_compressed, loadtxt\n        memmap : Create a memory-map to an array stored in a file on disk.\n        lib.format.open_memmap : Create or load a memory-mapped ``.npy`` file.\n    \n        Notes\n        -----\n        - If the file contains pickle data, then whatever object is stored\n          in the pickle is returned.\n        - If the file is a ``.npy`` file, then a single array is returned.\n        - If the file is a ``.npz`` file, then a dictionary-like object is\n          returned, containing ``{filename: array}`` key-value pairs, one for\n          each file in the archive.\n        - If the file is a ``.npz`` file, the returned value supports the\n          context manager protocol in a similar fashion to the open function::\n    \n            with load(&amp;#x27;foo.npz&amp;#x27;) as data:\n                a = data[&amp;#x27;a&amp;#x27;]\n    \n          The underlying file descriptor is closed when exiting the &amp;#x27;with&amp;#x27;\n          block.\n    \n        Examples\n        --------\n        &amp;gt;&amp;gt;&amp;gt; import numpy as np\n    \n        Store data to disk, and load it again:\n    \n        &amp;gt;&amp;gt;&amp;gt; np.save(&amp;#x27;/tmp/123&amp;#x27;, np.array([[1, 2, 3], [4, 5, 6]]))\n        &amp;gt;&amp;gt;&amp;gt; np.load(&amp;#x27;/tmp/123.npy&amp;#x27;)\n        array([[1, 2, 3],\n               [4, 5, 6]])\n    \n        Store compressed data to disk, and load it again:\n    \n        &amp;gt;&amp;gt;&amp;gt; a=np.array([[1, 2, 3], [4, 5, 6]])\n        &amp;gt;&amp;gt;&amp;gt; b=np.array([1, 2])\n        &amp;gt;&amp;gt;&amp;gt; np.savez(&amp;#x27;/tmp/123.npz&amp;#x27;, a=a, b=b)\n        &amp;gt;&amp;gt;&amp;gt; data = np.load(&amp;#x27;/tmp/123.npz&amp;#x27;)\n        &amp;gt;&amp;gt;&amp;gt; data[&amp;#x27;a&amp;#x27;]\n        array([[1, 2, 3],\n               [4, 5, 6]])\n        &amp;gt;&amp;gt;&amp;gt; data[&amp;#x27;b&amp;#x27;]\n        array([1, 2])\n        &amp;gt;&amp;gt;&amp;gt; data.close()\n    \n        Mem-map the stored array, and then access the second row\n        directly from disk:\n    \n        &amp;gt;&amp;gt;&amp;gt; X = np.load(&amp;#x27;/tmp/123.npy&amp;#x27;, mmap_mode=&amp;#x27;r&amp;#x27;)\n        &amp;gt;&amp;gt;&amp;gt; X[1, :]\n        memmap([4, 5, 6])\n    \n        &amp;quot;&amp;quot;&amp;quot;\n        if encoding not in (&amp;#x27;ASCII&amp;#x27;, &amp;#x27;latin1&amp;#x27;, &amp;#x27;bytes&amp;#x27;):\n            # The &amp;#x27;encoding&amp;#x27; value for pickle also affects what encoding\n            # the serialized binary data of NumPy arrays is loaded\n            # in. Pickle does not pass on the encoding information to\n            # NumPy. The unpickling code in numpy._core.multiarray is\n            # written to assume that unicode data appearing where binary\n            # should be is in &amp;#x27;latin1&amp;#x27;. &amp;#x27;bytes&amp;#x27; is also safe, as is &amp;#x27;ASCII&amp;#x27;.\n            #\n            # Other encoding values can corrupt binary data, and we\n            # purposefully disallow them. For the same reason, the errors=\n            # argument is not exposed, as values other than &amp;#x27;strict&amp;#x27;\n            # result can similarly silently corrupt numerical data.\n            raise ValueError(&amp;quot;encoding must be &amp;#x27;ASCII&amp;#x27;, &amp;#x27;latin1&amp;#x27;, or &amp;#x27;bytes&amp;#x27;&amp;quot;)\n    \n        pickle_kwargs = dict(encoding=encoding, fix_imports=fix_imports)\n    \n        with contextlib.ExitStack() as stack:\n            if hasattr(file, &amp;#x27;read&amp;#x27;):\n                fid = file\n                own_fid = False\n            else:\n&amp;gt;               fid = stack.enter_context(open(os.fspath(file), &amp;quot;rb&amp;quot;))\nE               FileNotFoundError: [Errno 2] No such file or directory: &amp;#x27;/test_data/epochs_overlap/sub-non-existent-path.npy&amp;#x27;\n\n.tox/py312-test/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:451: FileNotFoundError\n&#34;}], &#34;tests/test_synchronization_metrics_extraction.py::test_process_and_save_sync&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_synchronization_metrics_extraction.py::test_process_and_save_sync&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_synchronization_metrics_extraction.py::test_process_and_save_sync&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_process_and_save_sync():\n         graph_metrics_csv = &amp;quot;test_data/synchronization.csv&amp;quot;\n         synchronization_method = &amp;quot;pearson&amp;quot;\n         threshold_value = 0.6\n         output_folder = &amp;quot;test_data/epochs_overlap&amp;quot;\n&amp;gt;        files_to_process, processed_set = checkpoint(graph_metrics_csv, output_folder)\n\ntests/test_synchronization_metrics_extraction.py:55: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ncsv_file = &amp;#x27;test_data/synchronization.csv&amp;#x27;, output_folder = &amp;#x27;test_data/epochs_overlap&amp;#x27;\n\n    def checkpoint(csv_file, output_folder):\n        &amp;quot;&amp;quot;&amp;quot;\n        Checkpoint the processed data to disk.\n        &amp;quot;&amp;quot;&amp;quot;\n        # Precompute processed files upfront\n        if os.path.exists(csv_file) and os.path.getsize(csv_file) &amp;gt; 0:\n            existing_df = pd.read_csv(csv_file)[[&amp;#x27;subject_id&amp;#x27;, &amp;#x27;epoch_number&amp;#x27;]]\n            processed_set = set(\n                (row[&amp;#x27;subject_id&amp;#x27;], row[&amp;#x27;epoch_number&amp;#x27;]) for _, row in existing_df.iterrows())\n        else:\n            processed_set = set()\n    \n        # List files to process\n        files_to_process = [\n&amp;gt;           f for f in os.listdir(output_folder) if f.endswith(&amp;#x27;.npy&amp;#x27;)\n        ]\nE       FileNotFoundError: [Errno 2] No such file or directory: &amp;#x27;test_data/epochs_overlap&amp;#x27;\n\nsrc/utilities.py:18: FileNotFoundError\n&#34;}]}, &#34;renderCollapsed&#34;: [&#34;passed&#34;], &#34;initialSort&#34;: &#34;result&#34;, &#34;title&#34;: &#34;report.html&#34;}"></div>
    <script>
      (function(){function r(e,n,t){function o(i,f){if(!n[i]){if(!e[i]){var c="function"==typeof require&&require;if(!f&&c)return c(i,!0);if(u)return u(i,!0);var a=new Error("Cannot find module '"+i+"'");throw a.code="MODULE_NOT_FOUND",a}var p=n[i]={exports:{}};e[i][0].call(p.exports,function(r){var n=e[i][1][r];return o(n||r)},p,p.exports,r,e,n,t)}return n[i].exports}for(var u="function"==typeof require&&require,i=0;i<t.length;i++)o(t[i]);return o}return r})()({1:[function(require,module,exports){
const { getCollapsedCategory, setCollapsedIds } = require('./storage.js')

class DataManager {
    setManager(data) {
        const collapsedCategories = [...getCollapsedCategory(data.renderCollapsed)]
        const collapsedIds = []
        const tests = Object.values(data.tests).flat().map((test, index) => {
            const collapsed = collapsedCategories.includes(test.result.toLowerCase())
            const id = `test_${index}`
            if (collapsed) {
                collapsedIds.push(id)
            }
            return {
                ...test,
                id,
                collapsed,
            }
        })
        const dataBlob = { ...data, tests }
        this.data = { ...dataBlob }
        this.renderData = { ...dataBlob }
        setCollapsedIds(collapsedIds)
    }

    get allData() {
        return { ...this.data }
    }

    resetRender() {
        this.renderData = { ...this.data }
    }

    setRender(data) {
        this.renderData.tests = [...data]
    }

    toggleCollapsedItem(id) {
        this.renderData.tests = this.renderData.tests.map((test) =>
            test.id === id ? { ...test, collapsed: !test.collapsed } : test,
        )
    }

    set allCollapsed(collapsed) {
        this.renderData = { ...this.renderData, tests: [...this.renderData.tests.map((test) => (
            { ...test, collapsed }
        ))] }
    }

    get testSubset() {
        return [...this.renderData.tests]
    }

    get environment() {
        return this.renderData.environment
    }

    get initialSort() {
        return this.data.initialSort
    }
}

module.exports = {
    manager: new DataManager(),
}

},{"./storage.js":8}],2:[function(require,module,exports){
const mediaViewer = require('./mediaviewer.js')
const templateEnvRow = document.getElementById('template_environment_row')
const templateResult = document.getElementById('template_results-table__tbody')

function htmlToElements(html) {
    const temp = document.createElement('template')
    temp.innerHTML = html
    return temp.content.childNodes
}

const find = (selector, elem) => {
    if (!elem) {
        elem = document
    }
    return elem.querySelector(selector)
}

const findAll = (selector, elem) => {
    if (!elem) {
        elem = document
    }
    return [...elem.querySelectorAll(selector)]
}

const dom = {
    getStaticRow: (key, value) => {
        const envRow = templateEnvRow.content.cloneNode(true)
        const isObj = typeof value === 'object' && value !== null
        const values = isObj ? Object.keys(value).map((k) => `${k}: ${value[k]}`) : null

        const valuesElement = htmlToElements(
            values ? `<ul>${values.map((val) => `<li>${val}</li>`).join('')}<ul>` : `<div>${value}</div>`)[0]
        const td = findAll('td', envRow)
        td[0].textContent = key
        td[1].appendChild(valuesElement)

        return envRow
    },
    getResultTBody: ({ testId, id, log, extras, resultsTableRow, tableHtml, result, collapsed }) => {
        const resultBody = templateResult.content.cloneNode(true)
        resultBody.querySelector('tbody').classList.add(result.toLowerCase())
        resultBody.querySelector('tbody').id = testId
        resultBody.querySelector('.collapsible').dataset.id = id

        resultsTableRow.forEach((html) => {
            const t = document.createElement('template')
            t.innerHTML = html
            resultBody.querySelector('.collapsible').appendChild(t.content)
        })

        if (log) {
            // Wrap lines starting with "E" with span.error to color those lines red
            const wrappedLog = log.replace(/^E.*$/gm, (match) => `<span class="error">${match}</span>`)
            resultBody.querySelector('.log').innerHTML = wrappedLog
        } else {
            resultBody.querySelector('.log').remove()
        }

        if (collapsed) {
            resultBody.querySelector('.collapsible > td')?.classList.add('collapsed')
            resultBody.querySelector('.extras-row').classList.add('hidden')
        } else {
            resultBody.querySelector('.collapsible > td')?.classList.remove('collapsed')
        }

        const media = []
        extras?.forEach(({ name, format_type, content }) => {
            if (['image', 'video'].includes(format_type)) {
                media.push({ path: content, name, format_type })
            }

            if (format_type === 'html') {
                resultBody.querySelector('.extraHTML').insertAdjacentHTML('beforeend', `<div>${content}</div>`)
            }
        })
        mediaViewer.setup(resultBody, media)

        // Add custom html from the pytest_html_results_table_html hook
        tableHtml?.forEach((item) => {
            resultBody.querySelector('td[class="extra"]').insertAdjacentHTML('beforeend', item)
        })

        return resultBody
    },
}

module.exports = {
    dom,
    htmlToElements,
    find,
    findAll,
}

},{"./mediaviewer.js":6}],3:[function(require,module,exports){
const { manager } = require('./datamanager.js')
const { doSort } = require('./sort.js')
const storageModule = require('./storage.js')

const getFilteredSubSet = (filter) =>
    manager.allData.tests.filter(({ result }) => filter.includes(result.toLowerCase()))

const doInitFilter = () => {
    const currentFilter = storageModule.getVisible()
    const filteredSubset = getFilteredSubSet(currentFilter)
    manager.setRender(filteredSubset)
}

const doFilter = (type, show) => {
    if (show) {
        storageModule.showCategory(type)
    } else {
        storageModule.hideCategory(type)
    }

    const currentFilter = storageModule.getVisible()
    const filteredSubset = getFilteredSubSet(currentFilter)
    manager.setRender(filteredSubset)

    const sortColumn = storageModule.getSort()
    doSort(sortColumn, true)
}

module.exports = {
    doFilter,
    doInitFilter,
}

},{"./datamanager.js":1,"./sort.js":7,"./storage.js":8}],4:[function(require,module,exports){
const { redraw, bindEvents, renderStatic } = require('./main.js')
const { doInitFilter } = require('./filter.js')
const { doInitSort } = require('./sort.js')
const { manager } = require('./datamanager.js')
const data = JSON.parse(document.getElementById('data-container').dataset.jsonblob)

function init() {
    manager.setManager(data)
    doInitFilter()
    doInitSort()
    renderStatic()
    redraw()
    bindEvents()
}

init()

},{"./datamanager.js":1,"./filter.js":3,"./main.js":5,"./sort.js":7}],5:[function(require,module,exports){
const { dom, find, findAll } = require('./dom.js')
const { manager } = require('./datamanager.js')
const { doSort } = require('./sort.js')
const { doFilter } = require('./filter.js')
const {
    getVisible,
    getCollapsedIds,
    setCollapsedIds,
    getSort,
    getSortDirection,
    possibleFilters,
} = require('./storage.js')

const removeChildren = (node) => {
    while (node.firstChild) {
        node.removeChild(node.firstChild)
    }
}

const renderStatic = () => {
    const renderEnvironmentTable = () => {
        const environment = manager.environment
        const rows = Object.keys(environment).map((key) => dom.getStaticRow(key, environment[key]))
        const table = document.getElementById('environment')
        removeChildren(table)
        rows.forEach((row) => table.appendChild(row))
    }
    renderEnvironmentTable()
}

const addItemToggleListener = (elem) => {
    elem.addEventListener('click', ({ target }) => {
        const id = target.parentElement.dataset.id
        manager.toggleCollapsedItem(id)

        const collapsedIds = getCollapsedIds()
        if (collapsedIds.includes(id)) {
            const updated = collapsedIds.filter((item) => item !== id)
            setCollapsedIds(updated)
        } else {
            collapsedIds.push(id)
            setCollapsedIds(collapsedIds)
        }
        redraw()
    })
}

const renderContent = (tests) => {
    const sortAttr = getSort(manager.initialSort)
    const sortAsc = JSON.parse(getSortDirection())
    const rows = tests.map(dom.getResultTBody)
    const table = document.getElementById('results-table')
    const tableHeader = document.getElementById('results-table-head')

    const newTable = document.createElement('table')
    newTable.id = 'results-table'

    // remove all sorting classes and set the relevant
    findAll('.sortable', tableHeader).forEach((elem) => elem.classList.remove('asc', 'desc'))
    tableHeader.querySelector(`.sortable[data-column-type="${sortAttr}"]`)?.classList.add(sortAsc ? 'desc' : 'asc')
    newTable.appendChild(tableHeader)

    if (!rows.length) {
        const emptyTable = document.getElementById('template_results-table__body--empty').content.cloneNode(true)
        newTable.appendChild(emptyTable)
    } else {
        rows.forEach((row) => {
            if (!!row) {
                findAll('.collapsible td:not(.col-links', row).forEach(addItemToggleListener)
                find('.logexpander', row).addEventListener('click',
                    (evt) => evt.target.parentNode.classList.toggle('expanded'),
                )
                newTable.appendChild(row)
            }
        })
    }

    table.replaceWith(newTable)
}

const renderDerived = () => {
    const currentFilter = getVisible()
    possibleFilters.forEach((result) => {
        const input = document.querySelector(`input[data-test-result="${result}"]`)
        input.checked = currentFilter.includes(result)
    })
}

const bindEvents = () => {
    const filterColumn = (evt) => {
        const { target: element } = evt
        const { testResult } = element.dataset

        doFilter(testResult, element.checked)
        const collapsedIds = getCollapsedIds()
        const updated = manager.renderData.tests.map((test) => {
            return {
                ...test,
                collapsed: collapsedIds.includes(test.id),
            }
        })
        manager.setRender(updated)
        redraw()
    }

    const header = document.getElementById('environment-header')
    header.addEventListener('click', () => {
        const table = document.getElementById('environment')
        table.classList.toggle('hidden')
        header.classList.toggle('collapsed')
    })

    findAll('input[name="filter_checkbox"]').forEach((elem) => {
        elem.addEventListener('click', filterColumn)
    })

    findAll('.sortable').forEach((elem) => {
        elem.addEventListener('click', (evt) => {
            const { target: element } = evt
            const { columnType } = element.dataset
            doSort(columnType)
            redraw()
        })
    })

    document.getElementById('show_all_details').addEventListener('click', () => {
        manager.allCollapsed = false
        setCollapsedIds([])
        redraw()
    })
    document.getElementById('hide_all_details').addEventListener('click', () => {
        manager.allCollapsed = true
        const allIds = manager.renderData.tests.map((test) => test.id)
        setCollapsedIds(allIds)
        redraw()
    })
}

const redraw = () => {
    const { testSubset } = manager

    renderContent(testSubset)
    renderDerived()
}

module.exports = {
    redraw,
    bindEvents,
    renderStatic,
}

},{"./datamanager.js":1,"./dom.js":2,"./filter.js":3,"./sort.js":7,"./storage.js":8}],6:[function(require,module,exports){
class MediaViewer {
    constructor(assets) {
        this.assets = assets
        this.index = 0
    }

    nextActive() {
        this.index = this.index === this.assets.length - 1 ? 0 : this.index + 1
        return [this.activeFile, this.index]
    }

    prevActive() {
        this.index = this.index === 0 ? this.assets.length - 1 : this.index -1
        return [this.activeFile, this.index]
    }

    get currentIndex() {
        return this.index
    }

    get activeFile() {
        return this.assets[this.index]
    }
}


const setup = (resultBody, assets) => {
    if (!assets.length) {
        resultBody.querySelector('.media').classList.add('hidden')
        return
    }

    const mediaViewer = new MediaViewer(assets)
    const container = resultBody.querySelector('.media-container')
    const leftArrow = resultBody.querySelector('.media-container__nav--left')
    const rightArrow = resultBody.querySelector('.media-container__nav--right')
    const mediaName = resultBody.querySelector('.media__name')
    const counter = resultBody.querySelector('.media__counter')
    const imageEl = resultBody.querySelector('img')
    const sourceEl = resultBody.querySelector('source')
    const videoEl = resultBody.querySelector('video')

    const setImg = (media, index) => {
        if (media?.format_type === 'image') {
            imageEl.src = media.path

            imageEl.classList.remove('hidden')
            videoEl.classList.add('hidden')
        } else if (media?.format_type === 'video') {
            sourceEl.src = media.path

            videoEl.classList.remove('hidden')
            imageEl.classList.add('hidden')
        }

        mediaName.innerText = media?.name
        counter.innerText = `${index + 1} / ${assets.length}`
    }
    setImg(mediaViewer.activeFile, mediaViewer.currentIndex)

    const moveLeft = () => {
        const [media, index] = mediaViewer.prevActive()
        setImg(media, index)
    }
    const doRight = () => {
        const [media, index] = mediaViewer.nextActive()
        setImg(media, index)
    }
    const openImg = () => {
        window.open(mediaViewer.activeFile.path, '_blank')
    }
    if (assets.length === 1) {
        container.classList.add('media-container--fullscreen')
    } else {
        leftArrow.addEventListener('click', moveLeft)
        rightArrow.addEventListener('click', doRight)
    }
    imageEl.addEventListener('click', openImg)
}

module.exports = {
    setup,
}

},{}],7:[function(require,module,exports){
const { manager } = require('./datamanager.js')
const storageModule = require('./storage.js')

const genericSort = (list, key, ascending, customOrder) => {
    let sorted
    if (customOrder) {
        sorted = list.sort((a, b) => {
            const aValue = a.result.toLowerCase()
            const bValue = b.result.toLowerCase()

            const aIndex = customOrder.findIndex((item) => item.toLowerCase() === aValue)
            const bIndex = customOrder.findIndex((item) => item.toLowerCase() === bValue)

            // Compare the indices to determine the sort order
            return aIndex - bIndex
        })
    } else {
        sorted = list.sort((a, b) => a[key] === b[key] ? 0 : a[key] > b[key] ? 1 : -1)
    }

    if (ascending) {
        sorted.reverse()
    }
    return sorted
}

const durationSort = (list, ascending) => {
    const parseDuration = (duration) => {
        if (duration.includes(':')) {
            // If it's in the format "HH:mm:ss"
            const [hours, minutes, seconds] = duration.split(':').map(Number)
            return (hours * 3600 + minutes * 60 + seconds) * 1000
        } else {
            // If it's in the format "nnn ms"
            return parseInt(duration)
        }
    }
    const sorted = list.sort((a, b) => parseDuration(a['duration']) - parseDuration(b['duration']))
    if (ascending) {
        sorted.reverse()
    }
    return sorted
}

const doInitSort = () => {
    const type = storageModule.getSort(manager.initialSort)
    const ascending = storageModule.getSortDirection()
    const list = manager.testSubset
    const initialOrder = ['Error', 'Failed', 'Rerun', 'XFailed', 'XPassed', 'Skipped', 'Passed']

    storageModule.setSort(type)
    storageModule.setSortDirection(ascending)

    if (type?.toLowerCase() === 'original') {
        manager.setRender(list)
    } else {
        let sortedList
        switch (type) {
        case 'duration':
            sortedList = durationSort(list, ascending)
            break
        case 'result':
            sortedList = genericSort(list, type, ascending, initialOrder)
            break
        default:
            sortedList = genericSort(list, type, ascending)
            break
        }
        manager.setRender(sortedList)
    }
}

const doSort = (type, skipDirection) => {
    const newSortType = storageModule.getSort(manager.initialSort) !== type
    const currentAsc = storageModule.getSortDirection()
    let ascending
    if (skipDirection) {
        ascending = currentAsc
    } else {
        ascending = newSortType ? false : !currentAsc
    }
    storageModule.setSort(type)
    storageModule.setSortDirection(ascending)

    const list = manager.testSubset
    const sortedList = type === 'duration' ? durationSort(list, ascending) : genericSort(list, type, ascending)
    manager.setRender(sortedList)
}

module.exports = {
    doInitSort,
    doSort,
}

},{"./datamanager.js":1,"./storage.js":8}],8:[function(require,module,exports){
const possibleFilters = [
    'passed',
    'skipped',
    'failed',
    'error',
    'xfailed',
    'xpassed',
    'rerun',
]

const getVisible = () => {
    const url = new URL(window.location.href)
    const settings = new URLSearchParams(url.search).get('visible')
    const lower = (item) => {
        const lowerItem = item.toLowerCase()
        if (possibleFilters.includes(lowerItem)) {
            return lowerItem
        }
        return null
    }
    return settings === null ?
        possibleFilters :
        [...new Set(settings?.split(',').map(lower).filter((item) => item))]
}

const hideCategory = (categoryToHide) => {
    const url = new URL(window.location.href)
    const visibleParams = new URLSearchParams(url.search).get('visible')
    const currentVisible = visibleParams ? visibleParams.split(',') : [...possibleFilters]
    const settings = [...new Set(currentVisible)].filter((f) => f !== categoryToHide).join(',')

    url.searchParams.set('visible', settings)
    window.history.pushState({}, null, unescape(url.href))
}

const showCategory = (categoryToShow) => {
    if (typeof window === 'undefined') {
        return
    }
    const url = new URL(window.location.href)
    const currentVisible = new URLSearchParams(url.search).get('visible')?.split(',').filter(Boolean) ||
        [...possibleFilters]
    const settings = [...new Set([categoryToShow, ...currentVisible])]
    const noFilter = possibleFilters.length === settings.length || !settings.length

    noFilter ? url.searchParams.delete('visible') : url.searchParams.set('visible', settings.join(','))
    window.history.pushState({}, null, unescape(url.href))
}

const getSort = (initialSort) => {
    const url = new URL(window.location.href)
    let sort = new URLSearchParams(url.search).get('sort')
    if (!sort) {
        sort = initialSort || 'result'
    }
    return sort
}

const setSort = (type) => {
    const url = new URL(window.location.href)
    url.searchParams.set('sort', type)
    window.history.pushState({}, null, unescape(url.href))
}

const getCollapsedCategory = (renderCollapsed) => {
    let categories
    if (typeof window !== 'undefined') {
        const url = new URL(window.location.href)
        const collapsedItems = new URLSearchParams(url.search).get('collapsed')
        switch (true) {
        case !renderCollapsed && collapsedItems === null:
            categories = ['passed']
            break
        case collapsedItems?.length === 0 || /^["']{2}$/.test(collapsedItems):
            categories = []
            break
        case /^all$/.test(collapsedItems) || collapsedItems === null && /^all$/.test(renderCollapsed):
            categories = [...possibleFilters]
            break
        default:
            categories = collapsedItems?.split(',').map((item) => item.toLowerCase()) || renderCollapsed
            break
        }
    } else {
        categories = []
    }
    return categories
}

const getSortDirection = () => JSON.parse(sessionStorage.getItem('sortAsc')) || false
const setSortDirection = (ascending) => sessionStorage.setItem('sortAsc', ascending)

const getCollapsedIds = () => JSON.parse(sessionStorage.getItem('collapsedIds')) || []
const setCollapsedIds = (list) => sessionStorage.setItem('collapsedIds', JSON.stringify(list))

module.exports = {
    getVisible,
    hideCategory,
    showCategory,
    getCollapsedIds,
    setCollapsedIds,
    getSort,
    setSort,
    getSortDirection,
    setSortDirection,
    getCollapsedCategory,
    possibleFilters,
}

},{}]},{},[4]);
    </script>
  </footer>
</html>